{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "477790aa-9a9d-465d-b069-da63bf2807c7",
   "metadata": {},
   "source": [
    "# Invariant Ring Slot Attention\n",
    "\n",
    "**Goal:** In the `Invariant-Slot-Attention` notebook, I built an ISA model, and although I really liked the way this paper encoded the translation invariance into the physics model, I don't know if it's necessarily the _right_ model for this problem, b/c it will not correctly get the center or radius for rings that aren't fully contained inside of our cropped images.\n",
    "\n",
    "(See below example rings from Florian.)\n",
    "\n",
    "<img src=\"flo-esc-rings.png\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9569b6ac-5fe6-458c-81d7-87aa65bd3c7f",
   "metadata": {},
   "source": [
    "I think if I let the query dimension be $q \\in \\mathbb{R}^3$ denote the $(x,y,r)$ for each ring, I could compute the ring score based on the distance of each pixel to the ring predicted by the slot.\n",
    "\n",
    "Note, Lukas was not a super fan of this method b/c he liked the idea of having a general method that could learn any embedding, but I think infusing physics knowledge into our reconstruction is actually super awesome :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3146824a-d63c-4bf3-b4cc-388865588ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mlp\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import json, yaml, os\n",
    "os.sys.path.append('code')\n",
    "\n",
    "from plotting import plot_kslots, plot_kslots_iters\n",
    "from data import make_batch\n",
    "from model import SoftPositionalEmbed, build_grid\n",
    "from torch.nn import init\n",
    "from train import train\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef7453e-3bde-4b43-8b10-39144a8d9f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvariantSlotAttention(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 resolution=(32,32),\n",
    "                 xlow=-0.5,\n",
    "                 xhigh=0.5,\n",
    "                 k_slots=3, \n",
    "                 num_conv_layers=3,\n",
    "                 hidden_dim=32, \n",
    "                 final_cnn_relu=False,\n",
    "                 learn_init=False,\n",
    "                 query_dim=32, \n",
    "                 n_iter=2,\n",
    "                 pixel_mult=1,\n",
    "                 device='cpu' \n",
    "                 ):\n",
    "        '''\n",
    "        Slot attention encoder block with positional embedding\n",
    "\n",
    "        Inputs:\n",
    "        - resolution \n",
    "        - k_slots (default 3): number of slots (note, can vary between training and test time)\n",
    "        - num_conv_layers: # of convolutional layers to apply (google paper has 4)\n",
    "        - hidden_dim (default 32): The hidden dimension for the CNN (currently single layer w/ no non-linearities)\n",
    "        - final_cnn_relu: Whether to apply the final cnn relu for these experiments (use true to mimic google repo)\n",
    "        - query_dim (default 32): The latent space dimension that the slots and the queries get computed in\n",
    "        - n_iter (default  2): Number of slot attention steps to apply (defualt 2)\n",
    "        - T (str): Softmax temperature for scaling the logits \n",
    "            * default: 1/sqrt(query_dim)\n",
    "        - device (str): Which device to put the model on.\n",
    "            Options: cpu (default), mps, cuda:{i}\n",
    "            Also used when drawing random samples for the query points \n",
    "            and the grid generation for the positional encoding\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.k_slots = k_slots\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.query_dim = query_dim\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "        self.resolution = resolution\n",
    "        self.xlow, self.xhigh = xlow, xhigh\n",
    "        \n",
    "        self.device=device\n",
    "         \n",
    "        self.softmax_T = 10/np.sqrt(query_dim)\n",
    "        \n",
    "        self.dataN = torch.nn.LayerNorm(self.hidden_dim)\n",
    "        self.queryN = torch.nn.LayerNorm(self.query_dim)\n",
    "        \n",
    "        self.toK = torch.nn.Linear(self.hidden_dim, self.query_dim)\n",
    "        self.toV = torch.nn.Linear(self.hidden_dim, self.query_dim)\n",
    "        self.gru = torch.nn.GRUCell(self.query_dim, self.query_dim)\n",
    "\n",
    "        kwargs = {'out_channels': hidden_dim,'kernel_size': 5, 'padding':2 }\n",
    "        cnn_layers = [torch.nn.Conv2d(1,**kwargs)]\n",
    "        for i in range(num_conv_layers-1):\n",
    "            cnn_layers += [torch.nn.ReLU(), torch.nn.Conv2d(hidden_dim,**kwargs)] \n",
    "            \n",
    "        if final_cnn_relu:\n",
    "            cnn_layers.append(torch.nn.ReLU())\n",
    "\n",
    "        self.CNN_encoder = torch.nn.Sequential(*cnn_layers)\n",
    "            \n",
    "        '''\n",
    "        Positional embedding inputs\n",
    "        '''\n",
    "        self.abs_grid = self.build_grid()\n",
    "        \n",
    "        self.dense = torch.nn.Linear(2, query_dim) \n",
    "        self.pixel_mult = pixel_mult # LH's proposal... but almost same as 1/delta in ISA\n",
    "\n",
    "        # Apply after the data normalization\n",
    "        self.init_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_dim,hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_dim,hidden_dim)\n",
    "        )\n",
    "           \n",
    "        '''\n",
    "        Initializer jazz\n",
    "        '''\n",
    "        self.slots_mu = torch.nn.Parameter(torch.randn(1, 1, self.query_dim))\n",
    "        self.slots_logsigma = torch.nn.Parameter(torch.zeros(1, 1, self.query_dim))\n",
    "        init.xavier_uniform_(self.slots_logsigma)\n",
    "\n",
    "        self.init_slots = self.init_slots\n",
    "\n",
    "    def build_grid(self):\n",
    "        '''\n",
    "        From google slot attention repo:\n",
    "        https://github.com/nhartman94/google-research/blob/master/slot_attention/model.py#L357C1-L364C53\n",
    "        '''\n",
    "        resolution = self.resolution\n",
    "        xlow, xhigh = self.xlow, self.xhigh\n",
    "           \n",
    "        ranges = [np.linspace(xlow, xhigh, num=res) for res in resolution]\n",
    "        grid = np.meshgrid(*ranges, sparse=False, indexing=\"ij\")\n",
    "        grid = np.stack(grid, axis=-1)\n",
    "        grid = np.reshape(grid, [resolution[0], resolution[1], -1])\n",
    "        grid = np.expand_dims(grid, axis=0)\n",
    "        # grid = grid.astype(np.float32)\n",
    "        \n",
    "        # Now make it a pytorch tensor\n",
    "        grid = torch.FloatTensor( grid ).to(device)\n",
    "        grid = torch.flatten(grid,1,2)\n",
    "    \n",
    "        return grid\n",
    "        \n",
    "    def init_slots(self,Nbatch):\n",
    "        '''\n",
    "        Slot init taken from\n",
    "        https://github.com/lucidrains/slot-attention/blob/master/slot_attention/slot_attention.py\n",
    "        '''\n",
    "        mu = self.slots_mu.expand(Nbatch, self.k_slots, -1)\n",
    "        sigma = self.slots_logsigma.exp().expand(Nbatch, self.k_slots, -1)\n",
    "\n",
    "        queries = mu + sigma * torch.randn(mu.shape).to(self.device)\n",
    "    \n",
    "        # Add the position and scale initialization for the local ref frame\n",
    "        pos_scale = torch.rand(Nbatch, k_slots, ref_frame_dim).to(self.device)\n",
    "\n",
    "        pos_scale[:,:2] -= 0.5\n",
    "        pos_scale[:,-1]  = (stdhigh - stdlow) * pos_scale[:,-1] + stdlow\n",
    "        \n",
    "        return queries, pos_scale\n",
    "    \n",
    "    def get_keys_vals(self, encoded_data, pos_scale):\n",
    "\n",
    "        # Get the relative position embedding\n",
    "        rel_grid = self.abs_grid.unsqueeze(1) - pos_scale[:,:,:2].unsqueeze(2)\n",
    "        rel_grid /= pos_scale[:,:,-1].unsqueeze(2).unsqueeze(-1)\n",
    "        \n",
    "        # Embed it in the same space as the query dimension \n",
    "        embed_grid = self.pixel_mult * self.dense( rel_grid )\n",
    "        \n",
    "        # keys, vals: (bs, img_dim, query_dim)\n",
    "        keys = self.toK(encoded_data) + rel_grid\n",
    "        vals = self.toV(encoded_data) + rel_grid\n",
    "        \n",
    "        keys = self.init_mlp(self.dataN(keys))\n",
    "        vals = self.init_mlp(self.dataN(vals))\n",
    "        \n",
    "        return keys, vals\n",
    "                \n",
    "    def attention_and_weights(self,queries,keys):\n",
    "        \n",
    "        logits = torch.einsum('bse,bde->bsd',queries,keys) * self.softmax_T\n",
    "        \n",
    "        att = torch.nn.functional.softmax(logits, dim = 1)\n",
    "        \n",
    "        div = torch.sum(att, dim = -1, keepdims = True)\n",
    "        wts = att/div + 1e-8\n",
    "        return att,wts\n",
    "\n",
    "    def update_frames(self,wts):\n",
    "        # Update the relative frame position\n",
    "        \n",
    "        new_pos = torch.einsum('bsd,bde->bse',wts,self.abs_grid)\n",
    "        new_scale = torch.einsum('bsd,bde->bse',wts,torch.pow(self.abs_grid - new_pos,2))\n",
    "        new_scale = torch.sqrt(new_scale)\n",
    "        new_pos_scale = torch.cat([new_pos,new_scale.unsqueeze(-1)],axis=-1)\n",
    "        return new_pos_scale\n",
    "        \n",
    "    def iterate(self, queries, pos_scale, encoded_data):\n",
    "        \n",
    "        print('getting keys')\n",
    "        # Get the keys and values in the ref ref frame\n",
    "        keys, vals = self.get_keys_vals(encoded_data,pos_scale)\n",
    "        \n",
    "        print('getting attn')\n",
    "        # att,wts: (bs, k_slots, img_dim)\n",
    "        att,wts = self.attention_and_weights(self.queryN(queries),keys)   \n",
    "        \n",
    "        print('Updating ref frames')\n",
    "        new_pos_scale = self.update_frames(wts)\n",
    "        \n",
    "        print('Doing recurrence')\n",
    "        # Update the queries with the recurrent block\n",
    "        updates = torch.einsum('bsd,bde->bse',wts,vals) # bs, n_slots, query_dim\n",
    "        \n",
    "        updates = self.gru(\n",
    "            updates.reshape(-1,self.query_dim),\n",
    "            queries.reshape(-1,self.query_dim),\n",
    "        )\n",
    "        \n",
    "        return updates.reshape(queries.shape), new_pos_scale\n",
    "        \n",
    "    def forward(self, data):\n",
    "    \n",
    "        '''\n",
    "        Step 1: Extract the CNN features\n",
    "        '''\n",
    "        encoded_data = self.CNN_encoder(data) # Apply the CNN encoder\n",
    "        encoded_data = torch.permute(encoded_data,(0,2,3,1)) # Put channel dim at the end\n",
    "        encoded_data = torch.flatten(encoded_data,1,2) # flatten pixel dims\n",
    "        \n",
    "        print('encoded_data',encoded_data.shape)\n",
    "        \n",
    "        '''\n",
    "        Step 2: Initialize the slots\n",
    "        '''\n",
    "        Nbatch = data.shape[0]\n",
    "        \n",
    "        # Initialize the queries and pos_scale\n",
    "        queries, pos_scale = self.init_slots(Nbatch) # Shape (Nbatch, k_slots, query_dim)\n",
    "        \n",
    "        '''\n",
    "        Step 3: Iterate through the reconstruction\n",
    "        '''\n",
    "        for i in range(self.n_iter):\n",
    "            print('iter',i)\n",
    "            queries, pos_scale = self.iterate(queries, encoded_data, pos_scale)    \n",
    "            \n",
    "        # With the final query vector, calc the attn, weights, + rel ref frames\n",
    "        keys, vals = self.get_keys_vals(encoded_data,pos_scale)\n",
    "        att, wts = self.attention_and_weights(self.queryN(queries),keys)   \n",
    "        new_pos_scale = self.update_frames(wts)\n",
    "        \n",
    "        return queries, pos_scale, att, wts "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
