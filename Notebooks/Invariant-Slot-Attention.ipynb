{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b5383cf-09f3-4d62-8c8e-75b6ad9b0294",
   "metadata": {},
   "source": [
    "# Invariant Slot Attention\n",
    "\n",
    "**Goal:** I sort of thad this intuition for awhile that I want to be able to encode \"circleness\" into the slot representations that we're learning.\n",
    "\n",
    "This idea from the SA follow-up paper is not _exactly_ the same as this, but I think it's going in this direction!\n",
    "\n",
    "**Other optimization tricks included in this paper:**\n",
    "- Cosine decay (instead of exponential decay)\n",
    "- Use ResNet-34 as the image feature extractor model for \n",
    "    * They did modify the base block of this model to have stride 1 instead of 3\n",
    "- They do also add a $\\delta$ division with the positional embedding (they set $\\delta = 5$).\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d52811-e259-4fd6-a745-c839cc8c8417",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mlp\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import json, yaml, os\n",
    "os.sys.path.append('../code')\n",
    "\n",
    "from plotting import plot_kslots, plot_kslots_iters\n",
    "from data import make_batch\n",
    "from model import SoftPositionalEmbed, build_grid\n",
    "from torch.nn import init\n",
    "from train import hungarian_matching\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d7725f-432f-45e6-aa15-e11556e2a85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87055668-eff5-4333-8cb6-f9ec6f765de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hps = {\n",
    "    'hidden_dim': 16,\n",
    "    'k_slots':3,\n",
    "    'query_dim':128,\n",
    "    'pixel_mult':0.2,\n",
    "    'device':device\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187ada3e-5480-4230-a1e8-8eebe5adf4a8",
   "metadata": {},
   "source": [
    "**How was the data generator initialized?**\n",
    "- $x,y \\sim \\text{Unif}(-0.5, 0.5)$\n",
    "- $r \\sim \\text{Unif}(0.01, 0.05)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ec7e06-eee2-4db8-bb69-ac9d4efb54de",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdlow,stdhigh = 0.01, 0.05\n",
    "from copy import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ffd33b-ab89-465d-8af0-13a060d18a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvariantSlotAttention(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 resolution=(32,32),\n",
    "                 xlow=-0.5,\n",
    "                 xhigh=0.5,\n",
    "                 k_slots=3, \n",
    "                 num_conv_layers=3,\n",
    "                 hidden_dim=32, \n",
    "                 final_cnn_relu=False,\n",
    "                 query_dim=32, \n",
    "                 n_iter=2,\n",
    "                 pixel_mult=1,\n",
    "                 device='cpu' \n",
    "                 ):\n",
    "        '''\n",
    "        Slot attention encoder block with positional embedding\n",
    "\n",
    "        Inputs:\n",
    "        - resolution \n",
    "        - k_slots (default 3): number of slots (note, can vary between training and test time)\n",
    "        - num_conv_layers: # of convolutional layers to apply (google paper has 4)\n",
    "        - hidden_dim (default 32): The hidden dimension for the CNN (currently single layer w/ no non-linearities)\n",
    "        - final_cnn_relu: Whether to apply the final cnn relu for these experiments (use true to mimic google repo)\n",
    "        - query_dim (default 32): The latent space dimension that the slots and the queries get computed in\n",
    "        - n_iter (default  2): Number of slot attention steps to apply (defualt 2)\n",
    "        - T (str): Softmax temperature for scaling the logits \n",
    "            * default: 1/sqrt(query_dim)\n",
    "        - device (str): Which device to put the model on.\n",
    "            Options: cpu (default), mps, cuda:{i}\n",
    "            Also used when drawing random samples for the query points \n",
    "            and the grid generation for the positional encoding\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.k_slots = k_slots\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.query_dim = query_dim\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "        self.resolution = resolution\n",
    "        self.xlow, self.xhigh = xlow, xhigh\n",
    "        \n",
    "        self.device=device\n",
    "         \n",
    "        self.softmax_T = 1/np.sqrt(query_dim)\n",
    "        \n",
    "        self.dataN = torch.nn.LayerNorm(self.hidden_dim)\n",
    "        self.queryN = torch.nn.LayerNorm(self.query_dim)\n",
    "        \n",
    "        self.toK = torch.nn.Linear(self.hidden_dim, self.query_dim)\n",
    "        self.toV = torch.nn.Linear(self.hidden_dim, self.query_dim)\n",
    "        self.gru = torch.nn.GRUCell(self.query_dim, self.query_dim)\n",
    "\n",
    "        kwargs = {'out_channels': hidden_dim,'kernel_size': 5, 'padding':2 }\n",
    "        cnn_layers = [torch.nn.Conv2d(1,**kwargs)]\n",
    "        for i in range(num_conv_layers-1):\n",
    "            cnn_layers += [torch.nn.ReLU(), torch.nn.Conv2d(hidden_dim,**kwargs)] \n",
    "            \n",
    "        if final_cnn_relu:\n",
    "            cnn_layers.append(torch.nn.ReLU())\n",
    "\n",
    "        self.CNN_encoder = torch.nn.Sequential(*cnn_layers)\n",
    "            \n",
    "        '''\n",
    "        Positional embedding inputs\n",
    "        '''\n",
    "        self.abs_grid = self.build_grid()\n",
    "        \n",
    "        self.dense = torch.nn.Linear(2, query_dim) \n",
    "        self.pixel_mult = pixel_mult # LH's proposal... but almost same as 1/delta in ISA\n",
    "\n",
    "        # Apply after the data normalization\n",
    "        self.init_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(query_dim,query_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(query_dim,query_dim)\n",
    "        )\n",
    "           \n",
    "        '''\n",
    "        Slot initialization setup\n",
    "        '''\n",
    "        self.slots_mu = torch.nn.Parameter(torch.randn(1, 1, self.query_dim,device=device))\n",
    "        self.slots_logsigma = torch.nn.Parameter(torch.zeros(1, 1, self.query_dim,device=device))\n",
    "        init.xavier_uniform_(self.slots_logsigma)\n",
    "\n",
    "        self.init_slots = self.init_slots\n",
    "\n",
    "    def build_grid(self):\n",
    "        '''\n",
    "        From google slot attention repo:\n",
    "        https://github.com/nhartman94/google-research/blob/master/slot_attention/model.py#L357C1-L364C53\n",
    "        '''\n",
    "        resolution = self.resolution\n",
    "        xlow, xhigh = self.xlow, self.xhigh\n",
    "           \n",
    "        ranges = [np.linspace(xlow, xhigh, num=res) for res in resolution]\n",
    "        grid = np.meshgrid(*ranges, sparse=False, indexing=\"ij\")\n",
    "        grid = np.stack(grid, axis=-1)\n",
    "        grid = np.reshape(grid, [resolution[0], resolution[1], -1])\n",
    "        grid = np.expand_dims(grid, axis=0)\n",
    "        # grid = grid.astype(np.float32)\n",
    "        \n",
    "        # Now make it a pytorch tensor\n",
    "        grid = torch.FloatTensor( grid ).to(device)\n",
    "        grid = torch.flatten(grid,1,2)\n",
    "    \n",
    "        return grid\n",
    "        \n",
    "    def init_slots(self,Nbatch):\n",
    "        '''\n",
    "        Slot init taken from\n",
    "        https://github.com/lucidrains/slot-attention/blob/master/slot_attention/slot_attention.py\n",
    "        '''\n",
    "        mu = self.slots_mu.expand(Nbatch, self.k_slots, -1)\n",
    "        sigma = self.slots_logsigma.exp().expand(Nbatch, self.k_slots, -1)\n",
    "    \n",
    "        queries = mu + sigma * torch.randn(mu.shape,device=device)\n",
    "    \n",
    "        # Add the position and scale initialization for the local ref frame\n",
    "        ref_frame_dim = 3\n",
    "        pos_scale = torch.rand(Nbatch, self.k_slots, ref_frame_dim,device=device)\n",
    "\n",
    "        pos_scale[:,:2] -= 0.5\n",
    "        pos_scale[:,-1]  = (stdhigh - stdlow) * pos_scale[:,-1] + stdlow\n",
    "        \n",
    "        return queries, pos_scale\n",
    "    \n",
    "    def get_keys_vals(self, encoded_data, pos_scale):\n",
    "\n",
    "        # Get the relative position embedding\n",
    "        rel_grid = self.abs_grid.unsqueeze(1) - pos_scale[:,:,:2].unsqueeze(2)\n",
    "        rel_grid /= pos_scale[:,:,-1].unsqueeze(2).unsqueeze(-1)\n",
    "        \n",
    "        # Embed it in the same space as the query dimension \n",
    "        embed_grid = self.pixel_mult * self.dense( rel_grid )\n",
    "        \n",
    "        # keys, vals: (bs, img_dim, query_dim)\n",
    "        keys = m.toK(encoded_data).unsqueeze(1) + embed_grid\n",
    "        vals = m.toV(encoded_data).unsqueeze(1) + embed_grid\n",
    "        \n",
    "        keys = self.init_mlp(self.queryN(keys))\n",
    "        vals = self.init_mlp(self.queryN(vals))\n",
    "        \n",
    "        return keys, vals\n",
    "                \n",
    "    def attention_and_weights(self,queries,keys):\n",
    "        \n",
    "        logits = torch.einsum('bse,bsde->bsd',queries,keys) * self.softmax_T\n",
    "        \n",
    "        att = torch.nn.functional.softmax(logits, dim = 1)\n",
    "        \n",
    "        div = torch.sum(att, dim = -1, keepdims = True)\n",
    "        wts = att/div + 1e-8\n",
    "        return att,wts\n",
    "\n",
    "    def update_frames(self,wts):\n",
    "        '''\n",
    "        Update the relative frame position\n",
    "        '''\n",
    "        \n",
    "        # expand to include the batch dim\n",
    "        grid_exp = self.abs_grid.expand(wts.shape[0],-1,2)\n",
    "        \n",
    "        new_pos = torch.einsum('bsd,bde->bse',wts,grid_exp)\n",
    "        \n",
    "        new_scale = torch.sum(torch.pow(grid_exp.unsqueeze(1) - new_pos.unsqueeze(2),2),dim=-1)\n",
    "        \n",
    "        new_scale = torch.einsum('bsd,bsd->bs', wts, new_scale)\n",
    "        new_scale = torch.sqrt(new_scale)\n",
    "        \n",
    "        return torch.cat([new_pos,new_scale.unsqueeze(-1)],axis=-1)\n",
    "        \n",
    "    def iterate(self, queries, pos_scale, encoded_data):\n",
    "        \n",
    "        # Get the keys and values in the ref ref frame\n",
    "        keys, vals = self.get_keys_vals(encoded_data,pos_scale)\n",
    "        \n",
    "        # att,wts: (bs, k_slots, img_dim)\n",
    "        att,wts = self.attention_and_weights(self.queryN(queries),keys)   \n",
    "        \n",
    "        new_pos_scale = self.update_frames(wts)\n",
    "        \n",
    "        # Update the queries with the recurrent block\n",
    "        updates = torch.einsum('bsd,bsde->bse',wts,vals) # bs, n_slots, query_dim\n",
    "        \n",
    "        updates = self.gru(\n",
    "            updates.reshape(-1,self.query_dim),\n",
    "            queries.reshape(-1,self.query_dim),\n",
    "        )\n",
    "        \n",
    "        return updates.reshape(queries.shape), new_pos_scale\n",
    "        \n",
    "    def forward(self, data, return_init=False):\n",
    "    \n",
    "        '''\n",
    "        Step 1: Extract the CNN features\n",
    "        '''\n",
    "        encoded_data = self.CNN_encoder(data) # Apply the CNN encoder\n",
    "        encoded_data = torch.permute(encoded_data,(0,2,3,1)) # Put channel dim at the end\n",
    "        encoded_data = torch.flatten(encoded_data,1,2) # flatten pixel dims\n",
    "        encoded_data = self.dataN(encoded_data)\n",
    "        \n",
    "        '''\n",
    "        Step 2: Initialize the slots\n",
    "        '''\n",
    "        Nbatch = data.shape[0]\n",
    "        \n",
    "        # Initialize the queries and pos_scale\n",
    "        queries, pos_scale = self.init_slots(Nbatch) # Shape (Nbatch, k_slots, query_dim)\n",
    "        \n",
    "        init_queries = copy(queries)\n",
    "        init_pos = copy(pos_scale)\n",
    "        \n",
    "        '''\n",
    "        Step 3: Iterate through the reconstruction\n",
    "        '''\n",
    "        for i in range(self.n_iter):\n",
    "            queries, pos_scale = self.iterate(queries, pos_scale, encoded_data)    \n",
    "            \n",
    "        # With the final query vector, calc the attn, weights, + rel ref frames\n",
    "        keys, vals = self.get_keys_vals(encoded_data,pos_scale)\n",
    "        att, wts = self.attention_and_weights(self.queryN(queries),keys)   \n",
    "        new_pos_scale = self.update_frames(wts)\n",
    "        \n",
    "        if return_init:\n",
    "            return queries, new_pos_scale, att, wts, init_queries, init_pos \n",
    "        else:\n",
    "            return queries, new_pos_scale, att, wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce4b9a4-1261-4e3e-87b4-bf1643800a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = InvariantSlotAttention(**hps).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc85b772-e690-4b7f-956d-917559a18b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.abs_grid.sha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04c1fa0-5752-4267-b226-851636610f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m.load_state_dict(torch.load('code/models/test-isa/m_161.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0714d5-da8f-4b82-9fc9-26cb261f42e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nPixels=32\n",
    "from scipy.optimize import linear_sum_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee359f6-257b-44a1-812f-e2b0906e2f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ISA(model, \n",
    "          Ntrain = 5000, \n",
    "          bs=32, \n",
    "          lr=3e-4,\n",
    "          warmup_steps=5_000,\n",
    "          losses = [],\n",
    "          kwargs={'isRing': True, 'N_clusters':2},\n",
    "          device='cpu',\n",
    "          plot_every=250, \n",
    "          save_every=1000,\n",
    "          color='C0',cmap='Blues',\n",
    "          modelDir='.',figDir='',showImg=True):\n",
    "    '''\n",
    "    Same arg as train, rn just modifying for more outputs\n",
    "    '''\n",
    "\n",
    "    loss_fct = torch.nn.BCELoss(reduction='none')\n",
    "    \n",
    "    # Learning rate schedule config\n",
    "    base_learning_rate = lr\n",
    "    opt = torch.optim.Adam(model.parameters(), lr)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    k_slots = model.k_slots\n",
    "    resolution = model.resolution\n",
    "    kwargs['device'] = device\n",
    "\n",
    "    max_n_rings = kwargs['N_clusters']\n",
    "    isRing = kwargs[\"isRing\"]\n",
    "    print(f'Training model with {k_slots} slots on {max_n_rings}'+ (\"rings\" if isRing else \"blobs\"))\n",
    "\n",
    "    start = len(losses)\n",
    "    for i in range(start,start+Ntrain):\n",
    "           \n",
    "        learning_rate = base_learning_rate * 0.5 * (1 + np.cos(np.pi * i / Ntrain))\n",
    "        if i < warmup_steps:\n",
    "            learning_rate *= (i / warmup_steps)\n",
    "        \n",
    "        opt.param_groups[0]['lr'] = learning_rate\n",
    "            \n",
    "        X, Y, mask = make_batch(N_events=bs, **kwargs)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        out = model(X,return_init=True)\n",
    "        queries, pos_scale, att, wts, init_q, init_pos = out\n",
    "        if torch.isnan(init_q).sum() > 0 :\n",
    "            print('init_q is nan')\n",
    "        \n",
    "        if torch.isnan(att).sum() > 0 :\n",
    "        \n",
    "            print('# nan',torch.isnan(att).sum())\n",
    "            print('att',att)\n",
    "            print('try 2',torch.isnan(model(X)[2]).sum())\n",
    "            \n",
    "            # DEBUG: Save all sources of randomness\n",
    "            ks = ['queries', 'pos_scale', 'att', 'wts', 'init_q', 'init_pos']\n",
    "            data = {k: v.tolist() for k,v in zip(ks,out)}\n",
    "            with open() as f:\n",
    "                json.dump(data, f)\n",
    "            \n",
    "            return model, X,Y,mask, init_q, init_pos\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # Calculate the loss of _all_ possible combinations  \n",
    "            flat_mask = mask.reshape(-1,max_n_rings, np.prod(resolution))[:,None,:,:]\n",
    "        \n",
    "            att_ext  = torch.tile(att.unsqueeze(2),  dims=(1,1,max_n_rings,1)) \n",
    "            mask_ext = torch.tile(flat_mask,dims=(1,k_slots,1,1)) \n",
    "\n",
    "            pairwise_cost = loss_fct(att_ext,mask_ext).mean(axis=-1)\n",
    "            \n",
    "            indices = hungarian_matching(pairwise_cost)\n",
    "        \n",
    "        # Apply the sorting to the predict\n",
    "        bis=torch.arange(bs).to(device)\n",
    "        indices=indices.to(device)\n",
    "\n",
    "        slots_sorted = torch.cat([att[bis,indices[:,0,ri]].unsqueeze(1) for ri in range(max_n_rings)],dim=1)\n",
    "        \n",
    "        flat_mask = mask.reshape(-1,max_n_rings, np.prod(resolution))\n",
    "        rings_sorted = torch.cat([flat_mask[bis,indices[:,1,ri]].unsqueeze(1) for ri in range(max_n_rings)],dim=1)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = loss_fct(slots_sorted,rings_sorted).sum(axis=1).mean()\n",
    "        \n",
    "        # DEBUG: Save model before update\n",
    "        torch.save(model.state_dict(), f'{modelDir}/m_-2.pt')\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        # DEBUG: Save model after update\n",
    "        torch.save(model.state_dict(), f'{modelDir}/m_-1.pt')\n",
    "        \n",
    "        losses.append(float(loss))\n",
    "            \n",
    "        if i % plot_every == 0:\n",
    "            \n",
    "            print('iter',i,', loss',loss.detach().cpu().numpy(),', lr',opt.param_groups[0]['lr'])\n",
    "            \n",
    "            iEvt = 0\n",
    "            att_img = att[iEvt].reshape(k_slots,*resolution)\n",
    "            plot_kslots(losses, \n",
    "                        mask[iEvt].sum(axis=0).detach().cpu().numpy(), \n",
    "                        att_img.detach().cpu().numpy(),\n",
    "                        k_slots, color=color,cmap=cmap,\n",
    "                        figname=f'{figDir}/loss-slots-iter{i}-evt{iEvt}.jpg',showImg=showImg)\n",
    "#             plot_kslots_iters(model, X, iEvt=0, color=color,cmap=cmap, \n",
    "#                               figname=f'{figDir}/slots-unroll-iter{i}-evt{iEvt}.jpg',showImg=showImg)\n",
    "            \n",
    "        if i % save_every == 0:\n",
    "            torch.save(model.state_dict(), f'{modelDir}/m_{i}.pt')\n",
    "            with open(f'{modelDir}/loss.json','w') as f:\n",
    "                json.dump(losses, f)\n",
    "                \n",
    "    model.eval()\n",
    "    return model,losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043f2d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cID = 'isa-cosine-decay'\n",
    "\n",
    "modelDir = f'../code/models/{cID}'\n",
    "figDir = f'../code/figures/{cID}'\n",
    "\n",
    "# for d in [modelDir,figDir]:\n",
    "#     os.mkdir(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a26279",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=128\n",
    "max_n_rings=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5238ba3-5ec8-41ac-804e-09f60410ae0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf901c4-546d-44f3-b4f1-d6491b2dfd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses=[]\n",
    "\n",
    "out = train_ISA(m,Ntrain=40_000,bs=bs,device=device,\n",
    "                modelDir=modelDir,figDir=figDir,plot_every=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d6c0f1-a9ac-4fdf-8ece-bb3ffe292291",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, X,Y,mask, init_q, init_pos = out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610999ea-fd3e-4fed-9125-72f49e8bbfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries, pos_scale, att, wts = model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aff3377-236a-4388-bf83-453f78018031",
   "metadata": {},
   "outputs": [],
   "source": [
    "att[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0baceb-fc34-44c2-a84d-17e4390b015b",
   "metadata": {},
   "source": [
    "**Ideas:**\n",
    "1. I could see if an L2 regularization would help\n",
    "- Plot the min and max of the model parameters over time\n",
    "2. Would gradient clipping help?\n",
    "- I feel like it seems more sus smth else in the opt pipeline rn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd780d6-7f2d-4387-8f95-1175ef01a70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecca70d4-e326-439f-9859-d4b58132e346",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_fail = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c859c30-856e-42fb-9cec-4c5b0d56edb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data = model.CNN_encoder(X) # Apply the CNN encoder\n",
    "encoded_data = torch.permute(encoded_data,(0,2,3,1)) # Put channel dim at the end\n",
    "encoded_data = torch.flatten(encoded_data,1,2) # flatten pixel dims\n",
    "encoded_data = model.dataN(encoded_data)\n",
    "\n",
    "# Use the init Q from the failure mode\n",
    "queries = copy(init_q)\n",
    "pos_scale = copy(init_pos)\n",
    "\n",
    "'''\n",
    "Step 3: Iterate through the reconstruction\n",
    "'''\n",
    "for i in range(model.n_iter):\n",
    "    # queries, pos_scale = model.iterate(queries, pos_scale, encoded_data)    \n",
    "\n",
    "    print('t=',i)\n",
    "    \n",
    "    # Get the keys and values in the ref ref frame\n",
    "    keys, vals = model.get_keys_vals(encoded_data,pos_scale)\n",
    "\n",
    "    print('keys',keys[i_fail].isnan().sum().item(),f'max {keys[i_fail].max().item():.2f},min {keys[i_fail].min().item():.2f}')\n",
    "    print('vals',vals[i_fail].isnan().sum().item(),f'max {vals[i_fail].max().item():.2f},min {vals[i_fail].min().item():.2f}')\n",
    "    \n",
    "    # att,wts: (bs, k_slots, img_dim)\n",
    "    att,wts = model.attention_and_weights(model.queryN(queries),keys)   \n",
    "\n",
    "    print('att',att[i_fail].isnan().sum().item(),f'max {att[i_fail].max().item():.2f},min {att[i_fail].min().item():.2f}')\n",
    "    print('wts',wts[i_fail].isnan().sum().item(),f'max {wts[i_fail].max().item():.2f},min {wts[i_fail].min().item():.2f}')\n",
    "\n",
    "    new_pos_scale = model.update_frames(wts)\n",
    "\n",
    "    # Update the queries with the recurrent block\n",
    "    updates = torch.einsum('bsd,bsde->bse',wts,vals) # bs, n_slots, query_dim\n",
    "\n",
    "    updates = model.gru(\n",
    "        updates.reshape(-1,model.query_dim),\n",
    "        queries.reshape(-1,model.query_dim),\n",
    "    )\n",
    "\n",
    "    queries,pos_scale = updates.reshape(queries.shape), new_pos_scale\n",
    "    \n",
    "    \n",
    "    print(i,queries[i_fail],pos_scale[i_fail])\n",
    "    \n",
    "    break\n",
    "    \n",
    "    \n",
    "# # With the final query vector, calc the attn, weights, + rel ref frames\n",
    "# keys, vals = model.get_keys_vals(encoded_data,pos_scale)\n",
    "# att, wts = model.attention_and_weights(model.queryN(queries),keys)   \n",
    "# new_pos_scale = model.update_frames(wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e4a2df-6726-48ae-9455-87f55f63ce00",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2, Y2, mask2 = make_batch(N_events=2,device=device, **{'isRing': True, 'N_clusters':2})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ceb96d9-ff09-4f13-8b6f-f108e8bc7060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m_cpu = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e07c65-262b-4435-8cba-6696124048dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b22094-e81e-46cc-bf2f-03e193b7c67f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e2ea80-c202-4768-840f-e3b6939785c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(encoded_data.flatten().detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02456656-9d8a-4da8-9e6e-0ca0e22f271d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the relative position embedding\n",
    "rel_grid = model.abs_grid.unsqueeze(1) - init_pos[:,:,:2].unsqueeze(2)\n",
    "rel_grid /= init_pos[:,:,-1].unsqueeze(2).unsqueeze(-1)\n",
    "\n",
    "# Embed it in the same space as the query dimension \n",
    "embed_grid = model.pixel_mult * model.dense( rel_grid )\n",
    "\n",
    "# keys, vals: (bs, img_dim, query_dim)\n",
    "k0 = model.toK(encoded_data).unsqueeze(1) + embed_grid\n",
    "v0 = model.toV(encoded_data).unsqueeze(1) + embed_grid\n",
    "\n",
    "k0 = model.init_mlp(k0)\n",
    "v0 = model.init_mlp(v0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47255a11-8d48-4838-8de5-3e41b798bef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb=100\n",
    "r=(-5,5)\n",
    "plt.hist(k0.flatten().detach().cpu().numpy(),nb,r,label='keys',\n",
    "         color='g',histtype='step',lw=2)\n",
    "plt.hist(v0.flatten().detach().cpu().numpy(),nb,r,label='values',\n",
    "         color='b',histtype='step',lw=2)\n",
    "\n",
    "plt.xlabel('features')\n",
    "plt.ylabel('entries')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a74001-3cfd-4c06-86f8-b4577c283ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb=100\n",
    "r=(-5,5)\n",
    "plt.hist(k0[i_fail].flatten().detach().cpu().numpy(),nb,r,label='keys',\n",
    "         color='g',histtype='step',lw=2)\n",
    "plt.hist(v0[i_fail].flatten().detach().cpu().numpy(),nb,r,label='values',\n",
    "         color='b',histtype='step',lw=2)\n",
    "\n",
    "plt.xlabel('features')\n",
    "plt.ylabel('entries')\n",
    "plt.title(f'Event {i_fail} (evt with nans)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8ab597-9805-4332-b9f9-dd9b29f5cc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "k0[i_fail].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafd1811-c19a-4d94-a30d-3ee19e1e378c",
   "metadata": {},
   "outputs": [],
   "source": [
    "k0[i_fail][abs(k0[i_fail])>100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65989fe6-45cf-43cd-9d4e-d36011344395",
   "metadata": {},
   "outputs": [],
   "source": [
    "v0[i_fail][abs(v0[i_fail])>100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9453656c-e2fd-4441-ae75-d997f41e4082",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in model.parameters():\n",
    "    print(p.shape,p)\n",
    "    # plt.hist(p.detach().cpu().numpy(),histtype='step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bb3f7b-1fd1-4026-9eb4-366b8966bf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02723e4e-cbd0-4250-bac0-1d9a3bd76601",
   "metadata": {},
   "outputs": [],
   "source": [
    "att.isnan().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479ab6e6-d34c-4322-a066-9640e079098d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, att_i in enumerate(att):\n",
    "    if att_i.isnan().sum() > 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16ec01a-ff61-4319-bcb6-91e2c64b6b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_fail = 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11503f0a-35e6-49c3-878c-946743d446a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_q[i_fail]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf1ff27-f634-46ba-89a4-33d5b0399900",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_pos[i_fail]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748c0da5-ccd7-4a41-9643-b694abe5c705",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8def060f-3a35-42c4-8765-fd7f8113db1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a55c98-1afa-4230-aaa7-f049f159402c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bccd6f4-0d22-4d5d-9684-797808c9ff52",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.prod(att.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cb786c-a358-41c8-93e1-7ee4192ba79c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d0a1e0-86f7-420c-a8ff-5c77a9784c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3695a313-06b3-41c2-9708-023b7c80ab14",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96925b8-5bc1-413d-9258-fdc719cd709e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06dc0cc-2d88-40de-afe1-bda6000937a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b736b1a3-6482-4b1a-afa3-d07693c5a3f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d30342-d805-4152-8d00-77bde0594f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy(att)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8b7fd2-937d-4051-8694-bab17b9e742c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33899002-44a7-45f1-a39e-b5188e0f4e4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81474271-8e25-4c46-8b80-59873e738b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "att.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f511f0-f975-4c05-8782-d3b407d3a4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "m,losses = out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd75d4d5-dcd2-4e56-a148-4d9efee5bf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, X,Y,mask = out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cf9e89-51ab-4603-86bc-cf60707c669f",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries, pos_scale, att, wts = m(X)\n",
    "        \n",
    "print( torch.isnan(att).sum() >0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d096ad-788b-498c-baaf-c72739a1ddda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6db0cc-35d7-4a39-a26a-cda8cec351e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b72dff-9d4d-4f9f-ad77-6a12240c9805",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b532d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# att_img = att[iEvt].reshape(model.k_slots,*resolution)\n",
    "# plot_kslots(losses, \n",
    "#             mask[iEvt].sum(axis=0).detach().cpu().numpy(), \n",
    "#             att_img.detach().cpu().numpy(),\n",
    "#             k_slots, color=color,cmap=cmap,\n",
    "#             figname=f'{figDir}/loss-slots-iter{i}-evt{iEvt}.jpg',showImg=showImg)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
