{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7c911fd",
   "metadata": {},
   "source": [
    "# Hungarian Multi Rings debug (1)\n",
    "\n",
    "**Goal:** I started from the `Hungarian-multi-rings` nb, but the two rings example _wasn't training_. Fortunately, there are a finite number of things to try, so I'll just try to cross them off, one-by-one here!\n",
    "\n",
    "\n",
    "- Try to memorize single batch / example\n",
    "- Try double precision instead of float precision\n",
    "- Plot every the model activation and gradients for the other SA layers?\n",
    "    * Maybe PCA for viz the query_dim\n",
    "    * Or decrease the query_dim to 2 for visualizing the clustering in time?\n",
    "- Get a comparison for what the loss is when we train exactly two rings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ef9d668",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "import os\n",
    "os.sys.path.append('code')\n",
    "from model import build_grid,SoftPositionalEmbed\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940ff75b",
   "metadata": {},
   "source": [
    "### Modifying the Flo's generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f14a3b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nPixels = 32\n",
    "\n",
    "xlow = -0.5\n",
    "xhigh = 0.5\n",
    "binWidth = (xhigh-xlow)/(nPixels+1)\n",
    "\n",
    "stdlow = 0.01\n",
    "stdhigh = 0.05\n",
    "\n",
    "bins = np.arange(xlow,xhigh,binWidth)\n",
    "\n",
    "nBins = len(bins)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8bc6d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(\n",
    "    bins=np.arange(xlow,xhigh,binWidth),\n",
    "    N_events = 100000,\n",
    "    N_clusters = 1,\n",
    "    mean_var_cluster = np.log(0.001),\n",
    "    sigma_var_cluster = 0.1,\n",
    "    mean_lam_cluster = np.log(200.),\n",
    "    sigma_lam_cluster = 1.,\n",
    "    isRing=True,\n",
    "    blurImage=False,\n",
    "    xlow=-.5,\n",
    "    xhigh=0.5,\n",
    "    stdlow=0.01,\n",
    "    stdhigh=0.05,\n",
    "    device='cpu'\n",
    "):\n",
    "    '''\n",
    "    Starts from Florian's `gen_events` function\n",
    "    https://gitlab.lrz.de/neural_network_clustering/permutation_invariant_loss/-/blob/main/test_blur.ipynb\n",
    "    \n",
    "    But I wanted to return the occupancy maps that you multiply the OG image by to output partitioning\n",
    "    of the image into the OG ring signals.\n",
    "    '''\n",
    "\n",
    "    eventHistograms = np.zeros(shape=(N_events, len(bins)-1, len(bins)-1,1) ) \n",
    "    \n",
    "    eventMasks = np.zeros(shape=(N_events, N_clusters, len(bins)-1, len(bins)-1) ) \n",
    "    \n",
    "    eventNumbers = np.zeros(N_events)\n",
    "\n",
    "    nMaxClusters = N_clusters   \n",
    "    \n",
    "    eventInfo = np.zeros(shape=(N_events, nMaxClusters, 3))\n",
    "    \n",
    "    for iEvent in range(N_events):\n",
    "        \n",
    "        image = np.zeros_like(eventHistograms[iEvent,:,:,0])\n",
    "        \n",
    "        n_clusters = nMaxClusters\n",
    "\n",
    "        eventNumbers[iEvent] += n_clusters\n",
    "        eI = []\n",
    "        \n",
    "        for iCluster in range(min(n_clusters,nMaxClusters)):\n",
    "            \n",
    "            # how many events in this cluster\n",
    "            lam_N_events_in_cluster = 200. \n",
    "            \n",
    "            N_events_in_cluster = np.random.poisson(lam_N_events_in_cluster)\n",
    "\n",
    "            # where is the cluster center\n",
    "            cluster_center = np.random.uniform(low=xlow, high=xhigh, size=2)\n",
    "\n",
    "            # what is the cluster spread\n",
    "            var_cluster = np.random.uniform(stdlow,stdhigh) \n",
    "            \n",
    "            cluster_events_x0 = np.random.normal(loc=0., scale=1., size=N_events_in_cluster)\n",
    "            cluster_events_y0 = np.random.normal(loc=0., scale=1., size=N_events_in_cluster)\n",
    "\n",
    "            if isRing:\n",
    "                fact = np.sqrt(var_cluster/(cluster_events_x0**2+cluster_events_y0**2))\n",
    "            else:\n",
    "                fact = np.sqrt(var_cluster)\n",
    "\n",
    "            cluster_events_x = cluster_events_x0*fact + cluster_center[0]\n",
    "            cluster_events_y = cluster_events_y0*fact + cluster_center[1]\n",
    "            \n",
    "            # bin the events\n",
    "            H, _, _ = np.histogram2d(cluster_events_x, cluster_events_y, bins=[bins,bins])\n",
    "\n",
    "            eventMasks[iEvent,iCluster] = H.T\n",
    "            image += H.T\n",
    "                        \n",
    "            eI.append(np.concatenate([cluster_center, [var_cluster]]))\n",
    "\n",
    "        eventHistograms[iEvent,:,:,0] = np.copy(image)\n",
    "\n",
    "        eventInfo[iEvent] = np.array(eI)\n",
    "    \n",
    "    # reshape eventHistograms\n",
    "    eventHistograms = eventHistograms[:,None,:,:,0] # shape (bs, 1, nPixels, nPixels)\n",
    "    \n",
    "    eventMasks = np.where(eventHistograms>0,eventMasks/eventHistograms,eventMasks)\n",
    "    \n",
    "    return torch.DoubleTensor(eventHistograms).to(device), \\\n",
    "           torch.DoubleTensor(eventInfo).to(device), \\\n",
    "           torch.DoubleTensor(eventMasks).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0682896",
   "metadata": {},
   "source": [
    "Let's consider:\n",
    "- batch_size 4\n",
    "- 2 rings\n",
    "- 3 slots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4d55862",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_n_rings = 2\n",
    "device= 'cuda:2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3795328a",
   "metadata": {},
   "source": [
    "**Note on precision:**\n",
    "- The `mps` device doesn't support double precision \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40a5cf51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-fdb0c714b299>:83: RuntimeWarning: invalid value encountered in true_divide\n",
      "  eventMasks = np.where(eventHistograms>0,eventMasks/eventHistograms,eventMasks)\n"
     ]
    }
   ],
   "source": [
    "bs=2\n",
    "X, Y, true = make_batch(bins, N_events=bs, N_clusters=max_n_rings,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cc4a67f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [4., 1., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:2',\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfcb3258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x216 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "iEvt=1\n",
    "\n",
    "if max_n_rings>1:\n",
    "    \n",
    "    fig, axs = plt.subplots(1,max_n_rings,figsize=(max_n_rings*3+1,3))\n",
    "\n",
    "    for i,ax in enumerate(axs):\n",
    "\n",
    "        im = ax.imshow(true[iEvt,i].cpu().numpy(),vmin=0,vmax=1)\n",
    "\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "        fig.colorbar(im, cax=cax, orientation='vertical')\n",
    "\n",
    "        ax.set_title(f'Truth photon {i}')\n",
    "\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ad95d0-a0a4-4adf-848d-7af8d055607e",
   "metadata": {},
   "source": [
    "**Define the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86bcc56e-a588-4387-8172-dbe73fe78265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.nn import init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8b7af14-cfd0-42cd-9584-fac2b2e5aacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_slots=max_n_rings+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "861d23a3-b7d4-4004-8c6c-1671d72dac2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlotAttentionPosEmbed(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 resolution=(32,32),\n",
    "                 k_slots=3, \n",
    "                 hidden_dim=32, \n",
    "                 query_dim=32, \n",
    "                 n_iter=2,\n",
    "                 device='cpu',\n",
    "                 dtype=torch.float32\n",
    "                 ):\n",
    "        '''\n",
    "        Slot attention encoder block with positional embedding\n",
    "\n",
    "        Inputs:\n",
    "        - device (cpu, mps, cuda): Which device to put the model on \n",
    "                (needed for the random call when initializing the slots)\n",
    "        - k_slots: number of slots (note, can vary between training and test time)\n",
    "        - hidden_dim: The hidden dimension for the CNN (currently single layer w/ no non-linearities)\n",
    "        - query_dim: The latent space dimension that the slots and the queries get computed in\n",
    "        - n_iter: Number of slot attention steps to apply (defualt 2, and rn hard coded)\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.k_slots = k_slots\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.query_dim = query_dim\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "        self.device=device\n",
    "        self.dtype=dtype\n",
    "        \n",
    "        self.dataN = torch.nn.LayerNorm(self.hidden_dim)\n",
    "        self.queryN = torch.nn.LayerNorm(self.query_dim)\n",
    "        \n",
    "        self.toK = torch.nn.Linear(self.hidden_dim, self.query_dim)\n",
    "        self.toV = torch.nn.Linear(self.hidden_dim, self.query_dim)\n",
    "        self.gru = torch.nn.GRUCell(self.query_dim, self.query_dim)\n",
    "\n",
    "        filter_size=5\n",
    "        self.CNN_encoder = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1,self.hidden_dim,filter_size, padding = 2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(self.hidden_dim,self.hidden_dim,filter_size, padding = 2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(self.hidden_dim,self.hidden_dim,filter_size, padding = 2),\n",
    "        )\n",
    "        \n",
    "        self.posEnc = SoftPositionalEmbed(hidden_dim, resolution,device,dtype)\n",
    "        \n",
    "        self.init_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_dim,hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_dim,hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # self.final_mlp = torch.nn.Se\n",
    "#         self.slots_mu = torch.nn.Parameter(torch.randn(1, 1, self.query_dim))\n",
    "\n",
    "#         self.slots_logsigma = torch.nn.Parameter(torch.zeros(1, 1, self.query_dim))\n",
    "#         init.xavier_uniform_(self.slots_logsigma)\n",
    "    \n",
    "            \n",
    "    def init_slots(self, Nbatch):\n",
    "        noise = torch.randn(Nbatch, self.k_slots, self.query_dim,dtype=self.dtype).to(self.device)\n",
    "        \n",
    "        mu = torch.zeros(1,1,self.query_dim,dtype=self.dtype).to(self.device)\n",
    "        logsigma = torch.zeros(1,1,self.query_dim,dtype=self.dtype).to(self.device)\n",
    "        \n",
    "        return mu + noise*logsigma.exp()\n",
    "\n",
    "\n",
    "#     def init_slots(self,Nbatch):\n",
    "#         '''\n",
    "#         Slot init taken from\n",
    "#         https://github.com/lucidrains/slot-attention/blob/master/slot_attention/slot_attention.py\n",
    "#         '''\n",
    "#         mu = self.slots_mu.expand(Nbatch, self.k_slots, -1)\n",
    "#         sigma = self.slots_logsigma.exp().expand(Nbatch, self.k_slots, -1)\n",
    "\n",
    "#         return mu + sigma * torch.randn(mu.shape).to(self.device)\n",
    "\n",
    "    def encoder(self,data):\n",
    "        \n",
    "        # Apply the CNN encoder\n",
    "        encoded_data = self.CNN_encoder(data)\n",
    "        \n",
    "        # Put the channel dim at the end\n",
    "        encoded_data = torch.permute(encoded_data,(0,2,3,1)) \n",
    "                 \n",
    "        # Add the positional embeddings\n",
    "        encoded_data = self.posEnc(encoded_data)\n",
    "        \n",
    "        # Flatten the pixel dims and apply the data normalization + MLP\n",
    "        encoded_data = torch.flatten(encoded_data,1,2)\n",
    "        encoded_data = self.dataN(encoded_data)\n",
    "        encoded_data = self.init_mlp(encoded_data)\n",
    "        \n",
    "        return encoded_data\n",
    "    \n",
    "    # hook for the gradients of the activations\n",
    "    def activations_hook(self, grad):\n",
    "        self.gradients.append( grad )\n",
    "    \n",
    "    def attention_and_weights(self,queries,encoded_data):\n",
    "        keys = self.toK(encoded_data)\n",
    "        # logits = torch.einsum('bse,bde->bsd',queries,keys) * (self.query_dim ** (-0.5)) * 10\n",
    "        logits = torch.einsum('bse,bde->bsd',queries,keys) / self.query_dim \n",
    "        \n",
    "        att = torch.nn.functional.softmax(logits, dim = 1)\n",
    "\n",
    "        if att.requires_grad:\n",
    "            h = logits.register_hook(self.activations_hook)\n",
    "        \n",
    "        div = torch.sum(att, dim = -1, keepdims = True)\n",
    "        wts = att/div + 1e-8\n",
    "        return att,wts\n",
    "\n",
    "    def iterate(self, queries, encoded_data):\n",
    "        \n",
    "        # queries: (bs, k_slots, query_dim)\n",
    "        \n",
    "        att,wts = self.attention_and_weights(self.queryN(queries),encoded_data)   \n",
    "        \n",
    "        # att: (bs, k_slots, img_dim)\n",
    "        # wts: (bs, k_slots, img_dim)\n",
    "        \n",
    "        vals = self.toV(encoded_data) # bs, img_dim, query_dim\n",
    "        updates = torch.einsum('bsd,bde->bse',wts,vals) # bs, n_slots, query_dim\n",
    "        \n",
    "        updates = self.gru(\n",
    "            updates.reshape(-1,self.query_dim),\n",
    "            queries.reshape(-1,self.query_dim),\n",
    "        )\n",
    "\n",
    "        return updates.reshape(queries.shape)\n",
    "        \n",
    "    def forward(self, data):\n",
    "\n",
    "        self.gradients = []\n",
    "        \n",
    "        Nbatch = data.shape[0]\n",
    "        \n",
    "        # Initialize the queries\n",
    "        queries = self.init_slots(Nbatch) # Shape (Nbatch, k_slots, query_dim)\n",
    "        \n",
    "        encoded_data = self.encoder(data)\n",
    "        \n",
    "        for i in range(self.n_iter):\n",
    "            queries = self.iterate(queries, encoded_data)    \n",
    "            \n",
    "        # Then with the _final_ query vector, calc what the attn + weights would be\n",
    "        att, wts = self.attention_and_weights(self.queryN(queries),encoded_data)   \n",
    "            \n",
    "        return queries, att #.reshape(-1,self.k_slots,nPixels,nPixels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffae43b0-cafd-4a1e-8464-d6003d66fd7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ebcf6d7c-ab27-4700-a3d4-c3c26347fe70",
   "metadata": {},
   "source": [
    "**Make a nice plotting function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8cf2bdb-b07d-4172-b89a-b603a64020f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kslots(losses, mask, att, k_slots, color='C2',cmap='Greens'):\n",
    "    \n",
    "    fig, axs = plt.subplots(1,k_slots+2,figsize=(2.75 * (k_slots + 2) ,2.5))\n",
    "\n",
    "    axs[0].plot(losses,color=color)\n",
    "    axs[0].set_xlabel('Iters')\n",
    "    axs[0].set_ylabel('Loss')\n",
    "    \n",
    "    imgs   = [mask] + [att[i] for i in range(k_slots)]\n",
    "    titles = ['Target']+[f'Slot {i}' for i in range(k_slots)]\n",
    "    \n",
    "    for i, (ax,img,title) in enumerate(zip(axs[1:],imgs, titles)):\n",
    "        \n",
    "        im = ax.imshow(img,cmap=cmap,vmin=0,vmax=1)\n",
    "\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "        fig.colorbar(im, cax=cax, orientation='vertical')\n",
    "\n",
    "        ax.set_title(title)\n",
    "\n",
    "        ax.axis('off')\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24099c7e-9167-4948-b145-808f1abb9ad2",
   "metadata": {},
   "source": [
    "Train the model!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58915b1d-65e2-4440-bca9-98e482b7f4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hungarian_matching(att, mask,bs, k_slots,max_n_rings,nPixels):\n",
    "    '''\n",
    "    Hungarian section Translated from the TensorFlow loss function (from 2006.15055 code):\n",
    "    https://github.com/nhartman94/google-research/blob/master/slot_attention/utils.py#L26-L57\n",
    "    '''\n",
    "    \n",
    "    flat_mask = mask.reshape(-1,max_n_rings, nPixels*nPixels)[:,None,:,:]\n",
    "    \n",
    "    att_ext  = torch.tile(att.unsqueeze(2),  dims=(1,1,max_n_rings,1)) #.reshape(bs * k_slots * max_n_rings , nPixels**2)\n",
    "    mask_ext = torch.tile(flat_mask,dims=(1,k_slots,1,1)) #.reshape(bs * k_slots * max_n_rings , nPixels**2)\n",
    "    \n",
    "    pairwise_cost = F.binary_cross_entropy(att_ext,mask_ext,reduction='none').mean(axis=-1)\n",
    "    #pairwise_cost = pairwise_cost.reshape(bs, k_slots, max_n_rings)\n",
    "    \n",
    "    indices = list(map(linear_sum_assignment, pairwise_cost.cpu()))\n",
    "    indices = torch.LongTensor(indices)\n",
    "    \n",
    "    loss = 0\n",
    "    for pi,(ri,ci) in zip(pairwise_cost,indices):\n",
    "        loss += pi[ri,ci].sum()\n",
    "    \n",
    "    return indices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfe48bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, Ntrain = 5000, bs=32, device='cpu', color='C2',cmap='Greens'):\n",
    "    '''\n",
    "    train\n",
    "    -----------\n",
    "    \n",
    "    - model\n",
    "    - Ntrain: # of training iterations\n",
    "    - color,cmap -- options that get passed the\n",
    "    '''\n",
    "\n",
    "    # Learning rate schedule config\n",
    "    base_learning_rate = 3e-4\n",
    "    \n",
    "    decay_rate = 0.5\n",
    "    warmup_steps=1_000\n",
    "    decay_steps = 2_000\n",
    "    \n",
    "    opt = torch.optim.Adam(model.parameters(), base_learning_rate)\n",
    "    model.train()\n",
    "    losses = []\n",
    "    \n",
    "    X, Y, mask = make_batch(N_events=bs, N_clusters=max_n_rings,device=device)\n",
    "        \n",
    "    \n",
    "    for i in range(Ntrain):\n",
    "\n",
    "        learning_rate = base_learning_rate * decay_rate ** (i / decay_steps)\n",
    "        if i < warmup_steps:\n",
    "            learning_rate *= (i / warmup_steps)\n",
    "        \n",
    "        opt.param_groups[0]['lr'] = learning_rate\n",
    "        \n",
    "        queries, att = model(X)\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            indices = hungarian_matching(att,mask,bs,model.k_slots,max_n_rings,nPixels)\n",
    "\n",
    "        # Apply the sorting to the predict\n",
    "        bis=torch.arange(bs).to(device)\n",
    "        indices=indices.to(device)\n",
    "\n",
    "        slots_sorted = torch.cat([att[bis,indices[:,0,ri]].unsqueeze(1) for ri in range(max_n_rings)],dim=1)\n",
    "        \n",
    "        flat_mask = mask.reshape(-1,max_n_rings, nPixels*nPixels)\n",
    "        rings_sorted = torch.cat([flat_mask[bis,indices[:,1,ri]].unsqueeze(1) for ri in range(max_n_rings)],dim=1)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = F.binary_cross_entropy(slots_sorted,rings_sorted,reduction='none').sum(axis=1).mean()\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "        losses.append(float(loss))\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print('iter',i,', loss',loss.detach().cpu().numpy(),', lr',opt.param_groups[0]['lr'])\n",
    "            \n",
    "            iEvt = 0\n",
    "            att_img = att[iEvt].reshape(model.k_slots,nPixels,nPixels)\n",
    "            plot_kslots(losses, \n",
    "                        mask[iEvt].sum(axis=0).detach().cpu().numpy(), \n",
    "                        att_img.detach().cpu().numpy(),\n",
    "                        k_slots, color=color,cmap=cmap)\n",
    "            \n",
    "            \n",
    "            plot_kslots_iters(model, X, iEvt=0)\n",
    "            plot_kslots_grads(model.gradients,iEvt=0)\n",
    "\n",
    "    model.eval()\n",
    "    return model,losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512158cd-9ce8-4f94-950d-50b7d2e2a1d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "067d99a8-f203-497f-9152-d9ca920fde41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kslots_iters(model, data, iEvt, color='C2',cmap='Greens'):\n",
    "    '''\n",
    "    Plot the attention masks across the iterations\n",
    "    '''\n",
    "    \n",
    "    n_iter = model.n_iter\n",
    "    k_slots = model.k_slots\n",
    "    \n",
    "    attn_masks = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Run through the model code to eval the attn masks\n",
    "        queries = model.init_slots(data.shape[0]) \n",
    "        encoded_data = model.encoder(data)\n",
    "\n",
    "        for i in range(n_iter):\n",
    "            # Get the mask\n",
    "            att,wts = model.attention_and_weights(model.queryN(queries), encoded_data) \n",
    "            attn_masks.append(att.detach().cpu())\n",
    "\n",
    "            # Get the updated query\n",
    "            queries = model.iterate(queries, encoded_data)\n",
    "\n",
    "        # Get the final mask\n",
    "        att,wts = model.attention_and_weights(model.queryN(queries), encoded_data) \n",
    "        attn_masks.append(att.detach().cpu())\n",
    "    \n",
    "    '''\n",
    "    Make the plot\n",
    "    '''\n",
    "    fig, axs = plt.subplots(n_iter+1,k_slots,figsize=(2.75*k_slots,2.5*(n_iter+1)))\n",
    " \n",
    "    for i, (ax_i, att) in enumerate(zip(axs, attn_masks)):\n",
    "        \n",
    "        att_img = att[iEvt].reshape(k_slots,nPixels,nPixels)\n",
    "        \n",
    "        for j, (ax, img) in enumerate(zip(ax_i,att_img)):\n",
    "        \n",
    "            im = ax.imshow(img,cmap=cmap)#,vmin=0,vmax=1)\n",
    "\n",
    "            divider = make_axes_locatable(ax)\n",
    "            cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "            fig.colorbar(im, cax=cax, orientation='vertical')\n",
    "\n",
    "            ax.set_title(f'T={i}: Slot {j}')\n",
    "\n",
    "            ax.axis('off')\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "985f1e75-d164-4e55-8db3-090b86bc4798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kslots_grads(grads, iEvt, color='C2',cmap='Greens'):\n",
    "    '''\n",
    "    Plot the gradients across the attention maps\n",
    "    '''\n",
    "    \n",
    "    n_iter = model.n_iter\n",
    "    k_slots = model.k_slots\n",
    "    \n",
    "    '''\n",
    "    Make the plot\n",
    "    '''\n",
    "    fig, axs = plt.subplots(n_iter+1,k_slots,figsize=(2.75*k_slots,2.5*(n_iter+1)))\n",
    " \n",
    "    for i, ax_i, att in zip(range(n_iter+1)[::-1],axs, grads):\n",
    "        \n",
    "        att_img = att[iEvt].reshape(k_slots,nPixels,nPixels)\n",
    "        \n",
    "        for j, (ax, img) in enumerate(zip(ax_i,att_img)):\n",
    "        \n",
    "            im = ax.imshow(img.cpu().numpy(),cmap=cmap)#,vmin=0,vmax=1)\n",
    "\n",
    "            divider = make_axes_locatable(ax)\n",
    "            cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "            fig.colorbar(im, cax=cax, orientation='vertical')\n",
    "\n",
    "            ax.set_title(f'T={i}: Grad attn {j}')\n",
    "\n",
    "            ax.axis('off')\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13726dbe-99b5-4000-aa80-a7d96b040424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_kslots_grads(model.gradients,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2de5114d-e8ac-4631-a8c6-36851c043323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39780164",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-fdb0c714b299>:83: RuntimeWarning: invalid value encountered in true_divide\n",
      "  eventMasks = np.where(eventHistograms>0,eventMasks/eventHistograms,eventMasks)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]]], dtype=torch.float64),\n",
       " tensor([[[-0.0762,  0.1376,  0.0332],\n",
       "          [ 0.4517,  0.1615,  0.0197]]], dtype=torch.float64),\n",
       " tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]]], dtype=torch.float64))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_batch(N_events=1,N_clusters=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7cf633ca-8fe0-4495-ace5-1c8ff0735def",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes from 3 to 4 positional arguments but 5 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-09f5673d02c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# device='cuda:2'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSlotAttentionPosEmbed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_slots\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk_slots\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2_001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-1edfee5fd969>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, resolution, k_slots, hidden_dim, query_dim, n_iter, device, dtype)\u001b[0m\n\u001b[1;32m     46\u001b[0m         )\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposEnc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSoftPositionalEmbed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresolution\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         self.init_mlp = torch.nn.Sequential(\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes from 3 to 4 positional arguments but 5 were given"
     ]
    }
   ],
   "source": [
    "bs = 1\n",
    "# device='cuda:2'\n",
    "\n",
    "model = SlotAttentionPosEmbed(k_slots=k_slots,device=device,dtype=torch.float64).to(torch.float64).to(device)\n",
    "\n",
    "model,L = train(model, 2_001,bs=bs,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0af2dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f855a87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ef86418-bdcc-4625-8339-de54c6174fed",
   "metadata": {},
   "source": [
    "## Debugging below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6d12e4-fedd-4279-895e-709a060ed5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pi_img = pairwise_cost_all.reshape(3,2,nPixels,nPixels)\n",
    "\n",
    "# fig, ax = plt.subplots(3,2,figsize=(2.5*2,2.25*3))\n",
    "\n",
    "# for i in range(3):\n",
    "#     for j in range(2):\n",
    "#         im = ax[i,j].imshow(pi_img[i,j].detach().cpu().numpy())\n",
    "          \n",
    "#         divider = make_axes_locatable(ax[i,j])\n",
    "#         cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "#         fig.colorbar(im, cax=cax, orientation='vertical')\n",
    " \n",
    "#         ax[i,j].set_title(f'slot {i}, ring {j}: L={pi_img[i,j].mean():.3f}')\n",
    "\n",
    "#         ax[i,j].axis('off')\n",
    "        \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15aaaffc-b51d-4d6c-b24e-e1eaf9b6c0ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3158d911-30cf-46f3-b41c-1e463dd7dd04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946ea533-5dca-4d5e-b5a5-ce823fb11ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "iEvt=0\n",
    "cmap='Greens'\n",
    "\n",
    "fig,axs = plt.subplots(2,k_slots,figsize=(2.5*k_slots,4.75))\n",
    "\n",
    "for i,ax in enumerate(axs[0][0:max_n_rings]):\n",
    "\n",
    "    im = ax.imshow(mask[0,i].detach().cpu(),cmap=cmap,vmin=0,vmax=1)\n",
    "\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "    fig.colorbar(im, cax=cax, orientation='vertical')\n",
    "\n",
    "    ax.set_title(f'mask {i}')\n",
    "\n",
    "    ax.axis('off')\n",
    "\n",
    "axs[0,max_n_rings].axis('off')\n",
    "    \n",
    "    \n",
    "for i,ax in enumerate(axs[1]):\n",
    "\n",
    "    im = ax.imshow(att[0,i].reshape(nPixels,nPixels).detach().cpu(),cmap=cmap,vmin=0,vmax=1)\n",
    "\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "    fig.colorbar(im, cax=cax, orientation='vertical')\n",
    "\n",
    "    ax.set_title(f'slot {i}')\n",
    "\n",
    "    ax.axis('off')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dbfd57-e05f-4c5a-96cd-b3d52ffb94d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
