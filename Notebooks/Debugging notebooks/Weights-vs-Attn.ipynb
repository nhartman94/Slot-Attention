{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e22ef0f6-682a-4efc-8f56-2e84c0f3ff68",
   "metadata": {},
   "source": [
    "# Weights vs Attention\n",
    "\n",
    "I'm curious _which_ I should choose the attention of the version of the filter normalized over all the pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9dac0704-2532-469c-967a-71b97783d589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7146ffa8-0329-4c29-a98c-02af89cf0d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 1\n",
    "max_n_rings = 2\n",
    "k_slots = 3\n",
    "nPixels=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "0f8c8fac-1078-44b7-bbff-01c1e5e3c36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "att = torch.Tensor([[.9,.9,.9,.9],\n",
    "                     [.1,0,0,0],\n",
    "                     [0,.1,.1,.1]]).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "31d659a0-3c90-4075-8ee1-431941395081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 4])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "de7ebcd3-2cf2-4727-a35d-3790fd36aed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2500, 0.2500, 0.2500, 0.2500],\n",
       "         [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.3333, 0.3333, 0.3333]]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws = att / att.sum(dim=2,keepdim=True)\n",
    "ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5866a6f1-2990-4609-ad22-56c0725430b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2500, 0.2500, 0.2500, 0.2500],\n",
       "         [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.3333, 0.3333, 0.3333]]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9673689e-9e5b-4fe5-a6df-27a4a825c752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0., 0., 0.],\n",
       "         [0., 0., 0., 1.]]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5d4dc8d8-96e2-455f-b914-4def2dac0fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.Tensor(\n",
    "    [[1,0,0,0],[0,0,0,1]]\n",
    ").unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4e07f2b4-8145-495e-8d77-09fb7afb63b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a2686038-ed3a-4355-8bf1-f9646a040038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.3333, 0.3333, 0.3333],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.3333, 0.3333, 0.3333]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3d28e18f-bfee-45f2-818b-4217cc8017d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5623, 50.0000, 25.3041],\n",
       "         [ 0.5623,  0.0000,  0.4774]]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairwise_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "40baab5a-bf55-4530-90b0-b0ed4112bcb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 4])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "6704529d-9418-496e-81b3-c75585e96f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "att_ext torch.Size([1, 3, 2, 4])\n",
      "mask_ext torch.Size([1, 3, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "att_ext  = torch.tile(att.unsqueeze(2),  dims=(1,1,max_n_rings,1)) #.reshape(bs * k_slots * max_n_rings , nPixels**2)\n",
    "mask_ext = torch.tile(mask,dims=(1,k_slots,1,1)) #.reshape(bs * k_slots * max_n_rings , nPixels**2)\n",
    "\n",
    "print('att_ext',att_ext.shape)\n",
    "print('mask_ext',mask_ext.shape)\n",
    "\n",
    "pairwise_cost = F.binary_cross_entropy(att_ext,mask_ext,reduction='none').mean(axis=-1)\n",
    "# pairwise_cost = pairwise_cost.reshape(bs, k_slots,max_n_rings)\n",
    "\n",
    "indices = list(map(linear_sum_assignment, pairwise_cost.cpu()))\n",
    "indices = torch.LongTensor(indices)\n",
    "\n",
    "transposed_indices = torch.permute(indices,(0,2,1))\n",
    "\n",
    "# return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbc39d8-a63f-4653-a55c-d2afd60129d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1836ec51-59cf-406d-9cca-4ef31c8be6be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d32f9b-23fe-4be8-9bba-7db00127236f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "34ea8ca6-9e8f-419a-b67e-a16d23c918b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 2])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "b91b056d-cb3b-44a1-9d1c-0e5dfd9837ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7533,  1.7533],\n",
       "        [ 0.5756, 25.0263],\n",
       "        [25.0790,  0.6283]])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairwise_cost[0] #.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "e4adb63e-bdfe-4287-8e2f-410524d643e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5756, 0.6283])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for pi, (ri,ci) in zip(pairwise_cost,indices):\n",
    "    break\n",
    "\n",
    "pi[ri,ci]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5cd1045d-d094-4fe4-8e13-df58c52d8ede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5d79933c-53df-411a-a72d-927fdddf34f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b558e25f-7bed-4d40-8f64-97ca8d91392f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b256e31a-ead2-4467-9594-70472bd74b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bis=torch.arange(1)\n",
    "\n",
    "slots_sorted = torch.cat([ws[bis,indices[:,0,ri]].unsqueeze(1) for ri in range(max_n_rings)],dim=1)\n",
    "\n",
    "# flat_mask = mask.reshape(-1,max_n_rings, nPixels*nPixels)\n",
    "rings_sorted = torch.cat([mask[bis,indices[:,1,ri]].unsqueeze(1) for ri in range(max_n_rings)],dim=1)\n",
    "\n",
    "# Calculate the loss\n",
    "# loss = F.binary_cross_entropy(slots_sorted,rings_sorted,reduction='none').sum(axis=1).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "67c66966-7c2d-4d4f-b246-6f6f5f4a300d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.3333, 0.3333, 0.3333]]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slots_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5a45a444-5774-4a63-ac0f-d1ae8b37fbeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1.]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rings_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b32ccb0a-aa00-4ebd-a2e5-0690dd990bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(75.3041)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.binary_cross_entropy(slots_sorted,rings_sorted,reduction='none').sum(axis=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be468ef3-2a2f-4474-aec6-85fb045d57bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "263e9f3f-7ee0-4839-bf2d-6282b0e6d1a1",
   "metadata": {},
   "source": [
    "**Just try to understand the string manipulation!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "543b4b91-ff06-44f3-b8b6-0fa0a4ac2284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 4])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00536407-efe8-4615-890c-5e5e2d6eb357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# att = torch.Tensor([[10.1, 10.2, 10.3, 10.4],\n",
    "#                     [20.1, 20.2, 20.3, 20.4],\n",
    "#                     # [30.1, 30.2, 30.3, 30.4],\n",
    "#                    ])\n",
    "# mask = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5675e297-e5b3-441d-8a7c-435077e8af96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ddd046ff-fe4d-41c9-8a04-217021a2a6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = 10* (torch.arange(k_slots)+1).reshape(1,-1,1)\n",
    "\n",
    "trgt = (torch.arange(max_n_rings)+1).reshape(1,1,-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ac515bb3-4de0-46ee-aa99-272b22b5260e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[10],\n",
       "         [20],\n",
       "         [30]]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "cf11fc58-601a-448e-986c-95f9d3cbe23d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2]]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "14de7934-d963-4a03-8437-d14f5605d60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tile = torch.tile(pred,dims=(1,1,max_n_rings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "fa4cc6fe-1301-4d2c-b46c-5bf8689c04cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "trgt_tile = torch.tile(trgt,dims=(1,k_slots,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "7862610c-b845-412f-a499-5c26803d5478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[11, 12],\n",
       "         [21, 22],\n",
       "         [31, 32]]])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_tile+trgt_tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e49d4f45-3891-49ae-a461-b36a489e9114",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_flat = pred_tile.reshape(1,k_slots*max_n_rings)\n",
    "trgt_flat = trgt_tile.reshape(1,k_slots*max_n_rings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "241cd235-0e32-419a-adea-d0f3024b6bff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[11, 12],\n",
       "         [21, 22],\n",
       "         [31, 32]]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pred_flat+trgt_flat).reshape((1,k_slots,max_n_rings))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59fe12c-6328-4461-a855-6ab84b3beec8",
   "metadata": {},
   "source": [
    "**OK, I made a dummy example where reshaping _worked_**\n",
    "\n",
    "So... I'm not sure what wasn't working... was it not working when I have other dimensions involved?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "e665b98f-42b3-4346-a723-ea7827ab37ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = 10* (torch.arange(k_slots)+1).reshape(1,-1,1,1) + 0.001*torch.randn(1,k_slots,1,2)\n",
    "\n",
    "trgt = (torch.arange(max_n_rings)+1).reshape(1,1,-1,1) + 0.001*torch.randn(1,1,max_n_rings,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a94a27ad-91a8-48f3-8ee9-44d7d23d9faf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 9.9999,  9.9991]],\n",
       "\n",
       "         [[19.9988, 19.9977]],\n",
       "\n",
       "         [[29.9994, 30.0000]]]])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "e2b44eec-5168-4c8b-aa1f-ee4d81f6a3a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.9995, 0.9990],\n",
       "          [1.9982, 1.9999]]]])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26333ed9-4583-43ca-b271-77e2b84be1a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23909e94-e7a1-42d1-bb58-1298726a9d8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11b6f3f-4504-48c3-a914-92c539758203",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_flat = pred_tile.reshape(1,k_slots*max_n_rings)\n",
    "trgt_flat = trgt_tile.reshape(1,k_slots*max_n_rings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
