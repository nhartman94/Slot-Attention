{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dec626b3-c403-49ea-9376-f47e515c6347",
   "metadata": {},
   "source": [
    "# Implementation of ResNet to ISA \n",
    "\n",
    "Before I do something stupid I test it here to see how it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94791f0b-51dd-4bba-b969-c738c977abe9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mlp\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import json, yaml, os\n",
    "os.sys.path.append('./../../code')\n",
    "\n",
    "from plotting import plot_kslots, plot_kslots_iters\n",
    "from data_scclevr import makeRings \n",
    "from model import InvariantSlotAttention\n",
    "\n",
    "from matplotlib.patches import Circle\n",
    "import json\n",
    "\n",
    "# Set numpy seed for test set sampling \n",
    "torch_seed = 24082023\n",
    "torch.manual_seed( torch_seed )\n",
    "\n",
    "import random\n",
    "random.seed(torch_seed)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from plotting import plot_chosen_slots, plot_kslots, plot_kslots_iters, plot_kslots_grads\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15fc8a57-c62e-481b-9c56-acc1089e5fe5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn import init\n",
    "import scclevr\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b1bbd3c-5199-4227-85b3-78a81321ca25",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "cID_prev = 'isa-alpha3_scclevr'\n",
    "with open(f'./../../code/configs/{cID_prev}.yaml') as f:\n",
    "    cd = yaml.safe_load(f)\n",
    "\n",
    "hps = cd['hps']\n",
    "hps['device'] = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c222307f-18e4-4107-ae7a-fdd9afcf75cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Sampling(nn.Module):\n",
    "    def forward(self, z_mean, z_log_var):\n",
    "        # get the shape of the tensor for the mean and log variance\n",
    "        batch, dim = z_mean.shape\n",
    "        # generate a normal random tensor (epsilon) with the same shape as z_mean\n",
    "        # this tensor will be used for reparameterization trick\n",
    "        epsilon = Normal(0, 1).sample((batch, dim)).to(z_mean.device)\n",
    "        # apply the reparameterization trick to generate the samples in the\n",
    "        # latent space\n",
    "        return z_mean + torch.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "faa22e39-fa95-4408-90c4-4b962918628f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ModifiedResNet18(nn.Module):\n",
    "    def __init__(self, kwargs):\n",
    "        super(ModifiedResNet18, self).__init__()\n",
    "        resnet18 = models.resnet18()\n",
    "        resnet18.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.features = nn.Sequential(\n",
    "            *list(resnet18.children())[:-2]  # Remove the last global average pooling and fully connected layer\n",
    "        )\n",
    "        self.conv = nn.Conv2d(512, **kwargs)  # Modify this to suit your needs\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.conv(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5ee62874-9868-4153-ab92-7b0e7b65b174",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class InvariantSlotAttention_ResNet(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 resolution=(32,32),\n",
    "                 xlow=-0.5,\n",
    "                 xhigh=0.5,\n",
    "                 varlow=0.01,\n",
    "                 varhigh=0.05,\n",
    "                 k_slots=3, \n",
    "                 num_conv_layers=3,\n",
    "                 which_encoder='ResNet',\n",
    "                 hidden_dim=32, \n",
    "                 query_dim=32, \n",
    "                 n_iter=2,\n",
    "                 pixel_mult=1,\n",
    "                 device='cpu' ,\n",
    "                 learn_slot_feat=True\n",
    "                 ):\n",
    "        '''\n",
    "        Slot attention encoder block, block attention\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.k_slots = k_slots\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.query_dim = query_dim\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "        self.resolution = resolution\n",
    "        self.xlow, self.xhigh = xlow, xhigh\n",
    "        self.rlow, self.rhigh = np.sqrt(varlow), np.sqrt(varhigh)\n",
    "        \n",
    "        self.device=device\n",
    "         \n",
    "        self.softmax_T = 1/np.sqrt(query_dim)\n",
    "        \n",
    "        self.dataN = torch.nn.LayerNorm(self.hidden_dim)\n",
    "        self.queryN = torch.nn.LayerNorm(self.query_dim)\n",
    "        \n",
    "        self.toK = torch.nn.Linear(self.hidden_dim, self.query_dim)\n",
    "        self.toV = torch.nn.Linear(self.hidden_dim, self.query_dim)\n",
    "        self.gru = torch.nn.GRUCell(self.query_dim, self.query_dim)\n",
    "        \n",
    "        '''\n",
    "        CNN feature extractor\n",
    "        '''\n",
    "        kwargs = {'out_channels': hidden_dim,'kernel_size': 5, 'padding':2 }\n",
    "        cnn_layers = [torch.nn.Conv2d(1,**kwargs)]\n",
    "        for i in range(num_conv_layers-1):\n",
    "            cnn_layers += [torch.nn.ReLU(), torch.nn.Conv2d(hidden_dim,**kwargs)] \n",
    "        cnn_layers.append(torch.nn.ReLU())\n",
    "          \n",
    "        self.CNN_encoder = torch.nn.Sequential(*cnn_layers) # 3 CNN layers by default\n",
    "        if which_encoder=='ResNet':\n",
    "            #self.CNN_encoder = Encoder_resnet()\n",
    "            \n",
    "            self.CNN_encoder = ModifiedResNet18(kwargs)\n",
    "            \n",
    "            \n",
    "            #self.CNN_encoder = models.resnet18()  # You can choose a different ResNet variant\n",
    "            #in_features = self.CNN_encoder.fc.in_features\n",
    "            #self.CNN_encoder.fc = torch.nn.Identity()  # Remove the final fully connected layer\n",
    "            #self.CNN_encoder.conv1 = torch.nn.Conv2d(1, 64, kernel_size=5, padding=2)  # Adjust the input channels\n",
    "\n",
    "            \n",
    "          \n",
    "            \n",
    "        # Grid + query init\n",
    "        self.abs_grid = self.build_grid()\n",
    "                   \n",
    "        self.dense = torch.nn.Linear(2, query_dim) \n",
    "        self.pixel_mult = pixel_mult # LH's proposal... but almost same as 1/delta in ISA\n",
    "\n",
    "        # Apply after the data normalization\n",
    "        self.init_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(query_dim,query_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(query_dim,query_dim)\n",
    "        )\n",
    "            \n",
    "        self.slots_mu = torch.nn.Parameter(torch.randn(1, 1, self.query_dim))\n",
    "        self.slots_logsigma = torch.nn.Parameter(torch.zeros(1, 1, self.query_dim))\n",
    "        init.xavier_uniform_(self.slots_logsigma)\n",
    "\n",
    "        self.init_slots = self.init_slots\n",
    "\n",
    "        \n",
    "        '''\n",
    "        Option to add a final (x,y,r) prediction to each slot\n",
    "        '''\n",
    "        self.learn_slot_feat = learn_slot_feat\n",
    "        if self.learn_slot_feat:\n",
    "            self.final_mlp = torch.nn.Sequential(\n",
    "                torch.nn.Linear(query_dim,hidden_dim),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(hidden_dim, 3)\n",
    "            )\n",
    "        \n",
    "    def build_grid(self):\n",
    "        '''\n",
    "        From google slot attention repo:\n",
    "        https://github.com/nhartman94/google-research/blob/master/slot_attention/model.py#L357C1-L364C53\n",
    "        '''\n",
    "        resolution = self.resolution\n",
    "        xlow, xhigh = self.xlow, self.xhigh\n",
    "           \n",
    "        ranges = [np.linspace(xlow, xhigh, num=res) for res in resolution]\n",
    "        grid = np.meshgrid(*ranges, sparse=False, indexing=\"xy\")\n",
    "        grid = np.stack(grid, axis=-1)\n",
    "        grid = np.reshape(grid, [resolution[0], resolution[1], -1])\n",
    "        grid = np.expand_dims(grid, axis=0)\n",
    "        \n",
    "        grid = torch.FloatTensor( grid ).to(self.device)\n",
    "        grid = torch.flatten(grid,1,2)\n",
    "    \n",
    "        return grid\n",
    "                \n",
    "    def init_slots(self,Nbatch):\n",
    "        '''\n",
    "        Slot init taken from\n",
    "        https://github.com/lucidrains/slot-attention/blob/master/slot_attention/slot_attention.py\n",
    "        '''\n",
    "        \n",
    "        stdhigh, stdlow = self.rlow, self.rhigh\n",
    "        \n",
    "        mu = self.slots_mu.expand(Nbatch, self.k_slots, -1)\n",
    "        sigma = self.slots_logsigma.exp().expand(Nbatch, self.k_slots, -1)\n",
    "    \n",
    "        queries = mu + sigma * torch.randn(mu.shape,device=self.device)\n",
    "    \n",
    "        # Add the position and scale initialization for the local ref frame\n",
    "        ref_frame_dim = 3\n",
    "        pos_scale = torch.rand(Nbatch, self.k_slots, ref_frame_dim,device=self.device)\n",
    "\n",
    "        pos_scale[:,:2] -= 0.5\n",
    "        pos_scale[:,-1]  = (stdhigh - stdlow) * pos_scale[:,-1] + stdlow\n",
    "        \n",
    "        return queries, pos_scale\n",
    "     \n",
    "    def get_keys_vals(self, encoded_data, pos_scale):\n",
    "\n",
    "        # Get the relative position embedding\n",
    "        rel_grid = self.abs_grid.unsqueeze(1) - pos_scale[:,:,:2].unsqueeze(2)\n",
    "        rel_grid /= pos_scale[:,:,-1].unsqueeze(2).unsqueeze(-1)\n",
    "        \n",
    "        # Embed it in the same space as the query dimension \n",
    "        embed_grid = self.pixel_mult * self.dense( rel_grid )\n",
    "        \n",
    "        # keys, vals: (bs, img_dim, query_dim)\n",
    "        keys = self.toK(encoded_data).unsqueeze(1) + embed_grid\n",
    "        vals = self.toV(encoded_data).unsqueeze(1) + embed_grid\n",
    "        \n",
    "        keys = self.init_mlp(self.queryN(keys))\n",
    "        vals = self.init_mlp(self.queryN(vals))\n",
    "        \n",
    "        return keys, vals\n",
    "                \n",
    "    def attention_and_weights(self,queries,keys):\n",
    "        \n",
    "        logits = torch.einsum('bse,bsde->bsd',queries,keys) * self.softmax_T\n",
    "        \n",
    "        att = torch.nn.functional.softmax(logits, dim = 1)\n",
    "        \n",
    "        div = torch.sum(att, dim = -1, keepdims = True)\n",
    "        wts = att/div + 1e-8\n",
    "        return att,wts\n",
    "\n",
    "    def update_frames(self,wts):\n",
    "        '''\n",
    "        Update the relative frame position\n",
    "        '''\n",
    "        \n",
    "        # expand to include the batch dim\n",
    "        grid_exp = self.abs_grid.expand(wts.shape[0],-1,2)\n",
    "        \n",
    "        new_pos = torch.einsum('bsd,bde->bse',wts,grid_exp)\n",
    "        \n",
    "        new_scale = torch.sum(torch.pow(grid_exp.unsqueeze(1) - new_pos.unsqueeze(2),2),dim=-1)\n",
    "        \n",
    "        new_scale = torch.einsum('bsd,bsd->bs', wts, new_scale)\n",
    "        new_scale = torch.sqrt(new_scale)\n",
    "        \n",
    "        return torch.cat([new_pos,new_scale.unsqueeze(-1)],axis=-1)\n",
    "        \n",
    "    def iterate(self, queries, pos_scale, encoded_data):\n",
    "        \n",
    "        # Get the keys and values in the ref ref frame\n",
    "        keys, vals = self.get_keys_vals(encoded_data,pos_scale)\n",
    "        \n",
    "        # att,wts: (bs, k_slots, img_dim)\n",
    "        att,wts = self.attention_and_weights(self.queryN(queries),keys)   \n",
    "        \n",
    "        new_pos_scale = self.update_frames(wts)\n",
    "        \n",
    "        # Update the queries with the recurrent block\n",
    "        updates = torch.einsum('bsd,bsde->bse',wts,vals) # bs, n_slots, query_dim\n",
    "        \n",
    "        updates = self.gru(\n",
    "            updates.reshape(-1,self.query_dim),\n",
    "            queries.reshape(-1,self.query_dim),\n",
    "        )\n",
    "        \n",
    "        return updates.reshape(queries.shape), new_pos_scale\n",
    "        \n",
    "    def forward(self, data):\n",
    "    \n",
    "        '''\n",
    "        Step 1: Extract the CNN features\n",
    "        '''\n",
    "        print('data shape', data.shape)\n",
    "        encoded_data = self.CNN_encoder(data) # Apply the CNN encoder\n",
    "    \n",
    "        print(encoded_data.shape) \n",
    "        # for ModifiedResNet18: torch.Size([32, 16, 1, 1]) but should be torch.Size([32, 16, 32, 32]) with [bs, kernel, picture] - at least no error anymore!\n",
    "        encoded_data = torch.permute(encoded_data,(0,2,3,1)) # Put channel dim at the end\n",
    "        encoded_data = torch.flatten(encoded_data,1,2) # flatten pixel dims\n",
    "        print(encoded_data.shape)\n",
    "        encoded_data = self.dataN(encoded_data)\n",
    "        \n",
    "        '''\n",
    "        Step 2: Initialize the slots\n",
    "        '''\n",
    "        Nbatch = data.shape[0]\n",
    "        queries, pos_scale = self.init_slots(Nbatch) # Shape (Nbatch, k_slots, query_dim)\n",
    "                \n",
    "        '''\n",
    "        Step 3: Iterate through the reconstruction\n",
    "        '''\n",
    "        for i in range(self.n_iter):\n",
    "            queries, pos_scale = self.iterate(queries, pos_scale, encoded_data)    \n",
    "            \n",
    "        # With the final query vector, calc the attn, weights, + rel ref frames\n",
    "        keys, vals = self.get_keys_vals(encoded_data,pos_scale)\n",
    "        att, wts = self.attention_and_weights(self.queryN(queries),keys)   \n",
    "        new_pos_scale = self.update_frames(wts)\n",
    "                \n",
    "        if self.learn_slot_feat:\n",
    "            slot_feat = self.final_mlp(queries)\n",
    "            \n",
    "            # Want to learn the delta from the previously estimated position\n",
    "            slot_feat += new_pos_scale\n",
    "            \n",
    "            return queries, att, slot_feat \n",
    "        \n",
    "        else:\n",
    "            return queries, att, wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "930f9f2e-3c79-4395-b841-a184bd9dcb15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hungarian_matching(pairwise_cost):\n",
    "    '''\n",
    "    Input:\n",
    "    - pairwise_cost\n",
    "\n",
    "\n",
    "    Hungarian section Translated from the TensorFlow loss function (from 2006.15055 code):\n",
    "    https://github.com/nhartman94/google-research/blob/master/slot_attention/utils.py#L26-L57\n",
    "    '''\n",
    "    \n",
    "    indices = list(map(linear_sum_assignment, pairwise_cost.cpu()))\n",
    "    indices = torch.LongTensor(np.array(indices))\n",
    "    \n",
    "    loss = 0\n",
    "    for pi,(ri,ci) in zip(pairwise_cost,indices):\n",
    "        loss += pi[ri,ci].sum()\n",
    "    \n",
    "    return indices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "687fc01c-514c-4eec-8fa8-64053c044178",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _convert_into_pytorch_tensors(event_images, object_images, n_objects, object_features, device):\n",
    "    return torch.FloatTensor(event_images).to(device), \\\n",
    "               torch.FloatTensor(object_images).to(device), \\\n",
    "               torch.FloatTensor(n_objects).to(device), \\\n",
    "               torch.FloatTensor(object_features).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "dd3a54ee-d779-4655-a682-5531ba165957",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = InvariantSlotAttention_ResNet(**hps).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1a28c144-ef23-45e4-9aa2-0ce29b924f08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Ntrain = 5000 \n",
    "bs=32 \n",
    "lr=3e-4\n",
    "warmup_steps=5_000\n",
    "alpha=1\n",
    "losses = {'tot':[],'bce':[],'mse':[]}\n",
    "kwargs={'isRing': True, 'N_clusters':2}\n",
    "clip_val = 1\n",
    "device='cpu'\n",
    "plot_every=20 \n",
    "save_every=1000\n",
    "color='C0'\n",
    "cmap='Blues'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4d6944d5-3f7f-46f9-bc97-1e8fc77b4590",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Learning rate schedule config\n",
    "base_learning_rate = lr\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), base_learning_rate)\n",
    "model.train()\n",
    "\n",
    "k_slots = model.k_slots\n",
    "max_n_rings = kwargs['N_clusters']\n",
    "resolution = model.resolution\n",
    "kwargs['device'] = device\n",
    "N_obj = kwargs['N_clusters'] # pass to makeRing fct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d1db4ad3-4a07-4b0a-8b79-cbafda158442",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape torch.Size([32, 1, 32, 32])\n",
      "torch.Size([32, 16, 1, 1])\n",
      "torch.Size([32, 1, 16])\n",
      "torch.float32\n",
      "data shape torch.Size([32, 1, 32, 32])\n",
      "torch.Size([32, 16, 1, 1])\n",
      "torch.Size([32, 1, 16])\n",
      "torch.float32\n",
      "data shape torch.Size([32, 1, 32, 32])\n",
      "torch.Size([32, 16, 1, 1])\n",
      "torch.Size([32, 1, 16])\n",
      "torch.float32\n",
      "data shape torch.Size([32, 1, 32, 32])\n",
      "torch.Size([32, 16, 1, 1])\n",
      "torch.Size([32, 1, 16])\n",
      "torch.float32\n",
      "data shape torch.Size([32, 1, 32, 32])\n",
      "torch.Size([32, 16, 1, 1])\n",
      "torch.Size([32, 1, 16])\n",
      "torch.float32\n",
      "data shape torch.Size([32, 1, 32, 32])\n",
      "torch.Size([32, 16, 1, 1])\n",
      "torch.Size([32, 1, 16])\n",
      "torch.float32\n",
      "data shape torch.Size([32, 1, 32, 32])\n",
      "torch.Size([32, 16, 1, 1])\n",
      "torch.Size([32, 1, 16])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[108], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m mask \u001b[38;5;241m=\u001b[39m object_images\n\u001b[1;32m     17\u001b[0m Y \u001b[38;5;241m=\u001b[39m object_features\n\u001b[0;32m---> 19\u001b[0m queries, att, Y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Reshape the target mask to be flat in the pixels (same shape as att)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m flat_mask \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,max_n_rings, np\u001b[38;5;241m.\u001b[39mprod(resolution))   \n",
      "File \u001b[0;32m/raven/u/saumi/nn-venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/raven/u/saumi/nn-venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[102], line 231\u001b[0m, in \u001b[0;36mInvariantSlotAttention_ResNet.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03mStep 3: Iterate through the reconstruction\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter):\n\u001b[0;32m--> 231\u001b[0m     queries, pos_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoded_data\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[1;32m    233\u001b[0m \u001b[38;5;66;03m# With the final query vector, calc the attn, weights, + rel ref frames\u001b[39;00m\n\u001b[1;32m    234\u001b[0m keys, vals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_keys_vals(encoded_data,pos_scale)\n",
      "Cell \u001b[0;32mIn[102], line 187\u001b[0m, in \u001b[0;36mInvariantSlotAttention_ResNet.iterate\u001b[0;34m(self, queries, pos_scale, encoded_data)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21miterate\u001b[39m(\u001b[38;5;28mself\u001b[39m, queries, pos_scale, encoded_data):\n\u001b[1;32m    185\u001b[0m     \n\u001b[1;32m    186\u001b[0m     \u001b[38;5;66;03m# Get the keys and values in the ref ref frame\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m     keys, vals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_keys_vals\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpos_scale\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;66;03m# att,wts: (bs, k_slots, img_dim)\u001b[39;00m\n\u001b[1;32m    190\u001b[0m     att,wts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_and_weights(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueryN(queries),keys)   \n",
      "Cell \u001b[0;32mIn[102], line 153\u001b[0m, in \u001b[0;36mInvariantSlotAttention_ResNet.get_keys_vals\u001b[0;34m(self, encoded_data, pos_scale)\u001b[0m\n\u001b[1;32m    150\u001b[0m vals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoV(encoded_data)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m embed_grid\n\u001b[1;32m    152\u001b[0m keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_mlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueryN(keys))\n\u001b[0;32m--> 153\u001b[0m vals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_mlp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqueryN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvals\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keys, vals\n",
      "File \u001b[0;32m/raven/u/saumi/nn-venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/raven/u/saumi/nn-venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/raven/u/saumi/nn-venv/lib/python3.10/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/raven/u/saumi/nn-venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/raven/u/saumi/nn-venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/raven/u/saumi/nn-venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "start = len(losses)\n",
    "for i in range(start,start+Ntrain):\n",
    "\n",
    "    learning_rate = base_learning_rate * 0.5 * (1 + np.cos(np.pi * i / Ntrain))\n",
    "    if i < warmup_steps:\n",
    "        learning_rate *= (i / warmup_steps)\n",
    "    \n",
    "    opt.param_groups[0]['lr'] = learning_rate\n",
    "    \n",
    "    # make scclevr data\n",
    "    rings = scclevr.RingsBinaryUniform(N_obj) # two rings per imagne\n",
    "    event_images, object_images, n_objects, object_features =  rings.gen_events(bs)\n",
    "    event_images, object_images, n_objects, object_features =  _convert_into_pytorch_tensors(event_images, object_images, n_objects, object_features, device)\n",
    "    # assign shorter names\n",
    "    X = event_images\n",
    "    mask = object_images\n",
    "    Y = object_features\n",
    "    \n",
    "    queries, att, Y_pred = model(X)\n",
    "        \n",
    "    # Reshape the target mask to be flat in the pixels (same shape as att)\n",
    "    flat_mask = mask.reshape(-1,max_n_rings, np.prod(resolution))   \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        att_ext  = torch.tile(att.unsqueeze(2), dims=(1,1,max_n_rings,1)) \n",
    "        mask_ext = torch.tile(flat_mask.unsqueeze(1),dims=(1,k_slots,1,1)) \n",
    "        \n",
    "        pairwise_cost = F.binary_cross_entropy(att_ext,mask_ext,reduction='none').mean(axis=-1)\n",
    "        \n",
    "        # pairwise_cost = comb_loss(att,flat_mask,Y,Y_pred,alpha)\n",
    "        indices = hungarian_matching(pairwise_cost)\n",
    "\n",
    "    # Apply the sorting to the predict\n",
    "    bis=torch.arange(bs).to(device)\n",
    "    indices=indices.to(device)\n",
    "    # Loss calc\n",
    "    slots_sorted = torch.cat([att[bis,indices[:,0,ri]].unsqueeze(1) for ri in range(max_n_rings)],dim=1)\n",
    "    rings_sorted = torch.cat([flat_mask[bis,indices[:,1,ri]].unsqueeze(1) for ri in range(max_n_rings)],dim=1)\n",
    "    l_bce = F.binary_cross_entropy(slots_sorted,rings_sorted,reduction='none').sum(axis=1).mean()\n",
    "    \n",
    "    Y_pred_sorted = torch.cat([Y_pred[bis,indices[:,0,ri]].unsqueeze(1) for ri in range(max_n_rings)],dim=1)\n",
    "    Y_true_sorted = torch.cat([Y[bis,indices[:,1,ri]].unsqueeze(1) for ri in range(max_n_rings)],dim=1)\n",
    "\n",
    "    l_mse = torch.nn.MSELoss(reduction='none')(Y_pred_sorted,Y_true_sorted).sum(axis=1).mean()\n",
    "\n",
    "    # Calculate the loss\n",
    "    li = l_bce + alpha*l_mse\n",
    "    print(li.dtype)\n",
    "    \n",
    "    li.backward()\n",
    "    clip_val=1\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip_val)\n",
    "    \n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "\n",
    "    losses['tot'].append(float(li))\n",
    "    losses['bce'].append(float(l_bce))\n",
    "    losses['mse'].append(float(l_mse))\n",
    "    \n",
    "    if i % plot_every == 0:\n",
    "        print('iter',i,', loss',li.detach().cpu().numpy(),', lr',opt.param_groups[0]['lr'])  \n",
    "        iEvt = 0\n",
    "\n",
    "        # losses, mask, att_img, Y_true, Y_pred\n",
    "        plot_chosen_slots(losses,\n",
    "                            mask[iEvt].sum(axis=0), \n",
    "                            slots_sorted[iEvt].reshape(max_n_rings,*resolution),\n",
    "                            Y_true_sorted[iEvt],\n",
    "                            Y_pred_sorted[iEvt])\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1234f30c-c786-4bbf-85c0-6ce20e1782d9",
   "metadata": {},
   "source": [
    "Ok. It runs! Now try to adjust with ResNet!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6f1222fa-3c48-4daf-8fae-5b5ca85d6fa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, activation, pre_act):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.activation = activation\n",
    "        self.pre_act = pre_act\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv_res = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, inputs, inputs_scaled):\n",
    "        x = inputs\n",
    "        y = self.conv1(x)\n",
    "        y = self.bn1(y)\n",
    "        y = self.activation(y)\n",
    "        y = self.conv2(y)\n",
    "        y = self.bn2(y)\n",
    "        if self.pre_act:\n",
    "            y = self.activation(y)\n",
    "            \n",
    "        x = self.conv_res(x)\n",
    "        x = x + y\n",
    "        if not self.pre_act:\n",
    "            x = self.activation(x)\n",
    "            \n",
    "        x = torch.cat((x, inputs_scaled), dim=1)\n",
    "        return x\n",
    "\n",
    "class Encoder_resnet(nn.Module):\n",
    "    def __init__(self, nPixels=32, latent_dim=128, nMaxClusters=2, activation='mish', use_vae=False, pre_act=False, filters=[16,16,32]):\n",
    "        '''differneces to Sanzianas code: latent_dim=3 (128)'''\n",
    "        super(Encoder_resnet, self).__init__()\n",
    "        #self.initial_conv = nn.Conv2d(3, filters[0], kernel_size=3, stride=1, padding=1)\n",
    "        self.res_blocks = nn.ModuleList()\n",
    "        self.activation = activation\n",
    "        self.use_vae = use_vae\n",
    "        \n",
    "        self.res_block11 = ResidualBlock(1,16, activation, pre_act)\n",
    "        self.res_block12 = ResidualBlock(19,16, activation, pre_act)\n",
    "        self.res_block13 = ResidualBlock(19,16, activation, pre_act)\n",
    "        self.res_block14 = ResidualBlock(19,16, activation, pre_act)\n",
    "        self.res_block15 = ResidualBlock(19,16, activation, pre_act)\n",
    "\n",
    "        self.res_block21 = ResidualBlock(19,16, activation, pre_act)\n",
    "        self.res_block22 = ResidualBlock(19,16, activation, pre_act)\n",
    "        self.res_block23 = ResidualBlock(19,16, activation, pre_act)\n",
    "        self.res_block24 = ResidualBlock(19,16, activation, pre_act)\n",
    "        self.res_block25 = ResidualBlock(19,16, activation, pre_act)\n",
    "\n",
    "        self.res_block31 = ResidualBlock(19,32, activation, pre_act)\n",
    "        self.res_block32 = ResidualBlock(35,32, activation, pre_act)\n",
    "        self.res_block33 = ResidualBlock(35,32, activation, pre_act)\n",
    "        self.res_block34 = ResidualBlock(35,32, activation, pre_act)\n",
    "        self.res_block35 = ResidualBlock(35,32, activation, pre_act)\n",
    "        \n",
    "        self.pooling = nn.MaxPool2d(kernel_size=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense = nn.Linear((filters[-1]+3) * nPixels * nPixels // (2 ** len(filters))**2, 256, bias=False)\n",
    "       \n",
    "        self.bn_dense = nn.BatchNorm1d(256)\n",
    "        self.sampling_layer = Sampling()\n",
    "\n",
    "        if use_vae:\n",
    "            self.z_mean_full = nn.Linear(256, nMaxClusters*latent_dim)\n",
    "            self.z_log_var_full = nn.Linear(256,nMaxClusters*latent_dim)\n",
    "        else:\n",
    "            self.z_full = nn.Linear(256, nMaxClusters*latent_dim)\n",
    "            \n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        inputs_scaled = inputs\n",
    "        #inputs_scaled = F.interpolate(x, size=(nPixels, nPixels))\n",
    "\n",
    "        inputs_scaled = F.interpolate(inputs_scaled, size=(x.shape[2], x.shape[3]))\n",
    "        x = self.res_block11(x,inputs_scaled)\n",
    "        x = self.res_block12(x,inputs_scaled)\n",
    "        x = self.res_block13(x,inputs_scaled)\n",
    "        x = self.res_block14(x,inputs_scaled)\n",
    "        x = self.res_block15(x,inputs_scaled)  \n",
    "        x = self.pooling(x)\n",
    "\n",
    "        inputs_scaled = F.interpolate(inputs_scaled, size=(x.shape[2], x.shape[3]))\n",
    "        x = self.res_block21(x,inputs_scaled)\n",
    "        x = self.res_block22(x,inputs_scaled)\n",
    "        x = self.res_block23(x,inputs_scaled)\n",
    "        x = self.res_block24(x,inputs_scaled)\n",
    "        x = self.res_block25(x,inputs_scaled)\n",
    "        x = self.pooling(x)\n",
    "\n",
    "        inputs_scaled = F.interpolate(inputs_scaled, size=(x.shape[2], x.shape[3]))\n",
    "        x = self.res_block31(x,inputs_scaled)\n",
    "        x = self.res_block32(x,inputs_scaled)\n",
    "        x = self.res_block33(x,inputs_scaled)\n",
    "        x = self.res_block34(x,inputs_scaled)\n",
    "        x = self.res_block35(x,inputs_scaled)\n",
    "        x = self.pooling(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        #print(x.shape)\n",
    "        x = self.dense(x)\n",
    "        \n",
    "        x = self.bn_dense(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        if self.use_vae:\n",
    "            z_mean_full = self.z_mean_full(x)\n",
    "            z_log_var_full = self.z_log_var_full(x)\n",
    "            z_full = self.sampling_layer(z_mean_full, z_log_var_full)\n",
    "            return z_mean_full, z_log_var_full, z_full\n",
    "        else:\n",
    "            z_full = self.z_full(x)\n",
    "            return z_full, z_full, z_full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e54507a-2321-426f-aa65-2eefb0c9cc0c",
   "metadata": {},
   "source": [
    "Ok, let's back off a bit? Implement torch network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed60f43c-d047-4fff-ae8c-1256b70425d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686051d6-765e-4350-9baf-5adb0e71f253",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sarasvenv",
   "language": "python",
   "name": "sarasvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
