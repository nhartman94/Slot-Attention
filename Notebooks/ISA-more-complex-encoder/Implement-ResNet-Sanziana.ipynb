{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dec626b3-c403-49ea-9376-f47e515c6347",
   "metadata": {},
   "source": [
    "# Implementation of ResNet to ISA \n",
    "\n",
    "Let's implement Sanzianas ResNet encoder!\n",
    "\n",
    "Ok, it works kind of... but only because I've added a transverse CNN layer to upsample from a [bs, 33, 4, 4] to [bs, 16, 32, 32]. Maybe this is better solved with a Linear layers which is then reshaped into [bs, 16, 32, 32]?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94791f0b-51dd-4bba-b969-c738c977abe9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mlp\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import json, yaml, os\n",
    "os.sys.path.append('./../../code')\n",
    "\n",
    "from plotting import plot_kslots, plot_kslots_iters\n",
    "from data_scclevr import makeRings \n",
    "from model import InvariantSlotAttention\n",
    "\n",
    "from matplotlib.patches import Circle\n",
    "import json\n",
    "\n",
    "# Set numpy seed for test set sampling \n",
    "torch_seed = 24082023\n",
    "torch.manual_seed( torch_seed )\n",
    "\n",
    "import random\n",
    "random.seed(torch_seed)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from plotting import plot_chosen_slots, plot_kslots, plot_kslots_iters, plot_kslots_grads\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15fc8a57-c62e-481b-9c56-acc1089e5fe5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn import init\n",
    "import scclevr\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea26b8a4-9902-494b-8221-e989c2c7f69a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b1bbd3c-5199-4227-85b3-78a81321ca25",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "cID_prev = 'isa-alpha3_scclevr'\n",
    "with open(f'./../../code/configs/{cID_prev}.yaml') as f:\n",
    "    cd = yaml.safe_load(f)\n",
    "\n",
    "hps = cd['hps']\n",
    "hps['device'] = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c222307f-18e4-4107-ae7a-fdd9afcf75cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Sampling(nn.Module):\n",
    "    def forward(self, z_mean, z_log_var):\n",
    "        # get the shape of the tensor for the mean and log variance\n",
    "        batch, dim = z_mean.shape\n",
    "        # generate a normal random tensor (epsilon) with the same shape as z_mean\n",
    "        # this tensor will be used for reparameterization trick\n",
    "        epsilon = Normal(0, 1).sample((batch, dim)).to(z_mean.device)\n",
    "        # apply the reparameterization trick to generate the samples in the\n",
    "        # latent space\n",
    "        return z_mean + torch.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa22e39-fa95-4408-90c4-4b962918628f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c73baf-9059-4697-ac9b-07f12cbaa39c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ee62874-9868-4153-ab92-7b0e7b65b174",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class InvariantSlotAttention_ResNet(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 resolution=(32,32),\n",
    "                 xlow=-0.5,\n",
    "                 xhigh=0.5,\n",
    "                 varlow=0.01,\n",
    "                 varhigh=0.05,\n",
    "                 k_slots=3, \n",
    "                 num_conv_layers=3,\n",
    "                 which_encoder='ResNet',\n",
    "                 hidden_dim=32, \n",
    "                 query_dim=32, \n",
    "                 n_iter=2,\n",
    "                 pixel_mult=1,\n",
    "                 device='cpu' ,\n",
    "                 learn_slot_feat=True\n",
    "                 ):\n",
    "        '''\n",
    "        Slot attention encoder block, block attention\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.k_slots = k_slots\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.query_dim = query_dim\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "        self.resolution = resolution\n",
    "        self.xlow, self.xhigh = xlow, xhigh\n",
    "        self.rlow, self.rhigh = np.sqrt(varlow), np.sqrt(varhigh)\n",
    "        \n",
    "        self.device=device\n",
    "         \n",
    "        self.softmax_T = 1/np.sqrt(query_dim)\n",
    "        \n",
    "        self.dataN = torch.nn.LayerNorm(self.hidden_dim)\n",
    "        self.queryN = torch.nn.LayerNorm(self.query_dim)\n",
    "        \n",
    "        self.toK = torch.nn.Linear(self.hidden_dim, self.query_dim)\n",
    "        self.toV = torch.nn.Linear(self.hidden_dim, self.query_dim)\n",
    "        self.gru = torch.nn.GRUCell(self.query_dim, self.query_dim)\n",
    "        \n",
    "        '''\n",
    "        CNN feature extractor\n",
    "        '''\n",
    "        kwargs = {'out_channels': hidden_dim,'kernel_size': 5, 'padding':2 }\n",
    "        cnn_layers = [torch.nn.Conv2d(1,**kwargs)]\n",
    "        for i in range(num_conv_layers-1):\n",
    "            cnn_layers += [torch.nn.ReLU(), torch.nn.Conv2d(hidden_dim,**kwargs)] \n",
    "        cnn_layers.append(torch.nn.ReLU())\n",
    "          \n",
    "        self.CNN_encoder = torch.nn.Sequential(*cnn_layers) # 3 CNN layers by default\n",
    "        if which_encoder=='ResNet':\n",
    "            self.CNN_encoder = Encoder_resnet_1()\n",
    "            \n",
    "\n",
    "            \n",
    "          \n",
    "            \n",
    "        # Grid + query init\n",
    "        self.abs_grid = self.build_grid()\n",
    "                   \n",
    "        self.dense = torch.nn.Linear(2, query_dim) \n",
    "        self.pixel_mult = pixel_mult # LH's proposal... but almost same as 1/delta in ISA\n",
    "\n",
    "        # Apply after the data normalization\n",
    "        self.init_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(query_dim,query_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(query_dim,query_dim)\n",
    "        )\n",
    "            \n",
    "        self.slots_mu = torch.nn.Parameter(torch.randn(1, 1, self.query_dim))\n",
    "        self.slots_logsigma = torch.nn.Parameter(torch.zeros(1, 1, self.query_dim))\n",
    "        init.xavier_uniform_(self.slots_logsigma)\n",
    "\n",
    "        self.init_slots = self.init_slots\n",
    "\n",
    "        \n",
    "        '''\n",
    "        Option to add a final (x,y,r) prediction to each slot\n",
    "        '''\n",
    "        self.learn_slot_feat = learn_slot_feat\n",
    "        if self.learn_slot_feat:\n",
    "            self.final_mlp = torch.nn.Sequential(\n",
    "                torch.nn.Linear(query_dim,hidden_dim),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(hidden_dim, 3)\n",
    "            )\n",
    "        \n",
    "    def build_grid(self):\n",
    "        '''\n",
    "        From google slot attention repo:\n",
    "        https://github.com/nhartman94/google-research/blob/master/slot_attention/model.py#L357C1-L364C53\n",
    "        '''\n",
    "        resolution = self.resolution\n",
    "        xlow, xhigh = self.xlow, self.xhigh\n",
    "           \n",
    "        ranges = [np.linspace(xlow, xhigh, num=res) for res in resolution]\n",
    "        grid = np.meshgrid(*ranges, sparse=False, indexing=\"xy\")\n",
    "        grid = np.stack(grid, axis=-1)\n",
    "        grid = np.reshape(grid, [resolution[0], resolution[1], -1])\n",
    "        grid = np.expand_dims(grid, axis=0)\n",
    "        \n",
    "        grid = torch.FloatTensor( grid ).to(self.device)\n",
    "        grid = torch.flatten(grid,1,2)\n",
    "    \n",
    "        return grid\n",
    "                \n",
    "    def init_slots(self,Nbatch):\n",
    "        '''\n",
    "        Slot init taken from\n",
    "        https://github.com/lucidrains/slot-attention/blob/master/slot_attention/slot_attention.py\n",
    "        '''\n",
    "        \n",
    "        stdhigh, stdlow = self.rlow, self.rhigh\n",
    "        \n",
    "        mu = self.slots_mu.expand(Nbatch, self.k_slots, -1)\n",
    "        sigma = self.slots_logsigma.exp().expand(Nbatch, self.k_slots, -1)\n",
    "    \n",
    "        queries = mu + sigma * torch.randn(mu.shape,device=self.device)\n",
    "    \n",
    "        # Add the position and scale initialization for the local ref frame\n",
    "        ref_frame_dim = 3\n",
    "        pos_scale = torch.rand(Nbatch, self.k_slots, ref_frame_dim,device=self.device)\n",
    "\n",
    "        pos_scale[:,:2] -= 0.5\n",
    "        pos_scale[:,-1]  = (stdhigh - stdlow) * pos_scale[:,-1] + stdlow\n",
    "        \n",
    "        return queries, pos_scale\n",
    "     \n",
    "    def get_keys_vals(self, encoded_data, pos_scale):\n",
    "\n",
    "        # Get the relative position embedding\n",
    "        rel_grid = self.abs_grid.unsqueeze(1) - pos_scale[:,:,:2].unsqueeze(2)\n",
    "        rel_grid /= pos_scale[:,:,-1].unsqueeze(2).unsqueeze(-1)\n",
    "        \n",
    "        # Embed it in the same space as the query dimension \n",
    "        embed_grid = self.pixel_mult * self.dense( rel_grid )\n",
    "        \n",
    "        # keys, vals: (bs, img_dim, query_dim)\n",
    "        keys = self.toK(encoded_data).unsqueeze(1) + embed_grid\n",
    "        vals = self.toV(encoded_data).unsqueeze(1) + embed_grid\n",
    "        \n",
    "        keys = self.init_mlp(self.queryN(keys))\n",
    "        vals = self.init_mlp(self.queryN(vals))\n",
    "        \n",
    "        return keys, vals\n",
    "                \n",
    "    def attention_and_weights(self,queries,keys):\n",
    "        \n",
    "        logits = torch.einsum('bse,bsde->bsd',queries,keys) * self.softmax_T\n",
    "        \n",
    "        att = torch.nn.functional.softmax(logits, dim = 1)\n",
    "        \n",
    "        div = torch.sum(att, dim = -1, keepdims = True)\n",
    "        wts = att/div + 1e-8\n",
    "        return att,wts\n",
    "\n",
    "    def update_frames(self,wts):\n",
    "        '''\n",
    "        Update the relative frame position\n",
    "        '''\n",
    "        \n",
    "        # expand to include the batch dim\n",
    "        grid_exp = self.abs_grid.expand(wts.shape[0],-1,2)\n",
    "        \n",
    "        new_pos = torch.einsum('bsd,bde->bse',wts,grid_exp)\n",
    "        \n",
    "        new_scale = torch.sum(torch.pow(grid_exp.unsqueeze(1) - new_pos.unsqueeze(2),2),dim=-1)\n",
    "        \n",
    "        new_scale = torch.einsum('bsd,bsd->bs', wts, new_scale)\n",
    "        new_scale = torch.sqrt(new_scale)\n",
    "        \n",
    "        return torch.cat([new_pos,new_scale.unsqueeze(-1)],axis=-1)\n",
    "        \n",
    "    def iterate(self, queries, pos_scale, encoded_data):\n",
    "        \n",
    "        # Get the keys and values in the ref ref frame\n",
    "        keys, vals = self.get_keys_vals(encoded_data,pos_scale)\n",
    "        \n",
    "        # att,wts: (bs, k_slots, img_dim)\n",
    "        att,wts = self.attention_and_weights(self.queryN(queries),keys)   \n",
    "        \n",
    "        new_pos_scale = self.update_frames(wts)\n",
    "        \n",
    "        # Update the queries with the recurrent block\n",
    "        updates = torch.einsum('bsd,bsde->bse',wts,vals) # bs, n_slots, query_dim\n",
    "        \n",
    "        updates = self.gru(\n",
    "            updates.reshape(-1,self.query_dim),\n",
    "            queries.reshape(-1,self.query_dim),\n",
    "        )\n",
    "        \n",
    "        return updates.reshape(queries.shape), new_pos_scale\n",
    "        \n",
    "    def forward(self, data):\n",
    "    \n",
    "        '''\n",
    "        Step 1: Extract the CNN features\n",
    "        '''\n",
    "        #print('data shape', data.shape)\n",
    "        encoded_data = self.CNN_encoder(data) # Apply the CNN encoder\n",
    "    \n",
    "        #print(encoded_data.shape) \n",
    "        # for ModifiedResNet18: torch.Size([32, 16, 1, 1]) but should be torch.Size([32, 16, 32, 32]) with [bs, kernel, picture] - at least no error anymore!\n",
    "        # CustomResNet retruns indead [32,1,32, 32]\n",
    "        encoded_data = torch.permute(encoded_data,(0,2,3,1)) # Put channel dim at the end\n",
    "        encoded_data = torch.flatten(encoded_data,1,2) # flatten pixel dims\n",
    "        #print(encoded_data.shape)\n",
    "        encoded_data = self.dataN(encoded_data)\n",
    "        \n",
    "        '''\n",
    "        Step 2: Initialize the slots\n",
    "        '''\n",
    "        Nbatch = data.shape[0]\n",
    "        queries, pos_scale = self.init_slots(Nbatch) # Shape (Nbatch, k_slots, query_dim)\n",
    "                \n",
    "        '''\n",
    "        Step 3: Iterate through the reconstruction\n",
    "        '''\n",
    "        for i in range(self.n_iter):\n",
    "            queries, pos_scale = self.iterate(queries, pos_scale, encoded_data)    \n",
    "            \n",
    "        # With the final query vector, calc the attn, weights, + rel ref frames\n",
    "        keys, vals = self.get_keys_vals(encoded_data,pos_scale)\n",
    "        att, wts = self.attention_and_weights(self.queryN(queries),keys)   \n",
    "        new_pos_scale = self.update_frames(wts)\n",
    "                \n",
    "        if self.learn_slot_feat:\n",
    "            slot_feat = self.final_mlp(queries)\n",
    "            \n",
    "            # Want to learn the delta from the previously estimated position\n",
    "            slot_feat += new_pos_scale\n",
    "            \n",
    "            return queries, att, slot_feat \n",
    "        \n",
    "        else:\n",
    "            return queries, att, wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "930f9f2e-3c79-4395-b841-a184bd9dcb15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hungarian_matching(pairwise_cost):\n",
    "    '''\n",
    "    Input:\n",
    "    - pairwise_cost\n",
    "\n",
    "\n",
    "    Hungarian section Translated from the TensorFlow loss function (from 2006.15055 code):\n",
    "    https://github.com/nhartman94/google-research/blob/master/slot_attention/utils.py#L26-L57\n",
    "    '''\n",
    "    \n",
    "    indices = list(map(linear_sum_assignment, pairwise_cost.cpu()))\n",
    "    indices = torch.LongTensor(np.array(indices))\n",
    "    \n",
    "    loss = 0\n",
    "    for pi,(ri,ci) in zip(pairwise_cost,indices):\n",
    "        loss += pi[ri,ci].sum()\n",
    "    \n",
    "    return indices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "687fc01c-514c-4eec-8fa8-64053c044178",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _convert_into_pytorch_tensors(event_images, object_images, n_objects, object_features, device):\n",
    "    return torch.FloatTensor(event_images).to(device), \\\n",
    "               torch.FloatTensor(object_images).to(device), \\\n",
    "               torch.FloatTensor(n_objects).to(device), \\\n",
    "               torch.FloatTensor(object_features).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd3a54ee-d779-4655-a682-5531ba165957",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = InvariantSlotAttention_ResNet(**hps).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a28c144-ef23-45e4-9aa2-0ce29b924f08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Ntrain = 5000 \n",
    "bs=32 \n",
    "lr=3e-4\n",
    "warmup_steps=5_000\n",
    "alpha=1\n",
    "losses = {'tot':[],'bce':[],'mse':[]}\n",
    "kwargs={'isRing': True, 'N_clusters':2}\n",
    "clip_val = 1\n",
    "device='cpu'\n",
    "plot_every=20 \n",
    "save_every=1000\n",
    "color='C0'\n",
    "cmap='Blues'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d6944d5-3f7f-46f9-bc97-1e8fc77b4590",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Learning rate schedule config\n",
    "base_learning_rate = lr\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), base_learning_rate)\n",
    "model.train()\n",
    "\n",
    "k_slots = model.k_slots\n",
    "max_n_rings = kwargs['N_clusters']\n",
    "resolution = model.resolution\n",
    "kwargs['device'] = device\n",
    "N_obj = kwargs['N_clusters'] # pass to makeRing fct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1db4ad3-4a07-4b0a-8b79-cbafda158442",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:  torch.Size([32, 1, 32, 32])\n",
      "input shape:  torch.Size([32, 1, 32, 32])\n",
      "input shape:  torch.Size([32, 1, 32, 32])\n",
      "input shape:  torch.Size([32, 1, 32, 32])\n",
      "input shape:  torch.Size([32, 1, 32, 32])\n",
      "input shape:  torch.Size([32, 1, 32, 32])\n",
      "input shape:  torch.Size([32, 1, 32, 32])\n",
      "input shape:  torch.Size([32, 1, 32, 32])\n",
      "input shape:  torch.Size([32, 1, 32, 32])\n",
      "input shape:  torch.Size([32, 1, 32, 32])\n",
      "input shape:  torch.Size([32, 1, 32, 32])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Calculate the loss\u001b[39;00m\n\u001b[1;32m     47\u001b[0m li \u001b[38;5;241m=\u001b[39m l_bce \u001b[38;5;241m+\u001b[39m alpha\u001b[38;5;241m*\u001b[39ml_mse\n\u001b[0;32m---> 49\u001b[0m \u001b[43mli\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m clip_val\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     51\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), clip_val)\n",
      "File \u001b[0;32m/raven/u/saumi/nn-venv/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/raven/u/saumi/nn-venv/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "start = len(losses)\n",
    "for i in range(start,start+Ntrain):\n",
    "\n",
    "    learning_rate = base_learning_rate * 0.5 * (1 + np.cos(np.pi * i / Ntrain))\n",
    "    if i < warmup_steps:\n",
    "        learning_rate *= (i / warmup_steps)\n",
    "    \n",
    "    opt.param_groups[0]['lr'] = learning_rate\n",
    "    \n",
    "    # make scclevr data\n",
    "    rings = scclevr.RingsBinaryUniform(N_obj) # two rings per imagne\n",
    "    event_images, object_images, n_objects, object_features =  rings.gen_events(bs)\n",
    "    event_images, object_images, n_objects, object_features =  _convert_into_pytorch_tensors(event_images, object_images, n_objects, object_features, device)\n",
    "    # assign shorter names\n",
    "    X = event_images\n",
    "    mask = object_images\n",
    "    Y = object_features\n",
    "    \n",
    "    queries, att, Y_pred = model(X)\n",
    "        \n",
    "    # Reshape the target mask to be flat in the pixels (same shape as att)\n",
    "    flat_mask = mask.reshape(-1,max_n_rings, np.prod(resolution))   \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        att_ext  = torch.tile(att.unsqueeze(2), dims=(1,1,max_n_rings,1)) \n",
    "        mask_ext = torch.tile(flat_mask.unsqueeze(1),dims=(1,k_slots,1,1)) \n",
    "        \n",
    "        pairwise_cost = F.binary_cross_entropy(att_ext,mask_ext,reduction='none').mean(axis=-1)\n",
    "        \n",
    "        # pairwise_cost = comb_loss(att,flat_mask,Y,Y_pred,alpha)\n",
    "        indices = hungarian_matching(pairwise_cost)\n",
    "\n",
    "    # Apply the sorting to the predict\n",
    "    bis=torch.arange(bs).to(device)\n",
    "    indices=indices.to(device)\n",
    "    # Loss calc\n",
    "    slots_sorted = torch.cat([att[bis,indices[:,0,ri]].unsqueeze(1) for ri in range(max_n_rings)],dim=1)\n",
    "    rings_sorted = torch.cat([flat_mask[bis,indices[:,1,ri]].unsqueeze(1) for ri in range(max_n_rings)],dim=1)\n",
    "    l_bce = F.binary_cross_entropy(slots_sorted,rings_sorted,reduction='none').sum(axis=1).mean()\n",
    "    \n",
    "    Y_pred_sorted = torch.cat([Y_pred[bis,indices[:,0,ri]].unsqueeze(1) for ri in range(max_n_rings)],dim=1)\n",
    "    Y_true_sorted = torch.cat([Y[bis,indices[:,1,ri]].unsqueeze(1) for ri in range(max_n_rings)],dim=1)\n",
    "\n",
    "    l_mse = torch.nn.MSELoss(reduction='none')(Y_pred_sorted,Y_true_sorted).sum(axis=1).mean()\n",
    "\n",
    "    # Calculate the loss\n",
    "    li = l_bce + alpha*l_mse\n",
    "    \n",
    "    li.backward()\n",
    "    clip_val=1\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip_val)\n",
    "    \n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "\n",
    "    losses['tot'].append(float(li))\n",
    "    losses['bce'].append(float(l_bce))\n",
    "    losses['mse'].append(float(l_mse))\n",
    "    \n",
    "    if i % plot_every == 0:\n",
    "        print('iter',i,', loss',li.detach().cpu().numpy(),', lr',opt.param_groups[0]['lr'])  \n",
    "        iEvt = 0\n",
    "\n",
    "        # losses, mask, att_img, Y_true, Y_pred\n",
    "        plot_chosen_slots(losses,\n",
    "                            mask[iEvt].sum(axis=0), \n",
    "                            slots_sorted[iEvt].reshape(max_n_rings,*resolution),\n",
    "                            Y_true_sorted[iEvt],\n",
    "                            Y_pred_sorted[iEvt])\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1234f30c-c786-4bbf-85c0-6ce20e1782d9",
   "metadata": {},
   "source": [
    "Ok. It runs! Now try to adjust with ResNet!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f1222fa-3c48-4daf-8fae-5b5ca85d6fa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class mish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.tanh(torch.nn.functional.softplus(x))\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, activation, pre_act):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        if activation == 'mish':\n",
    "            activation = mish()\n",
    "        self.activation = activation\n",
    "        self.pre_act = pre_act\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv_res = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, inputs, inputs_scaled):\n",
    "        x = inputs\n",
    "        y = self.conv1(x)\n",
    "        y = self.bn1(y)\n",
    "        y = self.activation(y)\n",
    "        y = self.conv2(y)\n",
    "        y = self.bn2(y)\n",
    "        if self.pre_act:\n",
    "            y = self.activation(y)\n",
    "            \n",
    "        x = self.conv_res(x)\n",
    "        x = x + y\n",
    "        if not self.pre_act:\n",
    "            x = self.activation(x)\n",
    "            \n",
    "        x = torch.cat((x, inputs_scaled), dim=1)\n",
    "        return x\n",
    "\n",
    "class Encoder_resnet_1(nn.Module):\n",
    "    def __init__(self, nPixels=32, latent_dim=128, nMaxClusters=2, activation=mish(), use_vae=False, pre_act=False, filters=[16,16,32]):\n",
    "        super(Encoder_resnet_1, self).__init__()\n",
    "        #self.initial_conv = nn.Conv2d(3, filters[0], kernel_size=3, stride=1, padding=1)\n",
    "        self.res_blocks = nn.ModuleList()\n",
    "        self.activation = activation\n",
    "        self.use_vae = use_vae\n",
    "        \n",
    "        self.res_block11 = ResidualBlock(1,16, activation, pre_act)\n",
    "        self.res_block12 = ResidualBlock(17,16, activation, pre_act)\n",
    "        self.res_block13 = ResidualBlock(17,16, activation, pre_act)\n",
    "        self.res_block14 = ResidualBlock(17,16, activation, pre_act)\n",
    "        self.res_block15 = ResidualBlock(17,16, activation, pre_act)\n",
    "\n",
    "        self.res_block21 = ResidualBlock(17,16, activation, pre_act)\n",
    "        self.res_block22 = ResidualBlock(17,16, activation, pre_act)\n",
    "        self.res_block23 = ResidualBlock(17,16, activation, pre_act)\n",
    "        self.res_block24 = ResidualBlock(17,16, activation, pre_act)\n",
    "        self.res_block25 = ResidualBlock(17,16, activation, pre_act)\n",
    "\n",
    "        self.res_block31 = ResidualBlock(17,32, activation, pre_act)\n",
    "        self.res_block32 = ResidualBlock(33,32, activation, pre_act)\n",
    "        self.res_block33 = ResidualBlock(33,32, activation, pre_act)\n",
    "        self.res_block34 = ResidualBlock(33,32, activation, pre_act)\n",
    "        self.res_block35 = ResidualBlock(33,32, activation, pre_act)\n",
    "        \n",
    "        self.pooling = nn.MaxPool2d(kernel_size=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense = nn.Linear((filters[-1]+1) * nPixels * nPixels // (2 ** len(filters))**2, 256, bias=False)\n",
    "       \n",
    "        self.bn_dense = nn.BatchNorm1d(256)\n",
    "        #self.sampling_layer = Sampling()\n",
    "\n",
    "        if use_vae:\n",
    "            self.z_mean_full = nn.Linear(256, nMaxClusters*latent_dim)\n",
    "            self.z_log_var_full = nn.Linear(256,nMaxClusters*latent_dim)\n",
    "        else:\n",
    "            self.z_full = nn.Linear(256, nMaxClusters*latent_dim)\n",
    "            \n",
    "            \n",
    "        # Sara's adjustment ideas\n",
    "        self.upsample = nn.ConvTranspose2d(33, 16, 8, stride=8, padding=0) # is this a smart thing to do?\n",
    "        self.lastdense = nn.Linear(256, 16*32*32)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        inputs_scaled = inputs\n",
    "        print(\"input shape: \", inputs.shape)\n",
    "        #inputs_scaled = F.interpolate(x, size=(nPixels, nPixels))\n",
    "\n",
    "        inputs_scaled = F.interpolate(inputs_scaled, size=(x.shape[2], x.shape[3]))\n",
    "        x = self.res_block11(x,inputs_scaled)\n",
    "        x = self.res_block12(x,inputs_scaled)\n",
    "        x = self.res_block13(x,inputs_scaled)\n",
    "        x = self.res_block14(x,inputs_scaled)\n",
    "        x = self.res_block15(x,inputs_scaled)  \n",
    "        x = self.pooling(x)\n",
    "\n",
    "        inputs_scaled = F.interpolate(inputs_scaled, size=(x.shape[2], x.shape[3]))\n",
    "        x = self.res_block21(x,inputs_scaled)\n",
    "        x = self.res_block22(x,inputs_scaled)\n",
    "        x = self.res_block23(x,inputs_scaled)\n",
    "        x = self.res_block24(x,inputs_scaled)\n",
    "        x = self.res_block25(x,inputs_scaled)\n",
    "        x = self.pooling(x)\n",
    "\n",
    "        inputs_scaled = F.interpolate(inputs_scaled, size=(x.shape[2], x.shape[3]))\n",
    "        x = self.res_block31(x,inputs_scaled)\n",
    "        x = self.res_block32(x,inputs_scaled)\n",
    "        x = self.res_block33(x,inputs_scaled)\n",
    "        x = self.res_block34(x,inputs_scaled)\n",
    "        x = self.res_block35(x,inputs_scaled)\n",
    "        x = self.pooling(x)\n",
    "        #x = self.upsample(x) # option 1... upsampling with CNN from [bs, 33, 4,4] to [bs, 16, 32, 32]\n",
    "        #return x\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        #print(x.shape)\n",
    "        x = self.dense(x)\n",
    "        \n",
    "        x = self.bn_dense(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x = self.lastdense(x)\n",
    "        \n",
    "        x = x.reshape([inputs.shape[0], 16, 32, 32]) # horribly hard coded I know\n",
    "        \n",
    "        return x\n",
    "        \"\"\"\n",
    "        if self.use_vae:\n",
    "            z_mean_full = self.z_mean_full(x)\n",
    "            z_log_var_full = self.z_log_var_full(x)\n",
    "            z_full = self.sampling_layer(z_mean_full, z_log_var_full)\n",
    "            return z_mean_full, z_log_var_full, z_full\n",
    "        else:\n",
    "            z_full = self.z_full(x)\n",
    "            return z_full, z_full, z_full\n",
    "        \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e54507a-2321-426f-aa65-2eefb0c9cc0c",
   "metadata": {},
   "source": [
    "Ok, let's back off a bit? Implement torch network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed60f43c-d047-4fff-ae8c-1256b70425d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686051d6-765e-4350-9baf-5adb0e71f253",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sarasvenv",
   "language": "python",
   "name": "sarasvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
