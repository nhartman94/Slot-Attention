{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecb30679-ca17-49f4-a374-5364ad3465b1",
   "metadata": {},
   "source": [
    "# Implement 3 input channels into ISA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f79fe1ca-2523-40c0-bd0d-aa5d658b034f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mlp\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import json, yaml, os\n",
    "os.sys.path.append('./../../code')\n",
    "\n",
    "from plotting import plot_kslots, plot_kslots_iters\n",
    "from data_scclevr import makeRings \n",
    "from model import InvariantSlotAttention\n",
    "\n",
    "from matplotlib.patches import Circle\n",
    "import json\n",
    "\n",
    "# Set numpy seed for test set sampling \n",
    "torch_seed = 24082023\n",
    "torch.manual_seed( torch_seed )\n",
    "\n",
    "import random\n",
    "random.seed(torch_seed)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from plotting import plot_chosen_slots, plot_kslots, plot_kslots_iters, plot_kslots_grads\n",
    "\n",
    "from torch.nn import init\n",
    "import scclevr\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09b75972-6129-4c1d-a21b-12a9963adaad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "cID_prev = 'isa-scclevr-EncStudy-myBigResNet'\n",
    "with open(f'./../../code/configs/encoder-studies/{cID_prev}.yaml') as f:\n",
    "    cd = yaml.safe_load(f)\n",
    "\n",
    "hps = cd['hps']\n",
    "hps['device'] = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6735826-3a4f-4b40-b928-36617953c4d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _convert_into_pytorch_tensors(event_images, object_images, n_objects, object_features, device):\n",
    "    return torch.FloatTensor(event_images).to(device), \\\n",
    "               torch.FloatTensor(object_images).to(device), \\\n",
    "               torch.FloatTensor(n_objects).to(device), \\\n",
    "               torch.FloatTensor(object_features).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26ca9bd1-a1d4-4120-b0ad-263e82785d13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hungarian_matching(pairwise_cost):\n",
    "    '''\n",
    "    Input:\n",
    "    - pairwise_cost\n",
    "\n",
    "\n",
    "    Hungarian section Translated from the TensorFlow loss function (from 2006.15055 code):\n",
    "    https://github.com/nhartman94/google-research/blob/master/slot_attention/utils.py#L26-L57\n",
    "    '''\n",
    "    \n",
    "    indices = list(map(linear_sum_assignment, pairwise_cost.cpu()))\n",
    "    indices = torch.LongTensor(np.array(indices))\n",
    "    \n",
    "    loss = 0\n",
    "    for pi,(ri,ci) in zip(pairwise_cost,indices):\n",
    "        loss += pi[ri,ci].sum()\n",
    "    \n",
    "    return indices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c30d594-43bf-4423-8829-29fc20e5b0a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"Basic Residual Block\"\"\"\n",
    "    def __init__(self, inplanes, outplanes, stride=1, kernel_size=3, padding=0):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, outplanes, kernel_size=kernel_size, stride=stride, padding=padding, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(outplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(outplanes, outplanes, kernel_size=kernel_size, stride=stride, padding=padding, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(outplanes)\n",
    "        \n",
    "        self.sampling = nn.Conv2d(inplanes, outplanes, kernel_size=1, stride=1, bias=False)\n",
    "        self.stride = stride\n",
    "        \n",
    "        #self.linear output: 16*32*32 -> reshape it afterwards into [bs, 16, 32, 32]\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        #print(\"identity: \", x.shape) # [bs, 8, 32, 32]\n",
    "        #print(\"out: \", out.shape)  # [bs, 16, 32, 32]\n",
    "        if(identity.shape!=out.shape):\n",
    "            identity = self.sampling(x) # is that ok? -> Nicole! ####################################################################################################\n",
    "            \n",
    "        out += identity # this is the trick!!\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "class BigResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(n_channels=1)\n",
    "        self.inplanes = 64\n",
    "        self.conv1 = nn.Conv2d(n_channels, 8,kernel_size=5, stride=1, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # I choose my conv layers in a way that the final output will be [32, 32] still\n",
    "        \n",
    "        # FIRST STAGE -  capture basic features and patterns in the input while reducing its spatial resolution (second not needed?)\n",
    "        self.layer11 = BasicBlock(8, 8,kernel_size=5, stride=1, padding=2)\n",
    "        self.layer12 = BasicBlock(8, 8,kernel_size=5, stride=1, padding=2)\n",
    "        self.layer13 = BasicBlock(8, 8,kernel_size=5, stride=1, padding=2)\n",
    "        self.layer14 = BasicBlock(8, 8,kernel_size=5, stride=1, padding=2)\n",
    "        # SECOND STAGE - capture more complex features and patterns compared to the initial stage. The spatial dimensions are reduced, but the number of channels (depth) is increased.\n",
    "        self.layer21 = BasicBlock(8, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.layer22 = BasicBlock(16, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.layer23 = BasicBlock(16, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.layer24 = BasicBlock(16, 16, kernel_size=3, stride=1, padding=1)\n",
    "        # THIRD STAGE -  capture more abstract features and high-level representations, as the spatial dimensions continue to decrease.\n",
    "        self.layer31 = BasicBlock(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.layer32 = BasicBlock(32, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.layer33 = BasicBlock(32, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.layer34 = BasicBlock(32, 32, kernel_size=3, stride=1, padding=1)\n",
    "        # FOUTH STAGE- - capture very abstract and global features, consolidating the information learned from the previous stages.\n",
    "        self.layer4 = BasicBlock(32, 16, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        #self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer11(x)\n",
    "        x = self.layer12(x)\n",
    "        x = self.layer13(x)\n",
    "        x = self.layer14(x)\n",
    "        x = self.layer21(x)\n",
    "        x = self.layer22(x)\n",
    "        x = self.layer23(x)\n",
    "        x = self.layer24(x)\n",
    "        x = self.layer31(x)\n",
    "        x = self.layer32(x)\n",
    "        x = self.layer33(x)\n",
    "        x = self.layer34(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        #x = self.avgpool(x)\n",
    "        #print(\"shape after resnet block: \", x.shape)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cb9452b9-9acc-4d85-911a-d032437429ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class InvariantSlotAttention_ResNet(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 resolution=(32,32),\n",
    "                 xlow=-0.5,\n",
    "                 xhigh=0.5,\n",
    "                 varlow=0.01,\n",
    "                 varhigh=0.05,\n",
    "                 k_slots=3, \n",
    "                 num_conv_layers=3,\n",
    "                 which_encoder='ResNet',\n",
    "                 n_input_channels=1,\n",
    "                 hidden_dim=32, \n",
    "                 query_dim=32, \n",
    "                 n_iter=2,\n",
    "                 pixel_mult=1,\n",
    "                 device='cpu' ,\n",
    "                 learn_slot_feat=True\n",
    "                 ):\n",
    "        '''\n",
    "        Slot attention encoder block, block attention\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.k_slots = k_slots\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.query_dim = query_dim\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "        self.resolution = resolution\n",
    "        self.xlow, self.xhigh = xlow, xhigh\n",
    "        self.rlow, self.rhigh = np.sqrt(varlow), np.sqrt(varhigh)\n",
    "        \n",
    "        self.device=device\n",
    "         \n",
    "        self.softmax_T = 1/np.sqrt(query_dim)\n",
    "        \n",
    "        self.dataN = torch.nn.LayerNorm(self.hidden_dim)\n",
    "        self.queryN = torch.nn.LayerNorm(self.query_dim)\n",
    "        \n",
    "        self.toK = torch.nn.Linear(self.hidden_dim, self.query_dim)\n",
    "        self.toV = torch.nn.Linear(self.hidden_dim, self.query_dim)\n",
    "        self.gru = torch.nn.GRUCell(self.query_dim, self.query_dim)\n",
    "        \n",
    "        '''\n",
    "        CNN feature extractor\n",
    "        '''\n",
    "        kwargs = {'out_channels': hidden_dim,'kernel_size': 5, 'padding':2 }\n",
    "        cnn_layers = [torch.nn.Conv2d(1,**kwargs)]\n",
    "        for i in range(num_conv_layers-1):\n",
    "            cnn_layers += [torch.nn.ReLU(), torch.nn.Conv2d(hidden_dim,**kwargs)] \n",
    "        cnn_layers.append(torch.nn.ReLU())\n",
    "          \n",
    "        self.CNN_encoder = torch.nn.Sequential(*cnn_layers) # 3 CNN layers by default\n",
    "        if which_encoder=='ResNet':\n",
    "            self.CNN_encoder = ResNet()\n",
    "        elif which_encoder=='BigResNet':\n",
    "            self.CNN_encoder = BigResNet(n_channels=n_input_channels)\n",
    "\n",
    "            \n",
    "          \n",
    "            \n",
    "        # Grid + query init\n",
    "        self.abs_grid = self.build_grid()\n",
    "                   \n",
    "        self.dense = torch.nn.Linear(2, query_dim) \n",
    "        self.pixel_mult = pixel_mult # LH's proposal... but almost same as 1/delta in ISA\n",
    "\n",
    "        # Apply after the data normalization\n",
    "        self.init_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(query_dim,query_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(query_dim,query_dim)\n",
    "        )\n",
    "            \n",
    "        self.slots_mu = torch.nn.Parameter(torch.randn(1, 1, self.query_dim))\n",
    "        self.slots_logsigma = torch.nn.Parameter(torch.zeros(1, 1, self.query_dim))\n",
    "        init.xavier_uniform_(self.slots_logsigma)\n",
    "\n",
    "        self.init_slots = self.init_slots\n",
    "\n",
    "        \n",
    "        '''\n",
    "        Option to add a final (x,y,r) prediction to each slot\n",
    "        '''\n",
    "        self.learn_slot_feat = learn_slot_feat\n",
    "        if self.learn_slot_feat:\n",
    "            self.final_mlp = torch.nn.Sequential(\n",
    "                torch.nn.Linear(query_dim,hidden_dim),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(hidden_dim, 3)\n",
    "            )\n",
    "        \n",
    "    def build_grid(self):\n",
    "        '''\n",
    "        From google slot attention repo:\n",
    "        https://github.com/nhartman94/google-research/blob/master/slot_attention/model.py#L357C1-L364C53\n",
    "        '''\n",
    "        resolution = self.resolution\n",
    "        xlow, xhigh = self.xlow, self.xhigh\n",
    "           \n",
    "        ranges = [np.linspace(xlow, xhigh, num=res) for res in resolution]\n",
    "        grid = np.meshgrid(*ranges, sparse=False, indexing=\"xy\")\n",
    "        grid = np.stack(grid, axis=-1)\n",
    "        grid = np.reshape(grid, [resolution[0], resolution[1], -1])\n",
    "        grid = np.expand_dims(grid, axis=0)\n",
    "        \n",
    "        grid = torch.FloatTensor( grid ).to(self.device)\n",
    "        grid = torch.flatten(grid,1,2)\n",
    "    \n",
    "        return grid\n",
    "                \n",
    "    def init_slots(self,Nbatch):\n",
    "        '''\n",
    "        Slot init taken from\n",
    "        https://github.com/lucidrains/slot-attention/blob/master/slot_attention/slot_attention.py\n",
    "        '''\n",
    "        \n",
    "        stdhigh, stdlow = self.rlow, self.rhigh\n",
    "        \n",
    "        mu = self.slots_mu.expand(Nbatch, self.k_slots, -1)\n",
    "        sigma = self.slots_logsigma.exp().expand(Nbatch, self.k_slots, -1)\n",
    "    \n",
    "        queries = mu + sigma * torch.randn(mu.shape,device=self.device)\n",
    "    \n",
    "        # Add the position and scale initialization for the local ref frame\n",
    "        ref_frame_dim = 3\n",
    "        pos_scale = torch.rand(Nbatch, self.k_slots, ref_frame_dim,device=self.device)\n",
    "\n",
    "        pos_scale[:,:2] -= 0.5\n",
    "        pos_scale[:,-1]  = (stdhigh - stdlow) * pos_scale[:,-1] + stdlow\n",
    "        \n",
    "        return queries, pos_scale\n",
    "     \n",
    "    def get_keys_vals(self, encoded_data, pos_scale):\n",
    "\n",
    "        # Get the relative position embedding\n",
    "        rel_grid = self.abs_grid.unsqueeze(1) - pos_scale[:,:,:2].unsqueeze(2)\n",
    "        rel_grid /= pos_scale[:,:,-1].unsqueeze(2).unsqueeze(-1)\n",
    "        \n",
    "        # Embed it in the same space as the query dimension \n",
    "        embed_grid = self.pixel_mult * self.dense( rel_grid )\n",
    "        \n",
    "        # keys, vals: (bs, img_dim, query_dim)\n",
    "        keys = self.toK(encoded_data).unsqueeze(1) + embed_grid\n",
    "        vals = self.toV(encoded_data).unsqueeze(1) + embed_grid\n",
    "        \n",
    "        keys = self.init_mlp(self.queryN(keys))\n",
    "        vals = self.init_mlp(self.queryN(vals))\n",
    "        \n",
    "        return keys, vals\n",
    "                \n",
    "    def attention_and_weights(self,queries,keys):\n",
    "        \n",
    "        logits = torch.einsum('bse,bsde->bsd',queries,keys) * self.softmax_T\n",
    "        \n",
    "        att = torch.nn.functional.softmax(logits, dim = 1)\n",
    "        \n",
    "        div = torch.sum(att, dim = -1, keepdims = True)\n",
    "        wts = att/div + 1e-8\n",
    "        return att,wts\n",
    "\n",
    "    def update_frames(self,wts):\n",
    "        '''\n",
    "        Update the relative frame position\n",
    "        '''\n",
    "        \n",
    "        # expand to include the batch dim\n",
    "        grid_exp = self.abs_grid.expand(wts.shape[0],-1,2)\n",
    "        \n",
    "        new_pos = torch.einsum('bsd,bde->bse',wts,grid_exp)\n",
    "        \n",
    "        new_scale = torch.sum(torch.pow(grid_exp.unsqueeze(1) - new_pos.unsqueeze(2),2),dim=-1)\n",
    "        \n",
    "        new_scale = torch.einsum('bsd,bsd->bs', wts, new_scale)\n",
    "        new_scale = torch.sqrt(new_scale)\n",
    "        \n",
    "        return torch.cat([new_pos,new_scale.unsqueeze(-1)],axis=-1)\n",
    "        \n",
    "    def iterate(self, queries, pos_scale, encoded_data):\n",
    "        \n",
    "        # Get the keys and values in the ref ref frame\n",
    "        keys, vals = self.get_keys_vals(encoded_data,pos_scale)\n",
    "        \n",
    "        # att,wts: (bs, k_slots, img_dim)\n",
    "        att,wts = self.attention_and_weights(self.queryN(queries),keys)   \n",
    "        \n",
    "        new_pos_scale = self.update_frames(wts)\n",
    "        \n",
    "        # Update the queries with the recurrent block\n",
    "        updates = torch.einsum('bsd,bsde->bse',wts,vals) # bs, n_slots, query_dim\n",
    "        \n",
    "        updates = self.gru(\n",
    "            updates.reshape(-1,self.query_dim),\n",
    "            queries.reshape(-1,self.query_dim),\n",
    "        )\n",
    "        \n",
    "        return updates.reshape(queries.shape), new_pos_scale\n",
    "        \n",
    "    def forward(self, data):\n",
    "    \n",
    "        '''\n",
    "        Step 1: Extract the CNN features\n",
    "        '''\n",
    "        #print('data shape', data.shape)\n",
    "        encoded_data = self.CNN_encoder(data) # Apply the CNN encoder\n",
    "        encoded_data = torch.permute(encoded_data,(0,2,3,1)) # Put channel dim at the end\n",
    "        encoded_data = torch.flatten(encoded_data,1,2) # flatten pixel dims\n",
    "        encoded_data = self.dataN(encoded_data)\n",
    "        \n",
    "        '''\n",
    "        Step 2: Initialize the slots\n",
    "        '''\n",
    "        Nbatch = data.shape[0] # batchsize\n",
    "        queries, pos_scale = self.init_slots(Nbatch) # Shape (Nbatch, k_slots, query_dim)\n",
    "                \n",
    "        '''\n",
    "        Step 3: Iterate through the reconstruction\n",
    "        '''\n",
    "        for i in range(self.n_iter):\n",
    "            queries, pos_scale = self.iterate(queries, pos_scale, encoded_data)    \n",
    "            \n",
    "        # With the final query vector, calc the attn, weights, + rel ref frames\n",
    "        keys, vals = self.get_keys_vals(encoded_data,pos_scale)\n",
    "        att, wts = self.attention_and_weights(self.queryN(queries),keys)   \n",
    "        new_pos_scale = self.update_frames(wts)\n",
    "                \n",
    "        if self.learn_slot_feat:\n",
    "            slot_feat = self.final_mlp(queries)\n",
    "            \n",
    "            # Want to learn the delta from the previously estimated position\n",
    "            slot_feat += new_pos_scale\n",
    "            \n",
    "            return queries, att, slot_feat \n",
    "        \n",
    "        else:\n",
    "            return queries, att, wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "10b82d8e-bf9a-4449-b143-2c822221ff22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = InvariantSlotAttention_ResNet(**hps).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c87ee31d-7069-46b5-b637-a0d48a127590",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Ntrain = 5000 \n",
    "bs=32 \n",
    "lr=3e-4\n",
    "warmup_steps=5_000\n",
    "alpha=1\n",
    "losses = {'tot':[],'bce':[],'mse':[]}\n",
    "kwargs={'isRing': True, 'N_clusters':2}\n",
    "clip_val = 1\n",
    "device='cpu'\n",
    "plot_every=20 \n",
    "save_every=1000\n",
    "color='C0'\n",
    "cmap='Blues'\n",
    "\n",
    "# Learning rate schedule config\n",
    "base_learning_rate = lr\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), base_learning_rate)\n",
    "model.train()\n",
    "\n",
    "k_slots = model.k_slots\n",
    "max_n_rings = kwargs['N_clusters']\n",
    "resolution = model.resolution\n",
    "kwargs['device'] = device\n",
    "N_obj = kwargs['N_clusters'] # pass to makeRing fct\n",
    "n_channels=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "acb331a7-caa0-42ab-be8b-7fd8a64a5b64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.5        -0.46774194 -0.43548387 ...  0.43548387  0.46774194\n",
      "    0.5       ]\n",
      "  [-0.5        -0.46774194 -0.43548387 ...  0.43548387  0.46774194\n",
      "    0.5       ]\n",
      "  [-0.5        -0.46774194 -0.43548387 ...  0.43548387  0.46774194\n",
      "    0.5       ]\n",
      "  ...\n",
      "  [-0.5        -0.46774194 -0.43548387 ...  0.43548387  0.46774194\n",
      "    0.5       ]\n",
      "  [-0.5        -0.46774194 -0.43548387 ...  0.43548387  0.46774194\n",
      "    0.5       ]\n",
      "  [-0.5        -0.46774194 -0.43548387 ...  0.43548387  0.46774194\n",
      "    0.5       ]]]\n",
      "(32, 3, 32, 32)\n",
      "[[[-0.5        -0.46774194 -0.43548387 ...  0.43548387  0.46774194\n",
      "    0.5       ]\n",
      "  [-0.5        -0.46774194 -0.43548387 ...  0.43548387  0.46774194\n",
      "    0.5       ]\n",
      "  [-0.5        -0.46774194 -0.43548387 ...  0.43548387  0.46774194\n",
      "    0.5       ]\n",
      "  ...\n",
      "  [-0.5        -0.46774194 -0.43548387 ...  0.43548387  0.46774194\n",
      "    0.5       ]\n",
      "  [-0.5        -0.46774194 -0.43548387 ...  0.43548387  0.46774194\n",
      "    0.5       ]\n",
      "  [-0.5        -0.46774194 -0.43548387 ...  0.43548387  0.46774194\n",
      "    0.5       ]]]\n",
      "(32, 3, 32, 32)\n",
      "[[[-0.5        -0.46774194 -0.43548387 ...  0.43548387  0.46774194\n",
      "    0.5       ]\n",
      "  [-0.5        -0.46774194 -0.43548387 ...  0.43548387  0.46774194\n",
      "    0.5       ]\n",
      "  [-0.5        -0.46774194 -0.43548387 ...  0.43548387  0.46774194\n",
      "    0.5       ]\n",
      "  ...\n",
      "  [-0.5        -0.46774194 -0.43548387 ...  0.43548387  0.46774194\n",
      "    0.5       ]\n",
      "  [-0.5        -0.46774194 -0.43548387 ...  0.43548387  0.46774194\n",
      "    0.5       ]\n",
      "  [-0.5        -0.46774194 -0.43548387 ...  0.43548387  0.46774194\n",
      "    0.5       ]]]\n",
      "(32, 3, 32, 32)\n",
      "[[[-0.5        -0.46774194 -0.43548387 ...  0.43548387  0.46774194\n",
      "    0.5       ]\n",
      "  [-0.5        -0.46774194 -0.43548387 ...  0.43548387  0.46774194\n",
      "    0.5       ]\n",
      "  [-0.5        -0.46774194 -0.43548387 ...  0.43548387  0.46774194\n",
      "    0.5       ]\n",
      "  ...\n",
      "  [-0.5        -0.46774194 -0.43548387 ...  0.43548387  0.46774194\n",
      "    0.5       ]\n",
      "  [-0.5        -0.46774194 -0.43548387 ...  0.43548387  0.46774194\n",
      "    0.5       ]\n",
      "  [-0.5        -0.46774194 -0.43548387 ...  0.43548387  0.46774194\n",
      "    0.5       ]]]\n",
      "(32, 3, 32, 32)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 62\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Calculate the loss\u001b[39;00m\n\u001b[1;32m     60\u001b[0m li \u001b[38;5;241m=\u001b[39m l_bce \u001b[38;5;241m+\u001b[39m alpha\u001b[38;5;241m*\u001b[39ml_mse\n\u001b[0;32m---> 62\u001b[0m \u001b[43mli\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m clip_val\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     64\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), clip_val)\n",
      "File \u001b[0;32m/raven/u/saumi/nn-venv/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/raven/u/saumi/nn-venv/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "start = len(losses)\n",
    "for i in range(start,start+Ntrain):\n",
    "\n",
    "    learning_rate = base_learning_rate * 0.5 * (1 + np.cos(np.pi * i / Ntrain))\n",
    "    if i < warmup_steps:\n",
    "        learning_rate *= (i / warmup_steps)\n",
    "    \n",
    "    opt.param_groups[0]['lr'] = learning_rate\n",
    "    \n",
    "    # make scclevr data\n",
    "    rings = scclevr.RingsBinaryUniform(N_obj) # two rings per imagne\n",
    "    event_images, object_images, n_objects, object_features =  rings.gen_events(bs)\n",
    "    event_images, object_images, n_objects, object_features =  _convert_into_pytorch_tensors(event_images, object_images, n_objects, object_features, device)\n",
    "    # assign shorter names\n",
    "    X = event_images\n",
    "    mask = object_images\n",
    "    Y = object_features\n",
    "    if n_channels==3:\n",
    "        xhigh=0.5\n",
    "        xlow=-0.5\n",
    "        ranges = [np.linspace(xlow, xhigh, num=res) for res in resolution]\n",
    "        Xb, Yb = np.meshgrid(*ranges, sparse=False, indexing=\"xy\")\n",
    "        Xbtmp = Xb[np.newaxis, np.newaxis, :, :]\n",
    "        Ybtmp = Yb[np.newaxis, np.newaxis, :, :]\n",
    "        Xbtmp = np.repeat(Xbtmp,len(event_images),axis=0)\n",
    "        Ybtmp= np.repeat(Ybtmp,len(event_images),axis=0)\n",
    "\n",
    "        event_images = np.concatenate([event_images,Xbtmp,Ybtmp], axis=1)\n",
    "        print(Xbtmp[0])\n",
    "        print(event_images.shape)\n",
    "    \n",
    "    queries, att, Y_pred = model(X)\n",
    "        \n",
    "    # Reshape the target mask to be flat in the pixels (same shape as att)\n",
    "    flat_mask = mask.reshape(-1,max_n_rings, np.prod(resolution))   \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        att_ext  = torch.tile(att.unsqueeze(2), dims=(1,1,max_n_rings,1)) \n",
    "        mask_ext = torch.tile(flat_mask.unsqueeze(1),dims=(1,k_slots,1,1)) \n",
    "        \n",
    "        pairwise_cost = F.binary_cross_entropy(att_ext,mask_ext,reduction='none').mean(axis=-1)\n",
    "        \n",
    "        # pairwise_cost = comb_loss(att,flat_mask,Y,Y_pred,alpha)\n",
    "        indices = hungarian_matching(pairwise_cost)\n",
    "\n",
    "    # Apply the sorting to the predict\n",
    "    bis=torch.arange(bs).to(device)\n",
    "    indices=indices.to(device)\n",
    "    # Loss calc\n",
    "    slots_sorted = torch.cat([att[bis,indices[:,0,ri]].unsqueeze(1) for ri in range(max_n_rings)],dim=1)\n",
    "    rings_sorted = torch.cat([flat_mask[bis,indices[:,1,ri]].unsqueeze(1) for ri in range(max_n_rings)],dim=1)\n",
    "    l_bce = F.binary_cross_entropy(slots_sorted,rings_sorted,reduction='none').sum(axis=1).mean()\n",
    "    \n",
    "    Y_pred_sorted = torch.cat([Y_pred[bis,indices[:,0,ri]].unsqueeze(1) for ri in range(max_n_rings)],dim=1)\n",
    "    Y_true_sorted = torch.cat([Y[bis,indices[:,1,ri]].unsqueeze(1) for ri in range(max_n_rings)],dim=1)\n",
    "\n",
    "    l_mse = torch.nn.MSELoss(reduction='none')(Y_pred_sorted,Y_true_sorted).sum(axis=1).mean()\n",
    "\n",
    "    # Calculate the loss\n",
    "    li = l_bce + alpha*l_mse\n",
    "    \n",
    "    li.backward()\n",
    "    clip_val=1\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip_val)\n",
    "    \n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "\n",
    "    losses['tot'].append(float(li))\n",
    "    losses['bce'].append(float(l_bce))\n",
    "    losses['mse'].append(float(l_mse))\n",
    "    \n",
    "    if i % plot_every == 0:\n",
    "        print('iter',i,', loss',li.detach().cpu().numpy(),', lr',opt.param_groups[0]['lr'])  \n",
    "        iEvt = 0\n",
    "\n",
    "        # losses, mask, att_img, Y_true, Y_pred\n",
    "        plot_chosen_slots(losses,\n",
    "                            mask[iEvt].sum(axis=0), \n",
    "                            slots_sorted[iEvt].reshape(max_n_rings,*resolution),\n",
    "                            Y_true_sorted[iEvt],\n",
    "                            Y_pred_sorted[iEvt])\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f515185-5ed2-4664-8d86-bcfd25b65c39",
   "metadata": {},
   "source": [
    "# check out how to make grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "101fb6f9-5e4a-4d5e-98b4-db281991935f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_grid(resolution, xlow, xhigh):\n",
    "    '''\n",
    "    From google slot attention repo:\n",
    "    https://github.com/nhartman94/google-research/blob/master/slot_attention/model.py#L357C1-L364C53\n",
    "    '''\n",
    "\n",
    "    ranges = [np.linspace(xlow, xhigh, num=res) for res in resolution]\n",
    "    grid = np.meshgrid(*ranges, sparse=False, indexing=\"xy\")\n",
    "    print(np.array(grid).shape)\n",
    "    grid = np.stack(grid, axis=-1)\n",
    "    grid = np.reshape(grid, [resolution[0], resolution[1], -1])\n",
    "    grid = np.expand_dims(grid, axis=0)\n",
    "    \n",
    "\n",
    "    grid = torch.FloatTensor( grid ).to(device)\n",
    "    grid = torch.flatten(grid,1,2)\n",
    "\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b54335ed-94cd-4c7f-b55e-6085a1af22c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "grid = build_grid((32, 32), 0.5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a3ea8d57-9ed3-4d08-962a-b80427e194fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 2])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5783c338-8709-46c2-978a-632d0ce3dfc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sarasvenv",
   "language": "python",
   "name": "sarasvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
