{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62f66fc0",
   "metadata": {},
   "source": [
    "# Hungarian Multi Rings\n",
    "\n",
    "**Goal:** In the past notebooks\n",
    "- `SA-mini`\n",
    "- `SA-warm-up`\n",
    "\n",
    "I've been focusing on a single ring to stay in a good range for the hyperparameters.\n",
    "\n",
    "But _now_ I still need to diagnose how to translate the (official) TensorFlow implementation of the Hungarian Loss to Pytorch.\n",
    "\n",
    "Fortunately, the (concurrent work) DETR (End-to-End Object **DE**tection with **TR**ansformers) has an [implementation](https://www.kaggle.com/code/virajbagal/pytorch-starter-detection-transformer-train) of the Hungarian loss in Pytorch, using the same `scipy.optimize.linear_map_` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc4e47db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# import os\n",
    "# os.sys.path.append('code')\n",
    "# from data import gen_events # Florian's ring generation script\n",
    "\n",
    "'''\n",
    "Remember -- Flo's rings generation script already adds up \n",
    "all of the ring energies, and I think having this at truth level \n",
    "would be quite nice.\n",
    "'''\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0da8f4e",
   "metadata": {},
   "source": [
    "**Inputs:**\n",
    "\n",
    "I want a minimal example to _show_ that this is working so I'll make the masks and have the prediction:\n",
    "\n",
    "1. perturb the images by some noise\n",
    "2. Have the background slots just be 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98764422",
   "metadata": {},
   "outputs": [],
   "source": [
    "nPixels = 32\n",
    "\n",
    "xlow = -0.5\n",
    "xhigh = 0.5\n",
    "binWidth = (xhigh-xlow)/(nPixels+1)\n",
    "\n",
    "stdlow = 0.01\n",
    "stdhigh = 0.05\n",
    "\n",
    "bins = np.arange(xlow,xhigh,binWidth)\n",
    "\n",
    "nBins = len(bins)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2d61c9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(\n",
    "    bins,\n",
    "    N_events = 100000,\n",
    "    N_clusters = 1,\n",
    "    mean_var_cluster = np.log(0.001),\n",
    "    sigma_var_cluster = 0.1,\n",
    "    mean_lam_cluster = np.log(200.),\n",
    "    sigma_lam_cluster = 1.,\n",
    "    isRing=True,\n",
    "    blurImage=False,\n",
    "    xlow=-.5,\n",
    "    xhigh=0.5,\n",
    "    stdlow=0.01,\n",
    "    stdhigh=0.05,\n",
    "    device='cpu'\n",
    "):\n",
    "    '''\n",
    "    Starts from Florian's `gen_events` function\n",
    "    https://gitlab.lrz.de/neural_network_clustering/permutation_invariant_loss/-/blob/main/test_blur.ipynb\n",
    "    \n",
    "    But I wanted to return the occupancy maps that you multiply the OG image by to output partitioning\n",
    "    of the image into the OG ring signals.\n",
    "    '''\n",
    "\n",
    "    eventHistograms = np.zeros(shape=(N_events, len(bins)-1, len(bins)-1,1) ) \n",
    "    \n",
    "    eventMasks = np.zeros(shape=(N_events, N_clusters, len(bins)-1, len(bins)-1) ) \n",
    "    \n",
    "    eventNumbers = np.zeros(N_events)\n",
    "\n",
    "    nMaxClusters = N_clusters   \n",
    "    \n",
    "    eventInfo = np.zeros(shape=(N_events, nMaxClusters, 3))\n",
    "    \n",
    "    for iEvent in range(N_events):\n",
    "        \n",
    "        image = np.zeros_like(eventHistograms[iEvent,:,:,0])\n",
    "        \n",
    "        n_clusters = nMaxClusters\n",
    "\n",
    "        eventNumbers[iEvent] += n_clusters\n",
    "        eI = []\n",
    "        \n",
    "        for iCluster in range(min(n_clusters,nMaxClusters)):\n",
    "            \n",
    "            # how many events in this cluster\n",
    "            lam_N_events_in_cluster = 200. \n",
    "            \n",
    "            N_events_in_cluster = np.random.poisson(lam_N_events_in_cluster)\n",
    "\n",
    "            # where is the cluster center\n",
    "            cluster_center = np.random.uniform(low=xlow, high=xhigh, size=2)\n",
    "\n",
    "            # what is the cluster spread\n",
    "            var_cluster = np.random.uniform(stdlow,stdhigh) \n",
    "            \n",
    "            cluster_events_x0 = np.random.normal(loc=0., scale=1., size=N_events_in_cluster)\n",
    "            cluster_events_y0 = np.random.normal(loc=0., scale=1., size=N_events_in_cluster)\n",
    "\n",
    "            if isRing:\n",
    "                fact = np.sqrt(var_cluster/(cluster_events_x0**2+cluster_events_y0**2))\n",
    "            else:\n",
    "                fact = np.sqrt(var_cluster)\n",
    "\n",
    "            cluster_events_x = cluster_events_x0*fact + cluster_center[0]\n",
    "            cluster_events_y = cluster_events_y0*fact + cluster_center[1]\n",
    "            \n",
    "            # bin the events\n",
    "            H, _, _ = np.histogram2d(cluster_events_x, cluster_events_y, bins=[bins,bins])\n",
    "\n",
    "            eventMasks[iEvent,iCluster] = H.T\n",
    "            image += H.T\n",
    "                        \n",
    "            eI.append(np.concatenate([cluster_center, [var_cluster]]))\n",
    "\n",
    "        eventHistograms[iEvent,:,:,0] = np.copy(image)\n",
    "\n",
    "        eventInfo[iEvent] = np.array(eI)\n",
    "    \n",
    "    # reshape eventHistograms\n",
    "    eventHistograms = eventHistograms[:,None,:,:,0] # shape (bs, 1, nPixels, nPixels)\n",
    "    \n",
    "    eventMasks = np.where(eventHistograms>0,eventMasks/eventHistograms,eventMasks)\n",
    "    \n",
    "    return torch.FloatTensor(eventInfo).to(device), \\\n",
    "           torch.FloatTensor(eventHistograms).to(device), \\\n",
    "           torch.FloatTensor(eventMasks).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33cd3b8",
   "metadata": {},
   "source": [
    "Let's consider:\n",
    "- batch_size 4\n",
    "- 2 rings\n",
    "- 3 slots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "6e00d80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_n_rings = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "198ef6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5j/ynfw2g6n2cgdwgp6xdh3byn00000gp/T/ipykernel_70926/637650445.py:83: RuntimeWarning: invalid value encountered in divide\n",
      "  eventMasks = np.where(eventHistograms>0,eventMasks/eventHistograms,eventMasks)\n"
     ]
    }
   ],
   "source": [
    "X, Y, true = make_batch(bins, N_events=bs, N_clusters=max_n_rings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "84a7401b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x300 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1,2,figsize=(7,3))\n",
    "\n",
    "for i,ax in enumerate(axs):\n",
    "\n",
    "    im = ax.imshow(true[0,i],vmin=0,vmax=1)\n",
    "\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "    fig.colorbar(im, cax=cax, orientation='vertical')\n",
    "\n",
    "    ax.set_title(f'Truth photon {i}')\n",
    "\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "fccabc39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2, 32, 32])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b1db3315",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_slots=3\n",
    "bs,_,nx,ny = true.shape\n",
    "\n",
    "pred = torch.zeros(bs,k_slots,nx,ny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "bf1f0122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 32, 32])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "c771dee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, -1, -1, -1])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shift\n",
    "\n",
    "dx = np.random.randint(-shift,shift+1,size=bs)\n",
    "dy = np.random.randint(-shift,shift+1,size=bs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987d8278",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "cd7b72ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], size=(4, 2, 0, 32))"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true[:,:,-1:nx-1,:] #.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "23f9fd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "?np.random.randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c09c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smear_for_pred(true,k_slots=3):\n",
    "    '''\n",
    "    Goal: to setup a prediction that we can test against, just shift the OG image\n",
    "    '''\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "fa73dbe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2, 32, 32])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89549b06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e0a040a",
   "metadata": {},
   "source": [
    "## New plan\n",
    "\n",
    "\n",
    "OK, this is great, but I think I can make this even a little bit simpler for debugging!\n",
    "\n",
    "\n",
    "<img src=\"attn-2-rings-graphic.png\" style=\"height:250px\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "e81cf7d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true = torch.FloatTensor([[[1,0,0],[0,1,0]],\n",
    "                          [[0,0,1],[0,1,0]]\n",
    "                         ])\n",
    "true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "f6b323b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 3])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = torch.FloatTensor([[[.9,.4,0.],[.1,.1,.1],[0,.6,.9]],\n",
    "                          [[.9,.3,0.],[.1,.5,.1],[.9,.3,0.]]\n",
    "                         ])\n",
    "\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "d7696d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 3])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_flat = pred.flatten(0,1) # Shape batch_size * n_slots * pred_dim\n",
    "pred_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934a5bda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1616d746",
   "metadata": {},
   "source": [
    "`out_prob` should have shape 6, 4, 3\n",
    "\n",
    "and then we will sum along the last dim and it will have shape 6,4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9509944d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666bb2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or, do I want to calculate the ... for each ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743ffd4a",
   "metadata": {},
   "source": [
    "Right before I did this manually with tiling and broadcasting, but I _might_ want to do it manually now?\n",
    "\n",
    "**Some print statements I had from running the DETR kaggle nb:**\n",
    "\n",
    "```\n",
    "pred_logits torch.Size([2, 100, 2])\n",
    "out_prob torch.Size([2, 100, 2])\n",
    "v[labels] torch.Size([47])\n",
    "v[labels] torch.Size([46])\n",
    "tgt_ids torch.Size([93])\n",
    "-out_prob[:, tgt_ids] torch.Size([200, 93])\n",
    "```\n",
    "\n",
    "- `batch_size` = 2\n",
    "- `n_queries` = 100\n",
    "- There are two targets (?) \n",
    "    * one has 47 labels  \n",
    "    * the other has 49 labels\n",
    "    * Personal guess -- these are the number of truth labels for each of the events in the batch\n",
    "- Then the final loss over _all_ combinations has shape 200,93\n",
    "    * Personal guess these two targets _have_ to correspond to the batch size being  \n",
    "\n",
    "\n",
    "**TO DO:** Shape my data to have this shape :)\n",
    "\n",
    "- Should have shape (1 * 3, 4)\n",
    "    * **Step 1:** Do this with my \"dummy\" batch size of 1\n",
    "        - out_preds should have shape (1*3, 3) # bs * n_slot, n_dim (?)\n",
    "        - target should have shape (bs*)\n",
    "    * **Step 2:** Extend to a _bigger_ batch size \n",
    "    \n",
    "Hmmmm, I'm super not sure lol what to do here(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadf299a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418ece35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10a3973",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "60de63b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (1128360448.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[178], line 4\u001b[0;36m\u001b[0m\n\u001b[0;31m    out_prob = outputs[\"pred_logits\"].flatten(0, 1).softmax(-1)  # [batch_size * num_queries, num_classes]\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "bs, num_queries = outputs[\"pred_logits\"].shape[:2]\n",
    "\n",
    "        # We flatten to compute the cost matrices in a batch\n",
    "        out_prob = outputs[\"pred_logits\"].flatten(0, 1).softmax(-1)  # [batch_size * num_queries, num_classes]\n",
    "        out_bbox = outputs[\"pred_boxes\"].flatten(0, 1)  # [batch_size * num_queries, 4]\n",
    "\n",
    "        # Also concat the target labels and boxes\n",
    "        tgt_ids = torch.cat([v[\"labels\"] for v in targets])\n",
    "        tgt_bbox = torch.cat([v[\"boxes\"] for v in targets])\n",
    "\n",
    "        # Compute the classification cost. Contrary to the loss, we don't use the NLL,\n",
    "        # but approximate it in 1 - proba[target class].\n",
    "        # The 1 is a constant that doesn't change the matching, it can be ommitted.\n",
    "        cost_class = -out_prob[:, tgt_ids]\n",
    "\n",
    "        # Final cost matrix\n",
    "        C = self.cost_bbox * cost_bbox + self.cost_class * cost_class \n",
    "        C = C.view(bs, num_queries, -1).cpu()\n",
    "\n",
    "        sizes = [len(v[\"boxes\"]) for v in targets]\n",
    "        indices = [linear_sum_assignment(c[i]) for i, c in enumerate(C.split(sizes, -1))]\n",
    "        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce987af0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24adbdd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4b5417",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2471a7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a81f1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a291ef35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d0ed3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "class HungarianMatcher(nn.Module):\n",
    "    \"\"\"This class computes an assignment between the targets and the predictions of the network\n",
    "    For efficiency reasons, the targets don't include the no_object. Because of this, in general,\n",
    "    there are more predictions than targets. In this case, we do a 1-to-1 matching of the best predictions,\n",
    "    while the others are un-matched (and thus treated as non-objects).\n",
    "    \n",
    "    UNMODIFIED from https://www.kaggle.com/code/virajbagal/pytorch-starter-detection-transformer-train\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cost_class: float = 1, cost_bbox: float = 1, cost_giou: float = 1):\n",
    "        \"\"\"Creates the matcher\n",
    "        Params:\n",
    "            cost_class: This is the relative weight of the classification error in the matching cost\n",
    "            cost_bbox: This is the relative weight of the L1 error of the bounding box coordinates in the matching cost\n",
    "            cost_giou: This is the relative weight of the giou loss of the bounding box in the matching cost\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.cost_class = cost_class\n",
    "        self.cost_bbox = cost_bbox\n",
    "        self.cost_giou = cost_giou\n",
    "        assert cost_class != 0 or cost_bbox != 0 or cost_giou != 0, \"all costs cant be 0\"\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, outputs, targets):\n",
    "        \"\"\" Performs the matching\n",
    "        Params:\n",
    "            outputs: This is a dict that contains at least these entries:\n",
    "                 \"pred_logits\": Tensor of dim [batch_size, num_queries, num_classes] with the classification logits\n",
    "                 \"pred_boxes\": Tensor of dim [batch_size, num_queries, 4] with the predicted box coordinates\n",
    "            targets: This is a list of targets (len(targets) = batch_size), where each target is a dict containing:\n",
    "                 \"labels\": Tensor of dim [num_target_boxes] (where num_target_boxes is the number of ground-truth\n",
    "                           objects in the target) containing the class labels\n",
    "                 \"boxes\": Tensor of dim [num_target_boxes, 4] containing the target box coordinates\n",
    "        Returns:\n",
    "            A list of size batch_size, containing tuples of (index_i, index_j) where:\n",
    "                - index_i is the indices of the selected predictions (in order)\n",
    "                - index_j is the indices of the corresponding selected targets (in order)\n",
    "            For each batch element, it holds:\n",
    "                len(index_i) = len(index_j) = min(num_queries, num_target_boxes)\n",
    "        \"\"\"\n",
    "        bs, num_queries = outputs[\"pred_logits\"].shape[:2]\n",
    "\n",
    "        # We flatten to compute the cost matrices in a batch\n",
    "        out_prob = outputs[\"pred_logits\"].flatten(0, 1).softmax(-1)  # [batch_size * num_queries, num_classes]\n",
    "        out_bbox = outputs[\"pred_boxes\"].flatten(0, 1)  # [batch_size * num_queries, 4]\n",
    "\n",
    "        # Also concat the target labels and boxes\n",
    "        tgt_ids = torch.cat([v[\"labels\"] for v in targets])\n",
    "        tgt_bbox = torch.cat([v[\"boxes\"] for v in targets])\n",
    "\n",
    "        # Compute the classification cost. Contrary to the loss, we don't use the NLL,\n",
    "        # but approximate it in 1 - proba[target class].\n",
    "        # The 1 is a constant that doesn't change the matching, it can be ommitted.\n",
    "        cost_class = -out_prob[:, tgt_ids]\n",
    "\n",
    "        # Compute the L1 cost between boxes\n",
    "        cost_bbox = torch.cdist(out_bbox, tgt_bbox, p=1)\n",
    "\n",
    "        # Compute the giou cost betwen boxes\n",
    "        cost_giou = -generalized_box_iou(box_cxcywh_to_xyxy(out_bbox), box_cxcywh_to_xyxy(tgt_bbox))\n",
    "\n",
    "        # Final cost matrix\n",
    "        C = self.cost_bbox * cost_bbox + self.cost_class * cost_class + self.cost_giou * cost_giou\n",
    "        C = C.view(bs, num_queries, -1).cpu()\n",
    "\n",
    "        sizes = [len(v[\"boxes\"]) for v in targets]\n",
    "        indices = [linear_sum_assignment(c[i]) for i, c in enumerate(C.split(sizes, -1))]\n",
    "        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b308714f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8100becf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "class HungarianMatcher(nn.Module):\n",
    "    \"\"\"This class computes an assignment between the targets and the predictions of the network\n",
    "    For efficiency reasons, the targets don't include the no_object. Because of this, in general,\n",
    "    there are more predictions than targets. In this case, we do a 1-to-1 matching of the best predictions,\n",
    "    while the others are un-matched (and thus treated as non-objects).\n",
    "    \n",
    "    MODIFIED from https://www.kaggle.com/code/virajbagal/pytorch-starter-detection-transformer-train\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Creates the matcher\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def forward(self, outputs, targets):\n",
    "        \"\"\" Performs the matching\n",
    "        Params:\n",
    "            outputs: This is a dict that contains at least these entries:\n",
    "                 \"pred_logits\": Tensor of dim [batch_size, num_queries, num_classes] with the classification logits\n",
    "                 \"pred_boxes\": Tensor of dim [batch_size, num_queries, 4] with the predicted box coordinates\n",
    "            targets: This is a list of targets (len(targets) = batch_size), where each target is a dict containing:\n",
    "                 \"labels\": Tensor of dim [num_target_boxes] (where num_target_boxes is the number of ground-truth\n",
    "                           objects in the target) containing the class labels\n",
    "                 \"boxes\": Tensor of dim [num_target_boxes, 4] containing the target box coordinates\n",
    "        Returns:\n",
    "            A list of size batch_size, containing tuples of (index_i, index_j) where:\n",
    "                - index_i is the indices of the selected predictions (in order)\n",
    "                - index_j is the indices of the corresponding selected targets (in order)\n",
    "            For each batch element, it holds:\n",
    "                len(index_i) = len(index_j) = min(num_queries, num_target_boxes)\n",
    "        \"\"\"\n",
    "        bs, num_queries = outputs[\"pred_logits\"].shape[:2]\n",
    "\n",
    "        # We flatten to compute the cost matrices in a batch\n",
    "        out_prob = outputs[\"pred_logits\"].flatten(0, 1).softmax(-1)  # [batch_size * num_queries, num_classes]\n",
    "        out_bbox = outputs[\"pred_boxes\"].flatten(0, 1)  # [batch_size * num_queries, 4]\n",
    "\n",
    "        # Also concat the target labels and boxes\n",
    "        tgt_ids = torch.cat([v[\"labels\"] for v in targets])\n",
    "        tgt_bbox = torch.cat([v[\"boxes\"] for v in targets])\n",
    "\n",
    "        # Compute the classification cost. Contrary to the loss, we don't use the NLL,\n",
    "        # but approximate it in 1 - proba[target class].\n",
    "        # The 1 is a constant that doesn't change the matching, it can be ommitted.\n",
    "        cost_class = -out_prob[:, tgt_ids]\n",
    "\n",
    "        # Final cost matrix\n",
    "        C = self.cost_bbox * cost_bbox + self.cost_class * cost_class \n",
    "        C = C.view(bs, num_queries, -1).cpu()\n",
    "\n",
    "        sizes = [len(v[\"boxes\"]) for v in targets]\n",
    "        indices = [linear_sum_assignment(c[i]) for i, c in enumerate(C.split(sizes, -1))]\n",
    "        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "e5a9020b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5f6a28",
   "metadata": {},
   "source": [
    "**Goal:** Check out the `SetCriterion` class to see how they _applied_ these indices to get the final cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2e63ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetCriterion(nn.Module):\n",
    "    \"\"\" This class computes the loss for DETR.\n",
    "    The process happens in two steps:\n",
    "        1) we compute hungarian assignment between ground truth boxes and the outputs of the model\n",
    "        2) we supervise each pair of matched ground-truth / prediction (supervise class and box)\n",
    "        \n",
    "        MODIFIED from https://www.kaggle.com/code/virajbagal/pytorch-starter-detection-transformer-train    \n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, matcher, weight_dict, eos_coef, losses):\n",
    "        \"\"\" Create the criterion.\n",
    "        Parameters:\n",
    "            num_classes: number of object categories, omitting the special no-object category\n",
    "            matcher: module able to compute a matching between targets and proposals\n",
    "            weight_dict: dict containing as key the names of the losses and as values their relative weight.\n",
    "            eos_coef: relative classification weight applied to the no-object category\n",
    "            losses: list of all the losses to be applied. See get_loss for list of available losses.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.matcher = matcher\n",
    "        self.weight_dict = weight_dict\n",
    "        self.eos_coef = eos_coef\n",
    "        self.losses = losses\n",
    "        empty_weight = torch.ones(self.num_classes + 1)\n",
    "        empty_weight[-1] = self.eos_coef\n",
    "        self.register_buffer('empty_weight', empty_weight)\n",
    "\n",
    "    def loss_labels(self, outputs, targets, indices, num_boxes, log=True):\n",
    "        \"\"\"Classification loss (NLL)\n",
    "        targets dicts must contain the key \"labels\" containing a tensor of dim [nb_target_boxes]\n",
    "        \"\"\"\n",
    "        assert 'pred_logits' in outputs\n",
    "        src_logits = outputs['pred_logits']\n",
    "\n",
    "        idx = self._get_src_permutation_idx(indices)\n",
    "        target_classes_o = torch.cat([t[\"labels\"][J] for t, (_, J) in zip(targets, indices)])\n",
    "        target_classes = torch.full(src_logits.shape[:2], self.num_classes,\n",
    "                                    dtype=torch.int64, device=src_logits.device)\n",
    "        target_classes[idx] = target_classes_o\n",
    "\n",
    "        loss_ce = F.cross_entropy(src_logits.transpose(1, 2), target_classes, self.empty_weight)\n",
    "        losses = {'loss_ce': loss_ce}\n",
    "\n",
    "        if log:\n",
    "            # TODO this should probably be a separate loss, not hacked in this one here\n",
    "            losses['class_error'] = 100 - accuracy(src_logits[idx], target_classes_o)[0]\n",
    "        return losses\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def loss_cardinality(self, outputs, targets, indices, num_boxes):\n",
    "        \"\"\" Compute the cardinality error, ie the absolute error in the number of predicted non-empty boxes\n",
    "        This is not really a loss, it is intended for logging purposes only. It doesn't propagate gradients\n",
    "        \"\"\"\n",
    "        pred_logits = outputs['pred_logits']\n",
    "        device = pred_logits.device\n",
    "        tgt_lengths = torch.as_tensor([len(v[\"labels\"]) for v in targets], device=device)\n",
    "        # Count the number of predictions that are NOT \"no-object\" (which is the last class)\n",
    "        card_pred = (pred_logits.argmax(-1) != pred_logits.shape[-1] - 1).sum(1)\n",
    "        card_err = F.l1_loss(card_pred.float(), tgt_lengths.float())\n",
    "        losses = {'cardinality_error': card_err}\n",
    "        return losses\n",
    "\n",
    "    def loss_boxes(self, outputs, targets, indices, num_boxes):\n",
    "        \"\"\"Compute the losses related to the bounding boxes, the L1 regression loss and the GIoU loss\n",
    "           targets dicts must contain the key \"boxes\" containing a tensor of dim [nb_target_boxes, 4]\n",
    "           The target boxes are expected in format (center_x, center_y, w, h), normalized by the image size.\n",
    "        \"\"\"\n",
    "        assert 'pred_boxes' in outputs\n",
    "        idx = self._get_src_permutation_idx(indices)\n",
    "        src_boxes = outputs['pred_boxes'][idx]\n",
    "        target_boxes = torch.cat([t['boxes'][i] for t, (_, i) in zip(targets, indices)], dim=0)\n",
    "\n",
    "        loss_bbox = F.l1_loss(src_boxes, target_boxes, reduction='none')\n",
    "\n",
    "        losses = {}\n",
    "        losses['loss_bbox'] = loss_bbox.sum() / num_boxes\n",
    "\n",
    "        loss_giou = 1 - torch.diag(generalized_box_iou(\n",
    "            box_cxcywh_to_xyxy(src_boxes),\n",
    "            box_cxcywh_to_xyxy(target_boxes)))\n",
    "        losses['loss_giou'] = loss_giou.sum() / num_boxes\n",
    "        return losses\n",
    "\n",
    "    def loss_masks(self, outputs, targets, indices, num_boxes):\n",
    "        \"\"\"Compute the losses related to the masks: the focal loss and the dice loss.\n",
    "           targets dicts must contain the key \"masks\" containing a tensor of dim [nb_target_boxes, h, w]\n",
    "        \"\"\"\n",
    "        assert \"pred_masks\" in outputs\n",
    "\n",
    "        src_idx = self._get_src_permutation_idx(indices)\n",
    "        tgt_idx = self._get_tgt_permutation_idx(indices)\n",
    "\n",
    "        src_masks = outputs[\"pred_masks\"]\n",
    "\n",
    "        # TODO use valid to mask invalid areas due to padding in loss\n",
    "        target_masks, valid = nested_tensor_from_tensor_list([t[\"masks\"] for t in targets]).decompose()\n",
    "        target_masks = target_masks.to(src_masks)\n",
    "\n",
    "        src_masks = src_masks[src_idx]\n",
    "        # upsample predictions to the target size\n",
    "        src_masks = interpolate(src_masks[:, None], size=target_masks.shape[-2:],\n",
    "                                mode=\"bilinear\", align_corners=False)\n",
    "        src_masks = src_masks[:, 0].flatten(1)\n",
    "\n",
    "        target_masks = target_masks[tgt_idx].flatten(1)\n",
    "\n",
    "        losses = {\n",
    "            \"loss_mask\": sigmoid_focal_loss(src_masks, target_masks, num_boxes),\n",
    "            \"loss_dice\": dice_loss(src_masks, target_masks, num_boxes),\n",
    "        }\n",
    "        return losses\n",
    "\n",
    "    def _get_src_permutation_idx(self, indices):\n",
    "        # permute predictions following indices\n",
    "        batch_idx = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(indices)])\n",
    "        src_idx = torch.cat([src for (src, _) in indices])\n",
    "        return batch_idx, src_idx\n",
    "\n",
    "    def _get_tgt_permutation_idx(self, indices):\n",
    "        # permute targets following indices\n",
    "        batch_idx = torch.cat([torch.full_like(tgt, i) for i, (_, tgt) in enumerate(indices)])\n",
    "        tgt_idx = torch.cat([tgt for (_, tgt) in indices])\n",
    "        return batch_idx, tgt_idx\n",
    "\n",
    "    def get_loss(self, loss, outputs, targets, indices, num_boxes, **kwargs):\n",
    "        loss_map = {\n",
    "            'labels': self.loss_labels,\n",
    "            'cardinality': self.loss_cardinality,\n",
    "            'boxes': self.loss_boxes,\n",
    "            'masks': self.loss_masks\n",
    "        }\n",
    "        assert loss in loss_map, f'do you really want to compute {loss} loss?'\n",
    "        return loss_map[loss](outputs, targets, indices, num_boxes, **kwargs)\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        \"\"\" This performs the loss computation.\n",
    "        Parameters:\n",
    "             outputs: dict of tensors, see the output specification of the model for the format\n",
    "             targets: list of dicts, such that len(targets) == batch_size.\n",
    "                      The expected keys in each dict depends on the losses applied, see each loss' doc\n",
    "        \"\"\"\n",
    "        outputs_without_aux = {k: v for k, v in outputs.items() if k != 'aux_outputs'}\n",
    "\n",
    "        # Retrieve the matching between the outputs of the last layer and the targets\n",
    "        indices = self.matcher(outputs_without_aux, targets)\n",
    "\n",
    "        # Compute the average number of target boxes accross all nodes, for normalization purposes\n",
    "        num_boxes = sum(len(t[\"labels\"]) for t in targets)\n",
    "        num_boxes = torch.as_tensor([num_boxes], dtype=torch.float, device=next(iter(outputs.values())).device)\n",
    "#         if is_dist_avail_and_initialized():\n",
    "#             torch.distributed.all_reduce(num_boxes)\n",
    "        num_boxes = torch.clamp(num_boxes / 1, min=1).item()\n",
    "\n",
    "        # Compute all the requested losses\n",
    "        losses = {}\n",
    "        for loss in self.losses:\n",
    "            losses.update(self.get_loss(loss, outputs, targets, indices, num_boxes))\n",
    "\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b193964b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d646c824",
   "metadata": {},
   "source": [
    "OK, from reading through the code, I _think_ I catch the jist!\n",
    "1. Flatten the batch\n",
    "2. \n",
    "3. Return the indices\n",
    "4. Resort the predictions to match the loss\n",
    "5. Match them to derive the min loss over (disjoint) combinations\n",
    "\n",
    "\n",
    "KISS -- let's assume that I'll always have more predictions than targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe7308f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0f7728",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
