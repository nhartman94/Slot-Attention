{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b5383cf-09f3-4d62-8c8e-75b6ad9b0294",
   "metadata": {},
   "source": [
    "# Invariant Slot Attention\n",
    "\n",
    "**Goal:** I sort of thad this intuition for awhile that I want to be able to encode \"circleness\" into the slot representations that we're learning.\n",
    "\n",
    "This idea from the SA follow-up paper is not _exactly_ the same as this, but I think it's going in this direction!\n",
    "\n",
    "**Other optimization tricks included in this paper:**\n",
    "- Cosine decay (instead of exponential decay)\n",
    "- Use ResNet-34 as the image feature extractor model for \n",
    "    * They did modify the base block of this model to have stride 1 instead of 3\n",
    "- They do also add a $\\delta$ division with the positional embedding (they set $\\delta = 5$).\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "94d52811-e259-4fd6-a745-c839cc8c8417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mlp\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import json, yaml, os\n",
    "os.sys.path.append('code')\n",
    "\n",
    "from plotting import plot_kslots, plot_kslots_iters\n",
    "from data import make_batch\n",
    "from model import SoftPositionalEmbed, build_grid\n",
    "from torch.nn import init\n",
    "from train import hungarian_matching\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76d7725f-432f-45e6-aa15-e11556e2a85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87055668-eff5-4333-8cb6-f9ec6f765de7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hidden_dim': 16,\n",
       " 'k_slots': 3,\n",
       " 'query_dim': 128,\n",
       " 'learn_init': True,\n",
       " 'device': 'cpu'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('code/configs/learn-init-warm2.yaml') as f:\n",
    "    cd = yaml.safe_load(f)\n",
    "\n",
    "hps = cd['hps']\n",
    "del hps['softmax_T']\n",
    "hps['device'] = device\n",
    "\n",
    "hps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9a943fc-9e63-4123-9030-ff40190f0df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlow = -0.5,\n",
    "xhigh = 0.5,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187ada3e-5480-4230-a1e8-8eebe5adf4a8",
   "metadata": {},
   "source": [
    "**How was the data generator initialized?**\n",
    "- $x,y \\sim \\text{Unif}(-0.5, 0.5)$\n",
    "- $r \\sim \\text{Unif}(0.01, 0.05)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56c80a1c-46d3-46b6-9818-0104646d6876",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_slots=3\n",
    "\n",
    "ref_frame_dim = 3 # x,y,r\n",
    "\n",
    "stdlow=0.01\n",
    "stdhigh=0.05\n",
    "\n",
    "resolution = (32,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f6ffd33b-ab89-465d-8af0-13a060d18a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvariantSlotAttention(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 resolution=(32,32),\n",
    "                 xlow=-0.5,\n",
    "                 xhigh=0.5,\n",
    "                 k_slots=3, \n",
    "                 num_conv_layers=3,\n",
    "                 hidden_dim=32, \n",
    "                 final_cnn_relu=False,\n",
    "                 learn_init=False,\n",
    "                 query_dim=32, \n",
    "                 n_iter=2,\n",
    "                 pixel_mult=1,\n",
    "                 device='cpu' \n",
    "                 ):\n",
    "        '''\n",
    "        Slot attention encoder block with positional embedding\n",
    "\n",
    "        Inputs:\n",
    "        - resolution \n",
    "        - k_slots (default 3): number of slots (note, can vary between training and test time)\n",
    "        - num_conv_layers: # of convolutional layers to apply (google paper has 4)\n",
    "        - hidden_dim (default 32): The hidden dimension for the CNN (currently single layer w/ no non-linearities)\n",
    "        - final_cnn_relu: Whether to apply the final cnn relu for these experiments (use true to mimic google repo)\n",
    "        - query_dim (default 32): The latent space dimension that the slots and the queries get computed in\n",
    "        - n_iter (default  2): Number of slot attention steps to apply (defualt 2)\n",
    "        - T (str): Softmax temperature for scaling the logits \n",
    "            * default: 1/sqrt(query_dim)\n",
    "        - device (str): Which device to put the model on.\n",
    "            Options: cpu (default), mps, cuda:{i}\n",
    "            Also used when drawing random samples for the query points \n",
    "            and the grid generation for the positional encoding\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.k_slots = k_slots\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.query_dim = query_dim\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "        self.resolution = resolution\n",
    "        self.xlow, self.xhigh = xlow, xhigh\n",
    "        \n",
    "        self.device=device\n",
    "         \n",
    "        self.softmax_T = 10/np.sqrt(query_dim)\n",
    "        \n",
    "        self.dataN = torch.nn.LayerNorm(self.hidden_dim)\n",
    "        self.queryN = torch.nn.LayerNorm(self.query_dim)\n",
    "        \n",
    "        self.toK = torch.nn.Linear(self.hidden_dim, self.query_dim)\n",
    "        self.toV = torch.nn.Linear(self.hidden_dim, self.query_dim)\n",
    "        self.gru = torch.nn.GRUCell(self.query_dim, self.query_dim)\n",
    "\n",
    "        kwargs = {'out_channels': hidden_dim,'kernel_size': 5, 'padding':2 }\n",
    "        cnn_layers = [torch.nn.Conv2d(1,**kwargs)]\n",
    "        for i in range(num_conv_layers-1):\n",
    "            cnn_layers += [torch.nn.ReLU(), torch.nn.Conv2d(hidden_dim,**kwargs)] \n",
    "            \n",
    "        if final_cnn_relu:\n",
    "            cnn_layers.append(torch.nn.ReLU())\n",
    "\n",
    "        self.CNN_encoder = torch.nn.Sequential(*cnn_layers)\n",
    "            \n",
    "        '''\n",
    "        Positional embedding inputs\n",
    "        '''\n",
    "        self.abs_grid = self.build_grid()\n",
    "        \n",
    "        self.dense = torch.nn.Linear(2, query_dim) \n",
    "        self.pixel_mult = pixel_mult # LH's proposal... but almost same as 1/delta in ISA\n",
    "\n",
    "        # Apply after the data normalization\n",
    "        self.init_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(query_dim,query_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(query_dim,query_dim)\n",
    "        )\n",
    "           \n",
    "        '''\n",
    "        Initializer jazz\n",
    "        '''\n",
    "        self.slots_mu = torch.nn.Parameter(torch.randn(1, 1, self.query_dim))\n",
    "        self.slots_logsigma = torch.nn.Parameter(torch.zeros(1, 1, self.query_dim))\n",
    "        init.xavier_uniform_(self.slots_logsigma)\n",
    "\n",
    "        self.init_slots = self.init_slots\n",
    "\n",
    "    def build_grid(self):\n",
    "        '''\n",
    "        From google slot attention repo:\n",
    "        https://github.com/nhartman94/google-research/blob/master/slot_attention/model.py#L357C1-L364C53\n",
    "        '''\n",
    "        resolution = self.resolution\n",
    "        xlow, xhigh = self.xlow, self.xhigh\n",
    "           \n",
    "        ranges = [np.linspace(xlow, xhigh, num=res) for res in resolution]\n",
    "        grid = np.meshgrid(*ranges, sparse=False, indexing=\"ij\")\n",
    "        grid = np.stack(grid, axis=-1)\n",
    "        grid = np.reshape(grid, [resolution[0], resolution[1], -1])\n",
    "        grid = np.expand_dims(grid, axis=0)\n",
    "        # grid = grid.astype(np.float32)\n",
    "        \n",
    "        # Now make it a pytorch tensor\n",
    "        grid = torch.FloatTensor( grid ).to(device)\n",
    "        grid = torch.flatten(grid,1,2)\n",
    "    \n",
    "        return grid\n",
    "        \n",
    "    def init_slots(self,Nbatch):\n",
    "        '''\n",
    "        Slot init taken from\n",
    "        https://github.com/lucidrains/slot-attention/blob/master/slot_attention/slot_attention.py\n",
    "        '''\n",
    "        mu = self.slots_mu.expand(Nbatch, self.k_slots, -1)\n",
    "        sigma = self.slots_logsigma.exp().expand(Nbatch, self.k_slots, -1)\n",
    "\n",
    "        queries = mu + sigma * torch.randn(mu.shape).to(self.device)\n",
    "    \n",
    "        # Add the position and scale initialization for the local ref frame\n",
    "        ref_frame_dim = 3\n",
    "        pos_scale = torch.rand(Nbatch, k_slots, ref_frame_dim).to(self.device)\n",
    "\n",
    "        pos_scale[:,:2] -= 0.5\n",
    "        pos_scale[:,-1]  = (stdhigh - stdlow) * pos_scale[:,-1] + stdlow\n",
    "        \n",
    "        return queries, pos_scale\n",
    "    \n",
    "    def get_keys_vals(self, encoded_data, pos_scale):\n",
    "\n",
    "        # Get the relative position embedding\n",
    "        rel_grid = self.abs_grid.unsqueeze(1) - pos_scale[:,:,:2].unsqueeze(2)\n",
    "        rel_grid /= pos_scale[:,:,-1].unsqueeze(2).unsqueeze(-1)\n",
    "        \n",
    "        # Embed it in the same space as the query dimension \n",
    "        embed_grid = self.pixel_mult * self.dense( rel_grid )\n",
    "        \n",
    "        # keys, vals: (bs, img_dim, query_dim)\n",
    "        keys = m.toK(encoded_data).unsqueeze(1) + embed_grid\n",
    "        vals = m.toV(encoded_data).unsqueeze(1) + embed_grid\n",
    "        \n",
    "        keys = self.init_mlp(keys)\n",
    "        vals = self.init_mlp(vals)\n",
    "        \n",
    "        return keys, vals\n",
    "                \n",
    "    def attention_and_weights(self,queries,keys):\n",
    "        \n",
    "        logits = torch.einsum('bse,bsde->bsd',queries,keys) * self.softmax_T\n",
    "        \n",
    "        att = torch.nn.functional.softmax(logits, dim = 1)\n",
    "        \n",
    "        div = torch.sum(att, dim = -1, keepdims = True)\n",
    "        wts = att/div + 1e-8\n",
    "        return att,wts\n",
    "\n",
    "    def update_frames(self,wts):\n",
    "        '''\n",
    "        Update the relative frame position\n",
    "        '''\n",
    "        \n",
    "        # expand to include the batch dim\n",
    "        grid_exp = self.abs_grid.expand(wts.shape[0],-1,2)\n",
    "        \n",
    "        new_pos = torch.einsum('bsd,bde->bse',wts,grid_exp)\n",
    "        \n",
    "        spread = torch.sum(torch.pow(grid_exp.unsqueeze(1) - new_pos.unsqueeze(2),2),dim=-1)\n",
    "        \n",
    "        new_scale = torch.einsum('bsd,bsd->bs', wts, spread)\n",
    "        new_scale = torch.sqrt(new_scale)\n",
    "        \n",
    "        return torch.cat([new_pos,new_scale.unsqueeze(-1)],axis=-1)\n",
    "        \n",
    "    def iterate(self, queries, pos_scale, encoded_data):\n",
    "        \n",
    "        # Get the keys and values in the ref ref frame\n",
    "        keys, vals = self.get_keys_vals(encoded_data,pos_scale)\n",
    "        \n",
    "        # att,wts: (bs, k_slots, img_dim)\n",
    "        att,wts = self.attention_and_weights(self.queryN(queries),keys)   \n",
    "        \n",
    "        new_pos_scale = self.update_frames(wts)\n",
    "        \n",
    "        # Update the queries with the recurrent block\n",
    "        updates = torch.einsum('bsd,bsde->bse',wts,vals) # bs, n_slots, query_dim\n",
    "        \n",
    "        updates = self.gru(\n",
    "            updates.reshape(-1,self.query_dim),\n",
    "            queries.reshape(-1,self.query_dim),\n",
    "        )\n",
    "        \n",
    "        return updates.reshape(queries.shape), new_pos_scale\n",
    "        \n",
    "    def forward(self, data):\n",
    "    \n",
    "        '''\n",
    "        Step 1: Extract the CNN features\n",
    "        '''\n",
    "        encoded_data = self.CNN_encoder(data) # Apply the CNN encoder\n",
    "        encoded_data = torch.permute(encoded_data,(0,2,3,1)) # Put channel dim at the end\n",
    "        encoded_data = torch.flatten(encoded_data,1,2) # flatten pixel dims\n",
    "        encoded_data = self.dataN(encoded_data)\n",
    "        \n",
    "        '''\n",
    "        Step 2: Initialize the slots\n",
    "        '''\n",
    "        Nbatch = data.shape[0]\n",
    "        \n",
    "        # Initialize the queries and pos_scale\n",
    "        queries, pos_scale = self.init_slots(Nbatch) # Shape (Nbatch, k_slots, query_dim)\n",
    "        \n",
    "        '''\n",
    "        Step 3: Iterate through the reconstruction\n",
    "        '''\n",
    "        for i in range(self.n_iter):\n",
    "            queries, pos_scale = self.iterate(queries, pos_scale, encoded_data)    \n",
    "            \n",
    "        # With the final query vector, calc the attn, weights, + rel ref frames\n",
    "        keys, vals = self.get_keys_vals(encoded_data,pos_scale)\n",
    "        att, wts = self.attention_and_weights(self.queryN(queries),keys)   \n",
    "        new_pos_scale = self.update_frames(wts)\n",
    "        \n",
    "        return queries, pos_scale, att, wts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce4b9a4-1261-4e3e-87b4-bf1643800a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = InvariantSlotAttention(**hps)\n",
    "\n",
    "X, Y, mask = make_batch(N_events=5, **{'isRing': True, 'N_clusters':2})        \n",
    "\n",
    "with torch.no_grad():\n",
    "    queries, pos_scale, att, wts = m(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "f5bcc22d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.6287, 0.6514, 0.6443,  ..., 0.2491, 0.2406, 0.1891],\n",
       "         [0.3703, 0.3468, 0.3534,  ..., 0.3456, 0.3317, 0.2420],\n",
       "         [0.0010, 0.0017, 0.0022,  ..., 0.4053, 0.4277, 0.5688]],\n",
       "\n",
       "        [[0.5774, 0.4528, 0.3901,  ..., 0.0447, 0.0135, 0.0261],\n",
       "         [0.4156, 0.5374, 0.5976,  ..., 0.4350, 0.2243, 0.6055],\n",
       "         [0.0070, 0.0098, 0.0123,  ..., 0.5203, 0.7622, 0.3684]],\n",
       "\n",
       "        [[0.3796, 0.5927, 0.7783,  ..., 0.0120, 0.0089, 0.0054],\n",
       "         [0.3744, 0.1592, 0.0186,  ..., 0.0834, 0.1000, 0.0582],\n",
       "         [0.2460, 0.2481, 0.2031,  ..., 0.9046, 0.8911, 0.9364]],\n",
       "\n",
       "        [[0.7732, 0.8004, 0.8214,  ..., 0.0244, 0.0315, 0.0256],\n",
       "         [0.1821, 0.1686, 0.1356,  ..., 0.3372, 0.2806, 0.2437],\n",
       "         [0.0447, 0.0310, 0.0430,  ..., 0.6384, 0.6878, 0.7307]],\n",
       "\n",
       "        [[0.3790, 0.4040, 0.3970,  ..., 0.0836, 0.0757, 0.0651],\n",
       "         [0.5858, 0.5692, 0.5741,  ..., 0.2906, 0.3447, 0.3201],\n",
       "         [0.0351, 0.0268, 0.0289,  ..., 0.6258, 0.5796, 0.6148]]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b2ca02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758d0112",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "6ee359f6-257b-44a1-812f-e2b0906e2f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ISA(model, \n",
    "          Ntrain = 5000, \n",
    "          bs=32, \n",
    "          lr=3e-4,\n",
    "          warmup_steps=5_000,\n",
    "          decay_rate = 0.5,\n",
    "          decay_steps = 50_000,\n",
    "          losses = [],\n",
    "          kwargs={'isRing': True, 'N_clusters':2},\n",
    "          device='cpu',\n",
    "          plot_every=250, \n",
    "          save_every=1000,\n",
    "          color='C0',cmap='Blues',\n",
    "          modelDir='.',figDir='',showImg=True):\n",
    "    '''\n",
    "    Same arg as train, rn just modifying for more outputs\n",
    "    '''\n",
    "\n",
    "    # Learning rate schedule config\n",
    "    base_learning_rate = lr\n",
    "    \n",
    "    opt = torch.optim.Adam(model.parameters(), base_learning_rate)\n",
    "    model.train()\n",
    "    \n",
    "    k_slots = model.k_slots\n",
    "    resolution = model.resolution\n",
    "    kwargs['device'] = device\n",
    "\n",
    "    max_n_rings = kwargs['N_clusters']\n",
    "    isRing = kwargs[\"isRing\"]\n",
    "    print(f'Training model with {k_slots} slots on {max_n_rings}'+ (\"rings\" if isRing else \"blobs\"))\n",
    "\n",
    "    start = len(losses)\n",
    "    for i in range(start,start+Ntrain):\n",
    "        print('i=',i)\n",
    "        learning_rate = base_learning_rate * decay_rate ** (i / decay_steps)\n",
    "        if i < warmup_steps:\n",
    "            learning_rate *= (i / warmup_steps)\n",
    "        \n",
    "        opt.param_groups[0]['lr'] = learning_rate\n",
    "        \n",
    "        X, Y, mask = make_batch(N_events=bs, **kwargs)\n",
    "        \n",
    "        queries, pos_scale, att, wts = model(X)\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            indices = hungarian_matching(att,mask,bs,k_slots,max_n_rings,resolution[0])\n",
    "\n",
    "        # Apply the sorting to the predict\n",
    "        bis=torch.arange(bs).to(device)\n",
    "        indices=indices.to(device)\n",
    "\n",
    "        slots_sorted = torch.cat([att[bis,indices[:,0,ri]].unsqueeze(1) for ri in range(max_n_rings)],dim=1)\n",
    "        \n",
    "        flat_mask = mask.reshape(-1,max_n_rings, np.prod(resolution))\n",
    "        rings_sorted = torch.cat([flat_mask[bis,indices[:,1,ri]].unsqueeze(1) for ri in range(max_n_rings)],dim=1)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = F.binary_cross_entropy(slots_sorted,rings_sorted,reduction='none').sum(axis=1).mean()\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "        losses.append(float(loss))\n",
    "        \n",
    "        if i % plot_every == 0:\n",
    "            print('iter',i,', loss',loss.detach().cpu().numpy(),', lr',opt.param_groups[0]['lr'])\n",
    "            \n",
    "            iEvt = 0\n",
    "            att_img = att[iEvt].reshape(model.k_slots,*resolution)\n",
    "            plot_kslots(losses, \n",
    "                        mask[iEvt].sum(axis=0).detach().cpu().numpy(), \n",
    "                        att_img.detach().cpu().numpy(),\n",
    "                        k_slots, color=color,cmap=cmap,\n",
    "                        figname=f'{figDir}/loss-slots-iter{i}-evt{iEvt}.jpg',showImg=showImg)\n",
    "            \n",
    "        if i % save_every == 0:\n",
    "            torch.save(model.state_dict(), f'{modelDir}/m_{i}.pt')\n",
    "            with open(f'{modelDir}/loss.json','w') as f:\n",
    "                json.dump(losses, f)\n",
    "\n",
    "    model.eval()\n",
    "    return model,losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "043f2d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cID = 'test-isa'\n",
    "\n",
    "modelDir = f'code/models/{cID}'\n",
    "figDir = f'code/figures/{cID}'\n",
    "\n",
    "# for d in [modelDir,figDir]:\n",
    "#     os.mkdir(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "e1a26279",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=32\n",
    "max_n_rings=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf901c4-546d-44f3-b4f1-d6491b2dfd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with 3 slots on 2rings\n",
      "i= 1\n",
      "i= 2\n",
      "i= 3\n",
      "i= 4\n",
      "i= 5\n",
      "i= 6\n",
      "i= 7\n",
      "i= 8\n",
      "i= 9\n",
      "i= 10\n",
      "i= 11\n",
      "i= 12\n",
      "i= 13\n",
      "i= 14\n",
      "i= 15\n",
      "i= 16\n",
      "i= 17\n",
      "i= 18\n",
      "i= 19\n",
      "i= 20\n",
      "i= 21\n",
      "i= 22\n",
      "i= 23\n",
      "i= 24\n",
      "i= 25\n",
      "i= 26\n",
      "i= 27\n",
      "i= 28\n",
      "i= 29\n",
      "i= 30\n",
      "i= 31\n",
      "i= 32\n",
      "i= 33\n",
      "i= 34\n",
      "i= 35\n",
      "i= 36\n",
      "i= 37\n",
      "i= 38\n",
      "i= 39\n",
      "i= 40\n",
      "i= 41\n",
      "i= 42\n",
      "i= 43\n",
      "i= 44\n",
      "i= 45\n",
      "i= 46\n",
      "i= 47\n",
      "i= 48\n",
      "i= 49\n",
      "i= 50\n",
      "i= 51\n",
      "i= 52\n",
      "i= 53\n",
      "i= 54\n",
      "i= 55\n",
      "i= 56\n",
      "i= 57\n",
      "i= 58\n",
      "i= 59\n",
      "i= 60\n",
      "i= 61\n",
      "i= 62\n",
      "i= 63\n",
      "i= 64\n",
      "i= 65\n",
      "i= 66\n",
      "i= 67\n",
      "i= 68\n",
      "i= 69\n"
     ]
    }
   ],
   "source": [
    "m, losses = train_ISA(m,Ntrain=100,modelDir=modelDir,figDir=figDir,plot_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ec2402-1e86-4129-99fa-7b1400b1022f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b532d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_img = att[iEvt].reshape(model.k_slots,*resolution)\n",
    "plot_kslots(losses, \n",
    "            mask[iEvt].sum(axis=0).detach().cpu().numpy(), \n",
    "            att_img.detach().cpu().numpy(),\n",
    "            k_slots, color=color,cmap=cmap,\n",
    "            figname=f'{figDir}/loss-slots-iter{i}-evt{iEvt}.jpg',showImg=showImg)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
