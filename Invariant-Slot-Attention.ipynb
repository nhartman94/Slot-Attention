{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b5383cf-09f3-4d62-8c8e-75b6ad9b0294",
   "metadata": {},
   "source": [
    "# Invariant Slot Attention\n",
    "\n",
    "**Goal:** I sort of thad this intuition for awhile that I want to be able to encode \"circleness\" into the slot representations that we're learning.\n",
    "\n",
    "This idea from the SA follow-up paper is not _exactly_ the same as this, but I think it's going in this direction!\n",
    "\n",
    "**Other optimization tricks included in this paper:**\n",
    "- Cosine decay (instead of exponential decay)\n",
    "- Use ResNet-34 as the image feature extractor model for \n",
    "    * They did modify the base block of this model to have stride 1 instead of 3\n",
    "- They do also add a $\\delta$ division with the positional embedding (they set $\\delta = 5$).\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94d52811-e259-4fd6-a745-c839cc8c8417",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mlp\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import json, yaml, os\n",
    "os.sys.path.append('code')\n",
    "\n",
    "from plotting import plot_kslots, plot_kslots_iters\n",
    "from data import make_batch\n",
    "from model import SoftPositionalEmbed, build_grid\n",
    "from torch.nn import init\n",
    "from train import train\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76d7725f-432f-45e6-aa15-e11556e2a85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87055668-eff5-4333-8cb6-f9ec6f765de7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hidden_dim': 16,\n",
       " 'k_slots': 3,\n",
       " 'query_dim': 128,\n",
       " 'learn_init': True,\n",
       " 'device': 'cpu'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('code/configs/learn-init-warm2.yaml') as f:\n",
    "    cd = yaml.safe_load(f)\n",
    "\n",
    "hps = cd['hps']\n",
    "del hps['softmax_T']\n",
    "hps['device'] = device\n",
    "\n",
    "hps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9a943fc-9e63-4123-9030-ff40190f0df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlow = -0.5,\n",
    "xhigh = 0.5,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c06c717a-99b7-44f1-9b04-9133df9aeb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_grid(resolution, xlow=0., xhigh=1.):\n",
    "    '''\n",
    "    From google slot attention repo:\n",
    "    https://github.com/nhartman94/google-research/blob/master/slot_attention/model.py#L357C1-L364C53\n",
    "    '''\n",
    "    ranges = [np.linspace(xlow, xhigh, num=res) for res in resolution]\n",
    "    grid = np.meshgrid(*ranges, sparse=False, indexing=\"ij\")\n",
    "    grid = np.stack(grid, axis=-1)\n",
    "    grid = np.reshape(grid, [resolution[0], resolution[1], -1])\n",
    "    grid = np.expand_dims(grid, axis=0)\n",
    "    grid = grid.astype(np.float32)\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eba0f8-de15-4bfd-967d-a84e932d13dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "187ada3e-5480-4230-a1e8-8eebe5adf4a8",
   "metadata": {},
   "source": [
    "**How was the data generator initialized?**\n",
    "- $x,y \\sim \\text{Unif}(-0.5, 0.5)$\n",
    "- $r \\sim \\text{Unif}(0.01, 0.05)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56c80a1c-46d3-46b6-9818-0104646d6876",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_slots=3\n",
    "\n",
    "ref_frame_dim = 3 # x,y,r\n",
    "\n",
    "stdlow=0.01\n",
    "stdhigh=0.05\n",
    "\n",
    "resolution = (32,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7f77e51-34e5-4ff8-8199-48c7133ccc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_grid = build_grid(resolution,xlow,xhigh)\n",
    "abs_grid = torch.FloatTensor( abs_grid ).to(device)\n",
    "abs_grid = torch.flatten(abs_grid,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "072aea4e-199c-4b5c-ade8-29997d2d0d0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs_grid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5e12f93-0bea-42be-9e18-7faddc50ed95",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nbatch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6ffd33b-ab89-465d-8af0-13a060d18a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvariantSlotAttention(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 resolution=(32,32),\n",
    "                 xlow=-0.5,\n",
    "                 xhigh=0.5,\n",
    "                 k_slots=3, \n",
    "                 num_conv_layers=3,\n",
    "                 hidden_dim=32, \n",
    "                 final_cnn_relu=False,\n",
    "                 learn_init=False,\n",
    "                 query_dim=32, \n",
    "                 n_iter=2,\n",
    "                 pixel_mult=1,\n",
    "                 device='cpu' \n",
    "                 ):\n",
    "        '''\n",
    "        Slot attention encoder block with positional embedding\n",
    "\n",
    "        Inputs:\n",
    "        - resolution \n",
    "        - k_slots (default 3): number of slots (note, can vary between training and test time)\n",
    "        - num_conv_layers: # of convolutional layers to apply (google paper has 4)\n",
    "        - hidden_dim (default 32): The hidden dimension for the CNN (currently single layer w/ no non-linearities)\n",
    "        - final_cnn_relu: Whether to apply the final cnn relu for these experiments (use true to mimic google repo)\n",
    "        - query_dim (default 32): The latent space dimension that the slots and the queries get computed in\n",
    "        - n_iter (default  2): Number of slot attention steps to apply (defualt 2)\n",
    "        - T (str): Softmax temperature for scaling the logits \n",
    "            * default: 1/sqrt(query_dim)\n",
    "        - device (str): Which device to put the model on.\n",
    "            Options: cpu (default), mps, cuda:{i}\n",
    "            Also used when drawing random samples for the query points \n",
    "            and the grid generation for the positional encoding\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.k_slots = k_slots\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.query_dim = query_dim\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "        self.resolution = resolution\n",
    "        self.xlow, self.xhigh = xlow, xhigh\n",
    "        \n",
    "        self.device=device\n",
    "         \n",
    "        self.softmax_T = 10/np.sqrt(query_dim)\n",
    "        \n",
    "        self.dataN = torch.nn.LayerNorm(self.hidden_dim)\n",
    "        self.queryN = torch.nn.LayerNorm(self.query_dim)\n",
    "        \n",
    "        self.toK = torch.nn.Linear(self.hidden_dim, self.query_dim)\n",
    "        self.toV = torch.nn.Linear(self.hidden_dim, self.query_dim)\n",
    "        self.gru = torch.nn.GRUCell(self.query_dim, self.query_dim)\n",
    "\n",
    "        kwargs = {'out_channels': hidden_dim,'kernel_size': 5, 'padding':2 }\n",
    "        cnn_layers = [torch.nn.Conv2d(1,**kwargs)]\n",
    "        for i in range(num_conv_layers-1):\n",
    "            cnn_layers += [torch.nn.ReLU(), torch.nn.Conv2d(hidden_dim,**kwargs)] \n",
    "            \n",
    "        if final_cnn_relu:\n",
    "            cnn_layers.append(torch.nn.ReLU())\n",
    "\n",
    "        self.CNN_encoder = torch.nn.Sequential(*cnn_layers)\n",
    "            \n",
    "        '''\n",
    "        Positional embedding inputs\n",
    "        '''\n",
    "        self.abs_grid = self.build_grid()\n",
    "        \n",
    "        self.dense = torch.nn.Linear(2, query_dim) \n",
    "        self.pixel_mult = pixel_mult # LH's proposal... but almost same as 1/delta in ISA\n",
    "\n",
    "        # Apply after the data normalization\n",
    "        self.init_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_dim,hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_dim,hidden_dim)\n",
    "        )\n",
    "           \n",
    "        '''\n",
    "        Initializer jazz\n",
    "        '''\n",
    "        self.slots_mu = torch.nn.Parameter(torch.randn(1, 1, self.query_dim))\n",
    "        self.slots_logsigma = torch.nn.Parameter(torch.zeros(1, 1, self.query_dim))\n",
    "        init.xavier_uniform_(self.slots_logsigma)\n",
    "\n",
    "        self.init_slots = self.init_slots\n",
    "\n",
    "    def build_grid(self):\n",
    "        '''\n",
    "        From google slot attention repo:\n",
    "        https://github.com/nhartman94/google-research/blob/master/slot_attention/model.py#L357C1-L364C53\n",
    "        '''\n",
    "        resolution = self.resolution\n",
    "        xlow, xhigh = self.xlow, self.xhigh\n",
    "           \n",
    "        ranges = [np.linspace(xlow, xhigh, num=res) for res in resolution]\n",
    "        grid = np.meshgrid(*ranges, sparse=False, indexing=\"ij\")\n",
    "        grid = np.stack(grid, axis=-1)\n",
    "        grid = np.reshape(grid, [resolution[0], resolution[1], -1])\n",
    "        grid = np.expand_dims(grid, axis=0)\n",
    "        # grid = grid.astype(np.float32)\n",
    "        \n",
    "        # Now make it a pytorch tensor\n",
    "        grid = torch.FloatTensor( grid ).to(device)\n",
    "        grid = torch.flatten(grid,1,2)\n",
    "    \n",
    "        return grid\n",
    "        \n",
    "    def init_slots(self,Nbatch):\n",
    "        '''\n",
    "        Slot init taken from\n",
    "        https://github.com/lucidrains/slot-attention/blob/master/slot_attention/slot_attention.py\n",
    "        '''\n",
    "        mu = self.slots_mu.expand(Nbatch, self.k_slots, -1)\n",
    "        sigma = self.slots_logsigma.exp().expand(Nbatch, self.k_slots, -1)\n",
    "\n",
    "        queries = mu + sigma * torch.randn(mu.shape).to(self.device)\n",
    "    \n",
    "        # Add the position and scale initialization for the local ref frame\n",
    "        pos_scale = torch.rand(Nbatch, k_slots, ref_frame_dim).to(self.device)\n",
    "\n",
    "        pos_scale[:,:2] -= 0.5\n",
    "        pos_scale[:,-1]  = (stdhigh - stdlow) * pos_scale[:,-1] + stdlow\n",
    "        \n",
    "        return queries, pos_scale\n",
    "    \n",
    "    def get_keys_vals(self, encoded_data, pos_scale):\n",
    "\n",
    "        # Get the relative position embedding\n",
    "        rel_grid = self.abs_grid.unsqueeze(1) - pos_scale[:,:,:2].unsqueeze(2)\n",
    "        rel_grid /= pos_scale[:,:,-1].unsqueeze(2).unsqueeze(-1)\n",
    "        \n",
    "        # Embed it in the same space as the query dimension \n",
    "        embed_grid = self.pixel_mult * self.dense( rel_grid )\n",
    "        \n",
    "        # keys, vals: (bs, img_dim, query_dim)\n",
    "        keys = self.toK(encoded_data) + rel_grid\n",
    "        vals = self.toV(encoded_data) + rel_grid\n",
    "        \n",
    "        keys = self.init_mlp(self.dataN(keys))\n",
    "        vals = self.init_mlp(self.dataN(vals))\n",
    "        \n",
    "        return keys, vals\n",
    "                \n",
    "    def attention_and_weights(self,queries,keys):\n",
    "        \n",
    "        logits = torch.einsum('bse,bde->bsd',queries,keys) * self.softmax_T\n",
    "        \n",
    "        att = torch.nn.functional.softmax(logits, dim = 1)\n",
    "        \n",
    "        div = torch.sum(att, dim = -1, keepdims = True)\n",
    "        wts = att/div + 1e-8\n",
    "        return att,wts\n",
    "\n",
    "    def update_frames(self,wts):\n",
    "        # Update the relative frame position\n",
    "        \n",
    "        new_pos = torch.einsum('bsd,bde->bse',wts,self.abs_grid)\n",
    "        new_scale = torch.einsum('bsd,bde->bse',wts,torch.pow(self.abs_grid - new_pos,2))\n",
    "        new_scale = torch.sqrt(new_scale)\n",
    "        new_pos_scale = torch.cat([new_pos,new_scale.unsqueeze(-1)],axis=-1)\n",
    "        return new_pos_scale\n",
    "        \n",
    "    def iterate(self, queries, pos_scale, encoded_data):\n",
    "        \n",
    "        print('getting keys')\n",
    "        # Get the keys and values in the ref ref frame\n",
    "        keys, vals = self.get_keys_vals(encoded_data,pos_scale)\n",
    "        \n",
    "        print('getting attn')\n",
    "        # att,wts: (bs, k_slots, img_dim)\n",
    "        att,wts = self.attention_and_weights(self.queryN(queries),keys)   \n",
    "        \n",
    "        print('Updating ref frames')\n",
    "        new_pos_scale = self.update_frames(wts)\n",
    "        \n",
    "        print('Doing recurrence')\n",
    "        # Update the queries with the recurrent block\n",
    "        updates = torch.einsum('bsd,bde->bse',wts,vals) # bs, n_slots, query_dim\n",
    "        \n",
    "        updates = self.gru(\n",
    "            updates.reshape(-1,self.query_dim),\n",
    "            queries.reshape(-1,self.query_dim),\n",
    "        )\n",
    "        \n",
    "        return updates.reshape(queries.shape), new_pos_scale\n",
    "        \n",
    "    def forward(self, data):\n",
    "    \n",
    "        '''\n",
    "        Step 1: Extract the CNN features\n",
    "        '''\n",
    "        encoded_data = self.CNN_encoder(data) # Apply the CNN encoder\n",
    "        encoded_data = torch.permute(encoded_data,(0,2,3,1)) # Put channel dim at the end\n",
    "        encoded_data = torch.flatten(encoded_data,1,2) # flatten pixel dims\n",
    "        \n",
    "        print('encoded_data',encoded_data.shape)\n",
    "        \n",
    "        '''\n",
    "        Step 2: Initialize the slots\n",
    "        '''\n",
    "        Nbatch = data.shape[0]\n",
    "        \n",
    "        # Initialize the queries and pos_scale\n",
    "        queries, pos_scale = self.init_slots(Nbatch) # Shape (Nbatch, k_slots, query_dim)\n",
    "        \n",
    "        '''\n",
    "        Step 3: Iterate through the reconstruction\n",
    "        '''\n",
    "        for i in range(self.n_iter):\n",
    "            print('iter',i)\n",
    "            queries, pos_scale = self.iterate(queries, encoded_data, pos_scale)    \n",
    "            \n",
    "        # With the final query vector, calc the attn, weights, + rel ref frames\n",
    "        keys, vals = self.get_keys_vals(encoded_data,pos_scale)\n",
    "        att, wts = self.attention_and_weights(self.queryN(queries),keys)   \n",
    "        new_pos_scale = self.update_frames(wts)\n",
    "        \n",
    "        return queries, pos_scale, att, wts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce4b9a4-1261-4e3e-87b4-bf1643800a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_data torch.Size([100, 1024, 16])\n",
      "iter 0\n"
     ]
    }
   ],
   "source": [
    "m = InvariantSlotAttention(**hps)\n",
    "\n",
    "X, Y, mask = make_batch(N_events=100, **{'isRing': True, 'N_clusters':2})        \n",
    "queries, pos_scale, att, wts = m(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77e9abe-0557-49ef-8602-c359a90e4c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee359f6-257b-44a1-812f-e2b0906e2f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ISA(model, \n",
    "          Ntrain = 5000, \n",
    "          bs=32, \n",
    "          lr=3e-4,\n",
    "          warmup_steps=5_000,\n",
    "          decay_rate = 0.5,\n",
    "          decay_steps = 50_000,\n",
    "          losses = [],\n",
    "          kwargs={'isRing': True, 'N_clusters':2},\n",
    "          device='cpu',\n",
    "          plot_every=250, \n",
    "          save_every=1000,\n",
    "          color='C0',cmap='Blues',\n",
    "          modelDir='.',figDir='',showImg=True):\n",
    "    '''\n",
    "    Same arg as train, rn just modifying for more outputs\n",
    "    '''\n",
    "\n",
    "    # Learning rate schedule config\n",
    "    base_learning_rate = lr\n",
    "    \n",
    "    opt = torch.optim.Adam(model.parameters(), base_learning_rate)\n",
    "    model.train()\n",
    "    \n",
    "    k_slots = model.k_slots\n",
    "    resolution = model.resolution\n",
    "    kwargs['device'] = device\n",
    "\n",
    "    max_n_rings = kwargs['N_clusters']\n",
    "    isRing = kwargs[\"isRing\"]\n",
    "    print(f'Training model with {k_slots} slots on {max_n_rings}'+ (\"rings\" if isRing else \"blobs\"))\n",
    "\n",
    "    start = len(losses)\n",
    "    for i in range(start,start+Ntrain):\n",
    "\n",
    "        learning_rate = base_learning_rate * decay_rate ** (i / decay_steps)\n",
    "        if i < warmup_steps:\n",
    "            learning_rate *= (i / warmup_steps)\n",
    "        \n",
    "        opt.param_groups[0]['lr'] = learning_rate\n",
    "        \n",
    "        X, Y, mask = make_batch(N_events=bs, **kwargs)\n",
    "        \n",
    "        queries, pos_scale, att, wts = model(X)\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            indices = hungarian_matching(att,mask,bs,k_slots,max_n_rings,resolution[0])\n",
    "\n",
    "        # Apply the sorting to the predict\n",
    "        bis=torch.arange(bs).to(device)\n",
    "        indices=indices.to(device)\n",
    "\n",
    "        slots_sorted = torch.cat([att[bis,indices[:,0,ri]].unsqueeze(1) for ri in range(max_n_rings)],dim=1)\n",
    "        \n",
    "        flat_mask = mask.reshape(-1,max_n_rings, np.prod(resolution))\n",
    "        rings_sorted = torch.cat([flat_mask[bis,indices[:,1,ri]].unsqueeze(1) for ri in range(max_n_rings)],dim=1)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = F.binary_cross_entropy(slots_sorted,rings_sorted,reduction='none').sum(axis=1).mean()\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "        losses.append(float(loss))\n",
    "        \n",
    "        if i % plot_every == 0:\n",
    "            print('iter',i,', loss',loss.detach().cpu().numpy(),', lr',opt.param_groups[0]['lr'])\n",
    "            \n",
    "            iEvt = 0\n",
    "            att_img = att[iEvt].reshape(model.k_slots,*resolution)\n",
    "            plot_kslots(losses, \n",
    "                        mask[iEvt].sum(axis=0).detach().cpu().numpy(), \n",
    "                        att_img.detach().cpu().numpy(),\n",
    "                        k_slots, color=color,cmap=cmap,\n",
    "                        figname=f'{figDir}/loss-slots-iter{i}-evt{iEvt}.jpg',showImg=showImg)\n",
    "            \n",
    "        if i % save_every == 0:\n",
    "            torch.save(model.state_dict(), f'{modelDir}/m_{i}.pt')\n",
    "            with open(f'{modelDir}/loss.json','w') as f:\n",
    "                json.dump(losses, f)\n",
    "\n",
    "    model.eval()\n",
    "    return model,losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf901c4-546d-44f3-b4f1-d6491b2dfd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, losses = train(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ec2402-1e86-4129-99fa-7b1400b1022f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
